title,company,location,link,description
Analytics Engineer (Junior to Mid Level),Holistics Data,Vietnam,https://vn.linkedin.com/jobs/view/analytics-engineer-junior-to-mid-level-at-holistics-data-4301987191?position=1&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=zkXs7jSXtfuOwCgKplsz%2BQ%3D%3D,"Introduction
Holistics is a Business Intelligence platform built for the AI era. We are the team behind projects like the BI tool Holistics, the database diagram tool dbdiagram.io, and the open-source project dbml.org. We help data teams build a trusted, supervised self-service analytics experience for their business stakeholders.
We are searching for an
Analytics Engineer (junior to mid-level)
to help us evolve our data pipeline to support internal decisions, as well as helping our customers get more value of Holistics BI platform.
This is a full-time role based in
Ho Chi Minh City, Vietnam
, with a flexible, hybrid WFH/WFO setup.
What You Will Do
This role is a hybrid of technical analytics work and customer-facing engagements. Your time will be allocated across two core functions.
Internal data pipeline/modeling & BI:
You will actively contribute to our internal data & BI platform, ensuring our internal stakeholders get the data they need
Manage and enhance internal data pipeline: Contribute to the maintenance of our internal data ingestion, transformation, and reporting pipeline, ensuring data is timely and correct. This hands-on work will be a source of direct feedback for our product team.
Build best-practice data models: Develop and refactor internal data models using dbt, following data modeling paradigms like Kimball/Inmon and star/snowflake schemas. These models will serve as real-world examples for customer education.
Ensure data quality and integrity: Implement data tests and validation checks to ensure internal metrics are calculated correctly, translating these practices into customer-facing guidance.
Research and experiment with new tools: Test-drive emerging BI and analytics engineering tools, evaluate how they might enhance our demos or internal stack, and share your findings with the product and marketing teams.
Customer engagement:
Besides working on internal data pipeline, you’ll also get the chance to engage with our customers directly on their analytics projects
Guide customer onboarding: Work directly with new customers via shared Slack channels, review their data warehouse schemas, and coach them on data modeling best practices
Develop enablement assets: Convert modeling best practices and hard-won insights into step-by-step guides, sample Git repositories, and short video tutorials so customers and internal teams can self-serve.
Deliver scoped consulting: Spend focused time helping strategic accounts solve complex data modeling challenges, then document the patterns for future projects and content.
Our current data setup
We wrote an entire book explaining the complex analytics landscape, so it’s fair to say that we know a thing or two about setting up data stack. Our data setup is as follows:
Ingestion and Transformation: Event tracking (Snowplow) and ETLs from various data sources into our data lake (GCS) and data warehouse (BigQuery). The data is then transformed and moved to the data mart using dbt.
Serving: Reporting is served on Holistics, and reverse-ETLs are handled to push data back into our CRMs for operational purposes.
Orchestration: We use Prefect on top of the data stack to orchestrate and automate data pipelines and other operations.
On the business side, we practice what we preach. That means instead of building reports and dashboards, Data team works to maintain a common set of metric definitions and educate the business users on how to self-serve the platform effectively.
What We Look For In You
We are seeking a candidate with a unique blend of technical depth and communication skills.
A genuine interest in the data analytics / BI industry as a whole and a love for owning the entire data stack, not just one part of it.
Systems thinking: You can reason about how business operations, data collection, and system architecture interact and affect one another. When you solve a problem once, you instinctively template it for the next fifty times.
Excellent communication skills: You can communicate complex technical ideas clearly in both written and spoken English. You write like you code - clear, concise, and maintainable.
Problem-solving orientation: You have solid problem decomposition skills and are energized by live problem-solving in workshops and discovery calls.
Continuous learner: You possess a growth mindset and are excited to learn something new as part of your work every day.
Technical Skills
SQL: You have a good foundation in SQL. You can write readable queries to extract and manipulate data, and you can explain core concepts like grain and joins.
Data Modeling: You are familiar with fundamental data modeling concepts (e.g., dimensional modeling) and schema designs (star/snowflake) and understand why we transform raw data into user-friendly formats.
Programming: You have some experience with Python or an equivalent language for scripting and data manipulation tasks.
Version Control: You are familiar with the basics of Git for tracking changes and collaborating on projects.
Cloud Databases: You have exposure to, or academic experience with, a major cloud data warehouse (e.g., BigQuery, Snowflake, Redshift).
Nice-to-haves:
Exposure to dbt, Looker/LookML, Holistics, or other modern BI tools; a demonstrated interest in learning how data infrastructure empowers business decisions.
Interview Process
Round 1: A chat with the team and a light technical interview to discuss your skills, experience, and past projects.‍
Round 2: A short take-home assignment or on-site assignment to assess your technical and problem-solving skills.‍
Round 3: A culture and behavioral chat with management.
Why You'll Love Working
💕 Benefits
Flexible working arrangement, hybrid work (WFO+WFH) policies
14 annual leaves, 14 sick leaves, and childcare leave policy.
Annual budget for personal and professional growth, well-being, and interest cultivations.
24/7 healthcare insurance; periodic medical checkups.
13-month salary; annual compensation reviews
Weekly happy hours, company-organized events.
Workstations provided for maximum productivity
MacBook (or Laptop) Grant
Mechanical Keyboard + Mouse
Big-screen monitor(s)
Working time:
From Monday – Friday, from 9.00 a.m. to 6.00 p.m.
Work location:
457-459 Nguyen Dinh Chieu, Ward 5, District 3, Ho Chi Minh City, Vietnam."
Data Engineer,The Value Maximizer,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-the-value-maximizer-4309898349?position=2&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=A1wLxTXtgvJA139YzfcDEQ%3D%3D,"MÔ TẢ CÔNG VIỆC
Phân tích yêu cầu nghiệp vụ cho hệ thống dữ liệu từ nguồn, phối hợp xây dựng các yêu cầu dữ liệu phục vụ báo cáo, phân tích từ các hệ thống nguồn khác nhau như T24, W4,..
Vận hành , phát triển & tối ưu hóa luồng dữ liệu - cả mảng dữ liệu & sandbox cho các team Analytics
Duy trì & phát triển hệ thống từ điển dữ liệu cho khối KHCN Yêu cầu tuyển dụng1. Trình độ Học vấn
Đại học hoặc cao hơn liên quan đến 1 trong các ngành kinh tế, tài chính, ngân hàng, hoặc công nghệ thông tin, toán tin ứng dụng, ngoại thương...2.Kiến thức/ Chuyên môn Có Liên Quan
Có kinh nghiệm ở vai trò Kỹ sư Dữ liệu hoặc vai trò tương đương
Có kinh nghiệm thiết kế ở mức schema các hệ quản trị cơ sở dữ liệu
Có kinh nghiệm cũng như kiến thức nghiệp vụ ngân hàng cũng như các hệ thống nghiệp vụ ngân hàng
Có kinh nghiệm làm việc với các hệ thống dữ liệu: Data Warehouse/ Data LakeHouse/ Data Product
Có kiến thức về các ngôn ngữ lập trình (SQL, Python, Spark)
Có kinh nghiệm triển khai thực tế với các hệ thống IBM DWH, AWS, Databricks là 1 điểm cộng
Có chứng chỉ về Data Engineer là 1 điểm cộng Quyền lợi và chế độ đãi ngộ
Mức lương: 20- 40 triệu + Thưởng dự án
Đóng đầy đủ bảo hiểm
Thưởng lễ, Tết
Xét tăng lương hàng năm (8-10 triệu/năm)
Nghỉ phép theo quy định công ty
Onsite dự án bank (dài hạn)
Thử việc 2 tháng (lương thử việc 85%)"
Data Engineer Executive,TH Food Chain,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-executive-at-th-food-chain-4309552289?position=3&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=uiK2KSCviDuWVbrDB7AKnw%3D%3D,"Tìm việc làm tương tự:
Xem tất cả việc làm, THFC_Tất Cả Việc Làm, THFC_Công Nghệ Thông Tin, Công nghệ thông tin"
"Data Engineer (Python,SQL upto $2500)",FPT Software Innovation,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-python-sql-upto-%242500-at-fpt-software-innovation-4306343052?position=4&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=u9DqP%2Fw5qDRABex9RG0psg%3D%3D,"Position: Data Engineer (Midde/Senior)
Location: District 9, Ho Chi Minh City
About the Role
We are looking for a Data Engineer with 3+ years of experience, strong expertise in
Python and SQL
, and hands-on experience with Azure Data Services. This is a great opportunity to grow technically while contributing to an enterprise-level Big Data transformation for global projects.
Key Responsibilities
Develop and maintain scalable, production-grade data pipelines.
Work extensively with
Python and SQL
to build, optimize, and maintain ETL/ELT processes.
Work with Azure components (Data Lake, Data Factory, Synapse, etc.).
Support data modeling and integration across various structured data sources.
Contribute to platform migration efforts toward Big Data architecture.
Collaborate with analysts, architects, and stakeholders to deliver reusable data assets.
Participate in Agile ceremonies (standups, sprint planning, retros, etc.).
Required Skills & Experience
Bachelor’s degree in IT, Computer Science, Data Science, or related fields.
3+ years of experience in data engineering.
Strong programming skills in Python and SQL
with solid knowledge of RDBMS (Oracle, SQL Server).
Hands-on experience with Azure Data Services: Data Lake, Data Factory, Synapse.
Good understanding of data modeling, integration patterns, and pipeline orchestration.
Good English communication and teamwork skills; ability to work cross-functionally.
Nice to have:
Experience with Databricks (including Unity Catalog).
Benefits
“FPT Care” health insurance provided by Petrolimex (PJICO), exclusive for FPT employees.
Annual summer vacation starting from May, following company policy.
Annual salary review.
International, dynamic, and friendly working environment.
Annual leave and working conditions compliant with Vietnam labor laws.
Sponsorship for international certification exams and study programs.
Loan interest support policy for Fsofter employees.
Shuttle bus service for employees.
Contact
Ms. Thu – Talent Acquisition Specialist
Email: Thuthm1@fpt.com
| Tel/Zalo: 0522930490"
Middle Data Engineer,Global Fashion Group,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/middle-data-engineer-at-global-fashion-group-4306185198?position=5&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=WIgESyLqc%2FOkIoBKMi88zg%3D%3D,"About Global Fashion Group
Global Fashion Group is the leading fashion and lifestyle destination in growth markets across LATAM, SEA and ANZ. From our people to our customers and partners, we exist to empower everyone to express their true selves through fashion. Our three e-commerce platforms: Dafiti, ZALORA and THE ICONIC connect an assortment of international, local and own brands to over 800 million consumers from diverse cultures and lifestyles. GFG’s platforms provide seamless and inspiring customer experiences from discovery to delivery, powered by art & science that is infused with unparalleled local knowledge. As part of the Group’s vision is to be the #1 online destination for fashion & lifestyle in growth markets, we are committed to doing this responsibly by being people and planet positive across everything we do.
Since launching in 2011, THE ICONIC has redefined the future of retail in Australia and New Zealand. As the leading fashion, sports and lifestyle e-commerce destination in the region, our e-commerce platforms (Retail, Marketplace and Services) provide a seamless and inspiring end-to-end customer experience through our own technology innovations. We stand for benchmark-setting customer service, delivery options, returns policies, and curation of brands.
We are a diverse and dynamic community of over 1,000 people working towards our purpose “To bring on the future of shopping”. THE ICONIC is people and planet positive, and we strive towards creating a positive impact in the world by driving genuine and meaningful change for the better of all communities involved.
For more information visit: www.global-fashion-group.com
At GFG, Technology is driven by innovation and quality is highly valued. Our data team is the driving force behind our business strategies and decisions. We are integrated into all departments to ensure all employees at THE ICONIC have access to good quality and timely data.
Our Data Engineering team solves complex problems and delivers data to propel our business forward, powering the insights that are used in every decision we make. We are the engineers, the builders, the maintenance people for all our business data.
SCOPE OF ROLE
Develop and support our enterprise data warehouses, analytical databases and infrastructure
Work with the team to re-platform our existing data architecture to next generation tooling and data architecture
Build/Maintain our data pipelines in Python and SQL to ensure that data is delivered in a timely manner
Work closely with our Data Scientists and Data Analysts to implement new insights and statistical models
Assist in developing tools/processes to enable our business to self serve
What It Takes
Both AWS and GCP
BigQuery, SQL Server and Redshift
Docker & Kubernetes
Airflow, Cloud Composer and Pentaho
Cloud Dataflow/Apache Beam
High velocity streaming data, behavioural data and well as structured and unstructured data
Extensive programming knowledge of Data processing within Python
Dependency management within Python
Iterators, producers and consumer knowledge
Airflow DAG building
CI/CD - Bamboo Deployment / Delivery Experience
Strong SQL coding skills
Advanced Data Engineering Design Skills
Excellent communication skills
Significant experience in a similar Data Engineering role
Strong data warehousing/Data Engineering experience
Experience with data modelling and complex ETL solutions
Test and QA Experience in Data Pipelines
Ways to stand out from the crowd
GCP experience
BigQuery/Redshift
Cube/SSAS Experience
DevOps/CICD
What We Offer You
The ability to cross train into DevOps/Platforms or Data Engineering
The unique opportunity to have a serious impact on a growing organisation
A dynamic working environment shaping the face of fashion e-commerce in growth markets
Work closely with a global talent pool with international mindset
#1 IT company to work for in Vietnam in 2021
Best practice scaled agile engineering
Amazing office and great culture! Massage chairs, table tennis, video game room, quarterly team events, yearly company trip, end of the year party
Hybrid working environment and work from home set up allowance
Clear career progression plan and support
English classes
Macbook or laptop when you start
Social insurance, medical insurance & AON insurance
13th month salary
15 days of annual leave, 30 days of sick leave/mental health, 1 day of occasion leave
Support for Gym membership
VND 60,000,000 referral reward for successfully referring someone to GFG
Global Fashion Group embraces diversity and equality of opportunity. We are committed to building an inclusive and diverse team representing all backgrounds, with as wide a range of perspectives as possible, and harnessing industry-leading skills. We believe that the more inclusive we are, the better our work will be. We welcome and consider applications to join our team from all qualified candidates, regardless of their characteristics. We comply with all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements."
Data Engineer,Hitachi Consulting ( formerly Information Management Group),"Đà Nang, Da Nang City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-hitachi-consulting-formerly-information-management-group-4305957918?position=6&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=Tsx7jLxZCxK89fr%2Fhf%2FY2Q%3D%3D,"Our Company
Imagine the sheer breadth of talent it takes to bring a better tomorrow closer to today. We don’t expect you to ‘fit’ every requirement – your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.
Design, implement, maintain data models
Support data model and ETL solutions in production.
Build and maintain data pipelines using tools such as NIFI, Airflow, etc.
Evaluate and define KPIs to monitor and manage data quality
Tuning and improving the performance of DB/DWH.
Support data scientists, data analyst to explore data in Data warehouse.
Build and maintain integration data flow provided to other departments.
Maintain update-to-date documentation of data catalog, ETL flows, data dictionary.
Practice sustainable incident response and blameless postmortems.
Participate in end to end engineering solutions with regard to data processing and data delivery and integration, including data processing job/pipeline tool suite, batch framework and platform, micro-service framework and platform.
BASIC QUALIFICATIONS
Skill in one of the following languages Python / Java / Kotlin, and target in Big Data career.
Skill in one of the following technologies: PySpark / Spark / Flink / Airflow
PREFERRED QUALIFICATIONS
Experience in Design, ETL, data modeling and developing SQL database solutions.
Good understanding of data management - data lineage, meta data, data quality, data governance
Have basic understanding about big data, and big data platform.
Knowledge with S3/ Spark/ Jupyter/ Flink is a plus point.
Ability to debug and optimize code and automate routine tasks.
Skills in task and time management, proactive problem solver.
Teamwork and communication skills.
Championing diversity, equity, and inclusion
How We Look After You
We want to help you take care of your today and tomorrow – at home and at work. Which is why we offer industry-leading benefits that go far beyond compensation. That means support, services, and resources that also take care of your holistic health and wellbeing. We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. Here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent).
We’re a global team of innovators, together, we harness engineering excellence and passion for creating meaningful solutions to complex challenges. We turn organizations into industry leaders that can a make positive impact on their industries and society.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.
Fostering innovation through diverse perspectives
Hitachi is a global company operating across a wide range of industries and regions. One of the things that sets Hitachi apart is the diversity of our business and people, which drives our innovation and growth.
We are committed to building an inclusive culture based on mutual respect and merit-based systems. We believe that when people feel valued, heard, and safe to express themselves, they do their best work.
How We Look After You
We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status
or any other protected characteristic.
Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success."
Data Engineer-Upto 50M-Signing bonus 20M,SETA International,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/data-engineer-upto-50m-signing-bonus-20m-at-seta-international-4308204019?position=7&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=Lzt4VeioshKxoNFHlQ%2FmvA%3D%3D,"SETA INTERNATIONAL VIETNAM- RECRUITMENT
Job Title: Data Engineer
Responsibilities:
Automate and optimize data pipelines to ensure efficient and scalable data flow.
Perform ETL tasks for the data warehouse and optimize queries for data processing.
Conceptualize and build infrastructure to enable big data access and analysis.
Prepare and analyze raw data for manipulation by data scientists, and interpret trends and patterns.
Develop data tools to support analytics and data science teams.
Work with cloud infrastructure (preferably GCP) for deployment and scaling.
Explore and evaluate new data products and technologies.
Build and maintain analytic dashboards and reports.
Requirements:
Experience:
At least 3 years of experience as a Data Engineer.
Good communication skills in English.
Strong experience with cloud platforms (
GCP preferred
), including core services for data storage, processing, and analytics.
Big Data & Streaming:
Hands-on experience designing and implementing scalable data pipelines for both batch and real-time data. Experience with
Programming:
– must have
– nice to have
– understanding or willingness to learn for data ingestion from various sources
Strong skills in writing complex queries for data extraction, transformation, and analysis.
Workflow Orchestration:
Experience with
Apache Airflow
for building, scheduling, and maintaining data workflows.
Version Control:
Proficient with
and collaborative development workflows.
System Skills:
Comfortable working with the
Linux command line
for operations, scripting, and automation.
Familiarity with using AI tools to accelerate coding and improve productivity.
Soft Skills:
Strong leadership and self-motivation, with the ability to inspire and drive the team forward.
Strong analytical and requirement-gathering skills, able to translate business needs into technical solutions.
Critical thinking and problem-solving abilities.
Open-minded and growth-oriented: eager to learn, adapt, and accept feedback.
Detail-oriented, proactive, and able to perform well under pressure.
Benefits
Salary: Negotiate
An international, professional, young, but innovative and dynamic environment working closely with international experts and joining conferences and workshops on exciting new technologies.
Full benefits for employees according to the Vietnam Labor Laws: social and health insurance
Holidays based on the Vietnamese labor law + paid vacations
Opportunity to be onsite in the US
Contact
SETA International Viet Nam
: HL Tower, 82 Duy Tan, Ha Noi
hr@setacinq.vn
https://www.seta-international.com/
https://www.facebook.com/SETA.International.careers/"
Senior Data Engineer,Grab,Vietnam,https://vn.linkedin.com/jobs/view/senior-data-engineer-at-grab-4298873928?position=8&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=N4F%2FTzVR27rD%2BEm2QJVhew%3D%3D,"Company Description
About Grab and Our Workplace
Grab is Southeast Asia's leading superapp. From getting your favourite meals delivered to helping you manage your finances and getting around town hassle-free, we've got your back with everything. In Grab, purpose gives us joy and habits build excellence, while harnessing the power of Technology and AI to deliver the mission of driving Southeast Asia forward by economically empowering everyone, with heart, hunger, honour, and humility.
Job Description
Get to Know the Team
We maintain Grab's data infrastructure, a dependable and economical platform that supports internal data processes and company-wide data lake access. This includes managing compute systems such as Apache Spark, Trino, and Starrocks, scheduling via Airflow, and an AWS S3-based storage layer. Our storage solutions utilize modern open-source formats like Apache Iceberg and Delta, in addition to traditional Apache Hive Parquet tables.
We simplify data operations for internal users by providing managed Spark, Starrocks, Trino, and Airflow services. Furthermore, our team collaborates with Grab's Data Catalog team to offer self-service datalake capabilities powered by Datahub. We also partner closely with Grab's AI platforms, ensuring a smooth experience for users in their AI development.
Get to Know the Role
You will support the mission by maintaining and extending the platform capabilities through implementation of new features and continuous improvements. You will also explore new developments in the space and bring them to our platform there by helping the data community at Grab. You will work onsite in Grab Vietnam office, CMC Creative Space, D7, HCMC and report to Data Engineering Manager, who is based in Singapore.
The Critical Tasks You Will Perform
You will maintain and extend the Python/Go/Scala backend for Grab's Airflow, Spark, Trino and Starrocks platform
You will build, modify and extend Python/Scala Spark applications and Airflow pipelines for better performance, reliability, and cost.
You will design and implement architectural improvements for new use cases or efficiency.
You will build platforms that can scale to the 3 Vs of Big Data (Volume, Velocity, Variety)
You will follow various testing best practices and SRE best practices to ensure system stability and reliability.
Qualifications
What Essential Skills You Will Need
Software Engineering, Computer Science, or related undergraduate degree. Proficient in at least one of the following: Python, Go, or Scala and strong appetite to learn other programming languages.
You have 5 or more years of relevant professional experience
Good working knowledge in 3 or more of the following: Airflow, Spark, relational databases (ideally MySQL), Kubernetes, Starrocks, Trino, and backend API implementation and being passionate about learning the others.
Experience with AWS services (S3, EKS, IAM) and infrastructure as code tools like Terraform.
Proficiency in CI/CD tools (Jenkins, GitLab, etc.)
You are to intelligently utilize available AI resources at Grab.
Skills That Are Good To Have
Proficient in Kubernetes with hands of experience with building custom resources using frameworks like kubebuilder.
Proficient in Apache Spark, with good knowledge of resource managers like Yarn, Kubernetes and how spark interacts and work with them
Advanced understanding of Apache Airflow and its working with Celery and/or Kubernetes executor backend with exposure to Python SQLAlchemy framework.
Advanced knowledge of other query engines like Trino, Starrocks and others
Advanced knowledge of AWS Cloud
Good understanding of lakehouse table formats like Iceberg and Delta lake, how query engines work with it.
Additional Information
Life at Grab
We care about your well-being at Grab, here are some of the global benefits we offer:
We have your back with Term Life Insurance and comprehensive Medical Insurance.
With GrabFlex, create a benefits package that suits your needs and aspirations.
Celebrate moments that matter in life with loved ones through Parental and Birthday leave, and give back to your communities through Love-all-Serve-all (LASA) volunteering leave
We have a confidential Grabber Assistance Programme to guide and uplift you and your loved ones through life's challenges.
Balancing personal commitments and life's demands are made easier with our FlexWork arrangements such as differentiated hours
What We Stand For At Grab
We are committed to building an inclusive and equitable workplace that enables diverse Grabbers to grow and perform at their best. As an equal opportunity employer, we consider all candidates fairly and equally regardless of nationality, ethnicity, religion, age, gender identity, sexual orientation, family commitments, physical and mental impairments or disabilities, and other attributes that make them unique."
"Data Engineer (good EN, upto $2500)",FPT Software Innovation,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-good-en-upto-%242500-at-fpt-software-innovation-4311255057?position=9&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=BKP%2FFndapPAq56oKFMKuOw%3D%3D,"Position: Data Engineer (nice to have Databricks)
Location: HCMC
Key Responsibilities
• Develop and maintain scalable, production-grade data pipelines
• Support data modeling and integration across various structured data sources
• Contribute to platform migration efforts toward Databricks and Big Data architecture
• Collaborate with analysts, architects, and stakeholders to deliver reusable data assets
• Join Agile ceremonies (standups, sprint planning, retros, etc.)
Required Skills & Experience
Bachelor's degree in IT, Computer Science, Data Science, or related fields
At least 3 years of experience in data engineering with strong SQL and RDBMS knowledge (Oracle, SQL Server)
Advanced SQL skills for complex queries, performance tuning, and data manipulation
Solid programming skills in Python for data processing, automation, and ETL
Experience working with Spark (PySpark/Scala Spark), including building scalable distributed data processing pipelines
Nice to have experience with Databricks (including Unity Catalog is a plus)
Familiarity with Azure, AWS, or GCP cloud platforms
Understanding of data modeling, integration patterns, and pipeline orchestration
Good English communication and teamwork skills; ability to work cross-functionally
Benefits
• “FPT care” health insurance is provided by Petrolimex (PJICO) and is exclusive for FPT employees
• Annual Summer Vacation: follows the company’s policy and starts from May every year
• Salary review 1 time per year
• International, dynamic, and friendly working environment
• Annual leave and working conditions follow Vietnam labor laws
• Other benefits: sponsor for studying and taking international certification exams, sponsor loan interest policy for Fsofter
• Shuttle bus for employees
Contact: Ms. Thu – Talent Acquisition Specialist
Email: Thuthm1@fpt.com | Tel/Zalo: 052293049"
Data Engineer,BJAK,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-bjak-4313234269?position=10&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=EfYTokDhEu3A7JcmfRvzYA%3D%3D,"Transform Language Models into Real-World Applications
We’re building AI systems for a global audience. We are living in an era of AI transition - this new project team will be focusing on building applications to enable more real world impact and highest usage for the world.
This role is a global role with
remote work arrangement
. You’ll work closely with regional teams across product, engineering, operations, infrastructure and data to build and scale impactful AI solutions.
Why This Role Matters
You’ll fine-tune state-of-the-art models, design evaluation frameworks, and bring AI features into production. Your work ensures our models are not only intelligent, but also safe, trustworthy, and impactful at scale.
What You’ll Do
Collect, clean, and preprocess user-generated text and image data for fine-tuning large models
Design and manage scalable data labeling pipelines, leveraging both crowdsourcing and in-house labeling teams
Build and maintain automated datasets for content moderation (e.g., safe vs unsafe content)
Collaborate with researchers and engineers to ensure datasets are high-quality, diverse, and aligned with model training needs
What Is It Like
Likes ownership and independence
Believe clarity comes from action - prototype, test, and iterate without waiting for perfect plans.
Stay calm and effective in startup chaos - shifting priorities and building from zero doesn’t faze you.
Bias for speed - you believe it’s better to deliver something valuable now than a perfect version much later.
See feedback and failure as part of growth - you’re here to level up.
Possess humility, hunger, and hustle, and lift others up as you go.
Requirements
Proven experience preparing datasets for machine learning or fine-tuning large models
Strong skills in data cleaning, preprocessing, and transformation for both text and image data
Hands-on experience with data labeling workflows and quality assurance for labeled data
Familiarity with building and maintaining moderation datasets (safety, compliance, and filtering)
Proficiency in scripting (Python, SQL) and working with large-scale data pipelines
What You’ll Get
Flat structure & real ownership
Full involvement in direction and consensus decision making
Flexibility in work arrangement
High-impact role with visibility across product, data, and engineering
Top-of-market compensation and performance-based bonuses
Global exposure to product development
Lots of perks - housing rental subsidies, a quality company cafeteria, and overtime meals
Health, dental & vision insurance
Global travel insurance (for you & your dependents)
Unlimited, flexible time off
Our Team & Culture
We’re a densed, high-performance team focused on high quality work and global impact. We behave like owners. We value speed, clarity, and relentless ownership. If you’re hungry to grow and care deeply about excellence, join us.
BJAK is Southeast Asia’s #1 insurance aggregator with 8M+ users, fully owned by its employees. Headquartered in Malaysia and operating in Thailand, Taiwan, and Japan, we help millions of users access transparent and affordable financial protection through Bjak.com. We simplify complex financial products through cutting-edge technologies, including APIs, automation, and AI, to build the next generation of intelligent financial systems.
If you're excited to build real-world AI systems and grow fast in a high-impact environment, we’d love to hear from you."
Data Engineer,ĐÚNG NGƯỜI ĐÚNG VIỆC Community,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-%C4%91%C3%BAng-ng%C6%B0%E1%BB%9Di-%C4%91%C3%BAng-vi%E1%BB%87c-community-4313656496?position=11&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=1LifCe5h%2BmEBWwVQeMBonw%3D%3D,"Mô tả công việc
Trách Nhiệm Thiết Yếu
Xây dựng và duy trì pipeline dữ liệu phục vụ cho mô hình tín dụng
Đảm bảo dữ liệu được thu thập, đồng bộ và quản lý trong Data Lakehouse ổn định, bảo mật và tuân thủ quy định pháp lý
Công Việc Thiết Yếu
Thiết kế pipeline ETL/ELT để tích hợp dữ liệu tài chính và phi tài chính từ nhiều nguồn
Đồng bộ dữ liệu vào Data Lakehouse phục vụ các mô hình tín dụng
Tối ưu hiệu suất pipeline, độ tươi dữ liệu và chi phí hạ tầng
Đảm bảo dữ liệu chính xác, đầy đủ và tuân thủ SLA
Yêu cầu công việc
Trình Độ Học Vấn
Cử nhân về Công nghệ Thông tin, Khoa học dữ liệu hoặc Kỹ thuật phần mềm
Kinh Nghiệm Làm Việc
3–5 năm kinh nghiệm trong Data Engineering, quản lý pipeline dữ liệu
Kinh nghiệm với hệ thống Data Lake/DWH/Lakehouse
Kiến Thức Và Kỹ Năng
Thành thạo SQL, Python, Spark và Kafka
Kinh nghiệm với cloud platforms (AWS, GCP, Azure)
Am hiểu Data Quality và Data Governance
Kinh nghiệm tích hợp dữ liệu cho mô hình tín dụng
Hiểu biết về DataOps và data pipelines
Tại sao bạn yêu thích làm việc tại đây
Tiên phong công nghệ, uy tín
MISA là doanh nghiệp CNTT xuất sắc nhất khu vực Châu Á - Châu Đại Dương. Tiên phong xuất khẩu giải pháp SaaS
TOP đầu doanh nghiệp CNTT tăng trưởng liên tục với quy mô nhân sự tăng 20%/năm, doanh thu tăng 15%/năm
Hội tụ 3000 nhân tài cùng khát vọng đưa sản phẩm công nghệ “Make In Vietnam” vươn tầm quốc tế
Xây dựng niềm tin với 270.000 khách hàng là đơn vị HCSN, doanh nghiệp, 2.5 triệu khách hàng cá nhân tại Việt Nam và 20 quốc gia
Hơn 100 giải thưởng trong ngành CNTT trong nước và quốc tế
Nền tảng vững chắc cho phát triển sự nghiệp, thăng tiến, quyền lợi
Lương cứng cạnh tranh. Thưởng năng suất dựa trên kết quả công việc từ 2 tháng lương.
Đánh giá review lương 2 lần/năm, thưởng sáng kiến...
Huấn luyện “Hổ tướng”: chương trình đào tạo quản lý tài năng, bệ phóng trở thành Chiến tướng tinh nhuệ
Giải thưởng “Gấu vàng"": nơi tôn vinh những tài năng xuất sắc nhất
Gói chăm sóc sức khỏe toàn diện tại Medlatec, cháy hết mình tại các CLB theo sở thích, chương trình teambuilding, du lịch định kỳ
Môi trường thân thiện, chia sẻ, đồng hành
Kết nối tài năng: tập trung phát triển những con người có chung lý tưởng, mục tiêu, cùng trao giá trị và nhận thành công
Tư duy đột phá: môi trường tôn trọng sự khác biệt và đề cao sáng tạo, MISA-er được tự do phát triển các ý tưởng tiến bộ, cải tiến công việc
Công nghệ cao: trang bị máy tính làm việc, tối ưu hiệu suất công việc bằng ứng dụng công nghệ, phần mềm tự động (AMIS, Jira, Power BI, AI Marketing,...)
Nơi làm việc hạnh phúc: MISA mong muốn tạo một môi trường làm việc để bạn luôn cảm thấy hạnh phúc
Thời gian làm việc
Thứ 2 - Thứ 6 (từ 08:00 đến 17:30)"
Data & CMMS Engineer,Nghi Son Refinery and Petrochemical Limited Liability Company,"Thanh Hoa, Thanh Hoa, Vietnam",https://vn.linkedin.com/jobs/view/data-cmms-engineer-at-nghi-son-refinery-and-petrochemical-limited-liability-company-4260002707?position=12&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=fbCRwSgvKm1zjcY4yRPQNA%3D%3D,"Category Data & CMMS Engineer Location Thanh Hoa Number of vacancies 1 Responsibilities
The Data and CMMS Engineer is responsible for maintaining and updating a comprehensive asset register, ensuring data accuracy in the CMMS (Maximo), and supporting maintenance strategies through the analysis and reporting of reliability and maintenance data. He also coordinates closely with planners, supervisors, and reliability teams to ensure maintenance data (PM, worklog, failure code/ mode, job card, downtime, etc,…) are input/ updated on the system timely and accurately.
Detailed responsibilities of the Data & CMMS Engineer are as follows:
Asset register:
Establish a comprehensive register (Maintenance strategies for new assets);
Perform Storage of good quality technical information for asset;
Conduct Maximo Asset data (ECA, description, specification) changes;
Implement Maximo – Maintenance process changes.
Reliability Data:
Statistic, analyze, and report Breakdown maintenance cost on CMMS system;
Statistic, analyze, and report Breakdown maintenance failure code on CMMS system;
Statistic, analyze, and report PM compliance data on CMMS system;
Statistic, analyze, and report PM cost on CMMS system;
Statistic, analyze, and report Equipment downtime;
Statistic, analyze, and report System downtime;
Conduct Pareto statistics;
Report Work priority status.
Maintenance Work execution:
Coordinate with supervisor to ensure work log and actuals work are correct. Data should input in WO within 48hour for BM;
Coordinate with planner to ensure failure code input is correct;
Coordinate with planner/ scheduler to update PM status. PM can conduct as schedule & plan;
Coordinate with supervisor to ensure work log and actuals work are correct. Data should input in WO within 48hours;
Coordinate with reliability engineer, operation & refinery planning to update equipment & process system downtime, including.
Downtime to repair as plan; Downtime due to breakdown maintenance (time to repair, time for logistic of material & service, etc.); Downtime due to system, unit trip.
Requirements
Qualification: Graduated University or above on the areas of Computer Science, Information Technology, Mechanical Engineering, or relevant.
Work Experience: Above 05 years of experience in Computer Science, Information Technology, Mechanical Engineering with high CMMS proficiency or relevant.
Skills:
Well proficient on CMMS software;
Capable to configure and optimize CMMS for data tracking and reporting;
Experienced in SQL and database management;
Capable to manage and manipulate large datasets;
Knowledgeable with reliability functions including RCM, FMECA, RCA, ECA and Asset register hierarchy;
Strong skills in data collection, validation, and analysis;
Proficiency in statistical analysis software;
Fluently read, write, and speak English.
Benefits
High limit of medical and personal accident insurance for the employee and children;
Personal Income Tax 50% deduction;
Accommodation & Transportation to work will be provided by the Company;
Free meal at site canteen (lunch)."
[Junior/Middle]_Data Engineer (GCP),MTI Technology,Ho Chi Minh City Metropolitan Area,https://vn.linkedin.com/jobs/view/junior-middle-data-engineer-gcp-at-mti-technology-4308678333?position=13&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=3OiL7TAOYxolCkE5WFRPQQ%3D%3D,"About the job:
As a Junior Data Engineer, you will be part of a team responsible for building and maintaining our data infrastructure on the Google Cloud Platform (GCP). You will focus on assisting with the development, testing, and monitoring of data pipelines, ensuring data quality, and supporting data-driven initiatives under the guidance of senior engineers. This is an excellent opportunity to learn and grow your skills in cloud-based data engineering within a dynamic global company.
Key Responsibilities:
Assist in developing, testing, and deploying ETL/ELT processes to ingest and transform data from various sources into our GCP data warehouse/lake.
Write and optimize SQL queries for data extraction, transformation, and loading.
Monitor existing data pipelines and troubleshoot basic issues under supervision.
Support senior engineers in maintaining data quality and integrity.
Help document data pipelines, processes, and data models.
Learn and utilize various GCP data services (e.g., BigQuery, Cloud Storage, Dataflow basics).
Collaborate with team members and other IT professionals.
Required Skills and Qualifications:
Bachelor's degree in Computer Science, Engineering, Information Technology, or a related field, or equivalent practical experience.
Basic understanding of data warehousing, data lakes, and ETL/ELT concepts.
Foundational knowledge of SQL.
Familiarity with at least one programming language (e.g., Python, Java).
Eagerness to learn GCP data services and data engineering best practices.
Good problem-solving skills and attention to detail.
Ability to work effectively in a team environment.
Experience in LookML/Looker Studio is a plus
Nice to have:
Some exposure to cloud platforms, preferably GCP.
Experience with version control systems (e.g., Git).
Basic understanding of data modeling concepts.
Familiarity with the logistics or maritime industry.
Good verbal and written communication skills in English.
What We Offer
100% salary during probation
3 days of WFH per week (based on team's decision)
Lunch + Gasoline + Coffee Allowance
Health, Social and Unemployment Insurance (based on gross-based salary, according to Labor Code) and PVI Health Insurance
13th-month salary and Performance bonus
Annual salary review
12 days annual leave plus extra 02 days company leave
Company trips, sponsored team building, monthly Happy Hour, Sport Clubs (Soccer, Badminton, Pingpong, Yoga) and other joyful events;
A culture of relentless learning with free courses in specialized skills, soft skills, and English;
Yearly health-checkup;
Seniority benefits: allowance & PVI Health Insurances for family members
Technical-certificate bonus
Japanese-certificate bonus
Employee Referral Incentive"
Data Engineer-Upto 50M,SETA International Careers,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/data-engineer-upto-50m-at-seta-international-careers-4313574820?position=14&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=n7zscGqDQn321OUs5haI1w%3D%3D,"SETA INTERNATIONAL VIETNAM- RECRUITMENT
Job Title: Data Engineer
Responsibilities:
Automate and optimize data pipelines to ensure efficient and scalable data flow.
Perform ETL tasks for the data warehouse and optimize queries for data processing.
Conceptualize and build infrastructure to enable big data access and analysis.
Prepare and analyze raw data for manipulation by data scientists, and interpret trends and patterns.
Develop data tools to support analytics and data science teams.
Work with cloud infrastructure (preferably GCP) for deployment and scaling.
Explore and evaluate new data products and technologies.
Build and maintain analytic dashboards and reports.
Requirements:
Experience:
At least 3 years of experience as a Data Engineer.
Good communication skills in English.
Strong experience with cloud platforms (
GCP preferred
), including core services for data storage, processing, and analytics.
Big Data & Streaming:
Hands-on experience designing and implementing scalable data pipelines for both batch and real-time data. Experience with
Programming:
– must have
– nice to have
– understanding or willingness to learn for data ingestion from various sources
Strong skills in writing complex queries for data extraction, transformation, and analysis.
Workflow Orchestration:
Experience with
Apache Airflow
for building, scheduling, and maintaining data workflows.
Version Control:
Proficient with
and collaborative development workflows.
System Skills:
Comfortable working with the
Linux command line
for operations, scripting, and automation.
Familiarity with using AI tools to accelerate coding and improve productivity.
Soft Skills:
Strong leadership and self-motivation, with the ability to inspire and drive the team forward.
Strong analytical and requirement-gathering skills, able to translate business needs into technical solutions.
Critical thinking and problem-solving abilities.
Open-minded and growth-oriented: eager to learn, adapt, and accept feedback.
Detail-oriented, proactive, and able to perform well under pressure.
Benefits
Salary: Negotiate
An international, professional, young, but innovative and dynamic environment working closely with international experts and joining conferences and workshops on exciting new technologies.
Full benefits for employees according to the Vietnam Labor Laws: social and health insurance
Holidays based on the Vietnamese labor law + paid vacations
Opportunity to be onsite in the US
Contact
SETA International Viet Nam
: HL Tower, 82 Duy Tan, Ha Noi
hr@setacinq.vn
https://www.seta-international.com/
https://www.facebook.com/SETA.International.careers/"
Data engineer,Techvify,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-techvify-4295936255?position=15&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=IRZ5i5ND2yI0Era7LzbrtQ%3D%3D,"Địa điểm làm việc
TECHVIFY Corporation is an End to End AI-Powered Digital Transformation Partner.
At TECHVIFY, we don’t just build software.
We engineer breakthroughs.
We innovate with AI, craft with code, and scale with cloud.
We partner with startups, enterprises, and even competitors on a shared mission:
To turn bold ideas into real-world impact.
If you’re looking to lead digital transformation through intelligent software, we’re ready to build with you.
Let’s create the future – together.
Responsibilities
Design, develop, and maintain data pipelines and ETL processes to support data analytics and reporting.
Collaborate with stakeholders to understand data requirements and deliver effective solutions.
Ensure data quality and integrity through rigorous testing and validation processes.
Optimize and troubleshoot existing data workflows for performance improvements.
Participate in project planning and contribute to the development of data architecture strategies.
Stay updated with industry trends and best practices in data engineering and analytics.
Requirements
Bachelor’s degree in Computer Science, Information Technology, or a related field.
At least 2+ years in this position
Proficient in SQL and Python, with experience in Pyspark.
Understanding of data modeling, ETL processes, and data warehousing concepts.
Experience with data analysis and visualization tools.
Ability to communicate effectively in English, both written and verbal.
Familiarity with big data technologies is a plus.
Benefits
Salary: Upto 30M
Join a global team and work directly with many talents around the world.
Opportunities for onsite work in Japan, Singapore, Australia, and many other countries.
Work and grow in a dynamic, creative, and professional environment.
Healthcare: Premium Health Insurance TECHVIFY Care
13 months’ salary per year.
Annual salary evaluation.
Sponsor and encourage staff to study courses by covering tuition fees, such as Udemy, Coursera.
If you are a driven and talented Data Engineer looking for an opportunity to be part of something extraordinary, please submit your resume and cover letter for consideration.
Join our team and let’s create amazing things together! Send your updated CV to this email address:
Email: careers@techvify.com.vn
About us: Techvify | End-to-End AI-Powered Digital Transformation Partner
Vui lòng nhập họ tên của bạn
Vui lòng nhập email của bạn
Vui lòng nhập số điện thoại của bạn
Vị trí bạn quan tâm Java Engineer Data Scientist Senior Recruiter Intern AI Solution Engineer (Japanese) Senior Data engineer .Net Engineer Mid/Senior Python Engineer Data engineer AI Engineer Senior Backend Engineer (Java/Golang) Technical Leader - Python/AI Technical Lead QA Engineer Senior Recruiter (Japan Market) Automation QA Manager Software Engineer for JP market BrSE for JP Market DevOps Engineer Account Manager Senior BE in Python Django Quality Assurance Leader SRE Lead Application Managed Service L2 Senior Java Developer Senior PHP Developer Senior International Sales Executive Business Analyst Korea Sales Manager Senior PQA Lead Intern AI Solution Engineer Sales Executive (Japan Market) Devops Engineer Python Developer Mobile Developer Technical Project Manager Frontend Developer Senior Solutions Architect / Consultant Business Analyst/ Product Owner Intern Project Coordinator NodeJS Developer AI Engineer Golang Developer Senior Frontend Developer Product Quality Assurance Senior International Sales Frontend Developer Intern Sales IT Quality Assurance Engineer Senior Nodejs Developer Middle Fullstack Developer Senior C# Developer Fresher Japanese Content Creator (EB) IT Helpdesk Senior Ruby on Rails Developer Business Analyst Manual Tester Middle/Senior Auto QA Engineer Senior Frontend Developer (ReactJS) Project Manager/ Scrum Master Sales Manager Sales Manager - HN Content Marketing Specialist (Creative) Intern Sales - HCM Senior Software Engineer Project Manager Junior/Middle Quality Assurance Intern AI Engineer Senior UI/UX Designer Branding Executive (EB) Intern Fullstack/ReactJs/Python/Go Developer Intern ReactJS Developer Intern Fullstack (NodeJS, ReactJS) Developer Middle/Senior Backend Developer (NodeJS/Golang) onsite Singapore Junior/Middle PHP (Wordpress) Developer Mid-Senior Front-end Developer onsite Singapore Process Quality Assurance Manager Junior/Middle Java Developer Junior/Middle Golang Developer Junior/Middle .NET Developer Intern Employer Branding Junior/Middle NodeJs Developer Nhân viên Đào tạo Nội bộ Product Owner Support Engineering Junior/Middle Business Analyst Senior Java Developer Graphic Designer [Junior/Middle] Senior Quality Assurance Scrum Master/Project Manager [Intern/ Fresher] Marketing Manager Marketing Manager Sales Development Representative Senior Account Executive Head Of Account Management Senior Partner Sales Manager Senior Solution Sales Executive Senior Sales Executive Middle/Senior Python Developer Automation Test Engineer Talent Acquisition Specialist Talent Acquisition Intern Chuyên viên Đào tạo Nội bộ Quality Assurance [Junior/Middle] Senior Fullstack Developers IT Recruiter [Mid/Senior] Social Content Creator [thị trường Nhật] Front-end Developer [Junior/Middle] IT Recruitment Lead Chuyên viên Truyền thông Nội bộ Social Content Creator Technical Project Manager Delivery Manager [Global Market] Junior/Mid Java Engineer [Onsite] Junior AI Engineer International Sales Executive Senior React Native Engineer Junior Java Engineer Mid/Senior Java Developer Mid/Senior Java Developer Mid-Senior Nodejs Developer Junior/Mid-level .NET Software Engineer Junior /Middle Business Analyst Financial Assistant [Onsite] Junior ReactJS Developer Intern Golang Developer Senior IT Content Creator (Onsite/ Remote) Accountant Intern Office Administrator Human Resource Manager (HRM) Middle/Senior Quality Assurance ReactJS Developer Middle ReactJS Developer (Junior/Mid) Digital Marketing Executive Delivery Manager (Japan Market) Recruitment Lead Senior FullStack Developer Đà Nẵng Senior Flutter (Mobile) Golang Developer [Senior/Lead] Fresher Golang Solution Architect Digital Marketing Lead (Onsite/Remote) Sales Manager [Hanoi & HCMC] Business Development Interns [Hanoi & HCMC] Senior Sales Executive (Global Market) Senior Node.Js Developer DevOps Engineer (Azure) Quản lý chất lượng dự án phần mềm (PQA) Mid/Senior Java Developer [Japan] Senior/ Middle C# Developer [Japan] Senior/ Middle C/C++ Developer [Japan] IT Project Manager Middle/Senior Golang Developer Senior Sales Executive (Japan Market) Senior Java Developer Mid-Senior Java Developer Java Technical Leader Bridge System Engineer (BrSE) - Japan Senior .NET /C# Developer Middle/Senior Python Developer Performance Marketing Specialist Sales Assistant (Graduate - Junior) Senior Vue.Js Developer Senior ReactJs Developer PHP Team Leader Senior PHP Developer Chuyên Viên IT Recruiter Junior PHP Developer Automation QA Vị trí khác
Data engineer
Vị trí bạn quan tâm
Java Engineer
Data Scientist
Senior Recruiter
Intern AI Solution Engineer (Japanese)
Senior Data engineer
.Net Engineer
Mid/Senior Python Engineer
Data engineer
AI Engineer
Senior Backend Engineer (Java/Golang)
Technical Leader - Python/AI
Technical Lead
QA Engineer
Senior Recruiter (Japan Market)
Automation QA Manager
Software Engineer for JP market
BrSE for JP Market
DevOps Engineer
Account Manager
Senior BE in Python Django
Quality Assurance Leader
Application Managed Service L2
Senior Java Developer
Senior PHP Developer
Senior International Sales Executive
Business Analyst
Korea Sales Manager
Senior PQA Lead
Intern AI Solution Engineer
Sales Executive (Japan Market)
Devops Engineer
Python Developer
Mobile Developer
Technical Project Manager
Frontend Developer
Senior Solutions Architect / Consultant
Business Analyst/ Product Owner
Intern Project Coordinator
NodeJS Developer
AI Engineer
Golang Developer
Senior Frontend Developer
Product Quality Assurance
Senior International Sales
Frontend Developer
Intern Sales IT
Quality Assurance Engineer
Senior Nodejs Developer
Middle Fullstack Developer
Senior C# Developer
Fresher Japanese Content Creator (EB)
IT Helpdesk
Senior Ruby on Rails Developer
Business Analyst
Manual Tester
Middle/Senior Auto QA Engineer
Senior Frontend Developer (ReactJS)
Project Manager/ Scrum Master
Sales Manager
Sales Manager - HN
Content Marketing Specialist (Creative)
Intern Sales - HCM
Senior Software Engineer
Project Manager
Junior/Middle Quality Assurance
Intern AI Engineer
Senior UI/UX Designer
Branding Executive (EB)
Intern Fullstack/ReactJs/Python/Go Developer
Intern ReactJS Developer
Intern Fullstack (NodeJS, ReactJS) Developer
Middle/Senior Backend Developer (NodeJS/Golang) onsite Singapore
Junior/Middle PHP (Wordpress) Developer
Mid-Senior Front-end Developer onsite Singapore
Process Quality Assurance Manager
Junior/Middle Java Developer
Junior/Middle Golang Developer
Junior/Middle .NET Developer
Intern Employer Branding
Junior/Middle NodeJs Developer
Nhân viên Đào tạo Nội bộ
Product Owner
Support Engineering
Junior/Middle Business Analyst
Senior Java Developer
Graphic Designer [Junior/Middle]
Senior Quality Assurance
Scrum Master/Project Manager [Intern/ Fresher]
Marketing Manager
Marketing Manager
Sales Development Representative
Senior Account Executive
Head Of Account Management
Senior Partner Sales Manager
Senior Solution Sales Executive
Senior Sales Executive
Middle/Senior Python Developer
Automation Test Engineer
Talent Acquisition Specialist
Talent Acquisition Intern
Chuyên viên Đào tạo Nội bộ
Quality Assurance [Junior/Middle]
Senior Fullstack Developers
IT Recruiter [Mid/Senior]
Social Content Creator [thị trường Nhật]
Front-end Developer [Junior/Middle]
IT Recruitment Lead
Chuyên viên Truyền thông Nội bộ
Social Content Creator
Technical Project Manager
Delivery Manager [Global Market]
Junior/Mid Java Engineer [Onsite]
Junior AI Engineer
International Sales Executive
Senior React Native Engineer
Junior Java Engineer
Mid/Senior Java Developer
Mid/Senior Java Developer
Mid-Senior Nodejs Developer
Junior/Mid-level .NET Software Engineer
Junior /Middle Business Analyst
Financial Assistant [Onsite]
Junior ReactJS Developer
Intern Golang Developer
Senior IT Content Creator (Onsite/ Remote)
Accountant Intern
Office Administrator
Human Resource Manager (HRM)
Middle/Senior Quality Assurance
ReactJS Developer Middle
ReactJS Developer (Junior/Mid)
Digital Marketing Executive
Delivery Manager (Japan Market)
Recruitment Lead
Senior FullStack Developer Đà Nẵng
Senior Flutter (Mobile)
Golang Developer [Senior/Lead]
Fresher Golang
Solution Architect
Digital Marketing Lead (Onsite/Remote)
Sales Manager [Hanoi & HCMC]
Business Development Interns [Hanoi & HCMC]
Senior Sales Executive (Global Market)
Senior Node.Js Developer
DevOps Engineer (Azure)
Quản lý chất lượng dự án phần mềm (PQA)
Mid/Senior Java Developer [Japan]
Senior/ Middle C# Developer [Japan]
Senior/ Middle C/C++ Developer [Japan]
IT Project Manager
Middle/Senior Golang Developer
Senior Sales Executive (Japan Market)
Senior Java Developer
Mid-Senior Java Developer
Java Technical Leader
Bridge System Engineer (BrSE) - Japan
Senior .NET /C# Developer
Middle/Senior Python Developer
Performance Marketing Specialist
Sales Assistant (Graduate - Junior)
Senior Vue.Js Developer
Senior ReactJs Developer
PHP Team Leader
Senior PHP Developer
Chuyên Viên IT Recruiter
Junior PHP Developer
Automation QA
Vị trí khác
Vui lòng chọn vị trí tuyển dụng bạn quan tâm
Tải lên CV của bạn
(Hãy tải lên CV của bạn ở định dạng .doc .docx .pdf không quá 5MB)
Chọn file hoặc kéo thả vào đây
Vui lòng tải CV của bạn
Bạn biết đến thông tin tuyển dụng của TECHVIFY qua kênh nào?
Bạn biết đến thông tin tuyển dụng của TECHVIFY qua kênh nào?
Group facebook
Fanpage TECHVIFY Careers
Google Search
Tiktok Techvify Official
Tiktok Celine Nguyen
Tiktok Thedev_dad
Chuyên viên tuyển dụng Techvify
Youtube TECHVIFY Careers
Vui lòng chọn Bạn biết đến thông tin tuyển dụng của TECHVIFY qua kênh nào?
Gửi CV Qua Email
careers@techvify.com.vn
Liên Hệ Số Điện Thoại
02477760688"
Data Analytics Engineer,Colgate-Palmolive,"Bến Cát, Binh Duong, Vietnam",https://vn.linkedin.com/jobs/view/data-analytics-engineer-at-colgate-palmolive-4281356935?position=16&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=EWnpd7JjQME84XH2BVdlNw%3D%3D,"No Relocation Assistance Offered
Job Number #168531 - Ben Cat town, Binh Duong, Viet Nam
Colgate-Palmolive Company is a global consumer products company operating in over 200 countries specialising in Oral Care, Personal Care, Home Care, Skin Care, and Pet Nutrition. Our products are trusted in more households than any other brand in the world, making us a household name!
Join Colgate-Palmolive, a caring, innovative growth company reimagining a healthier future for people, their pets, and our planet. Guided by our core values—Caring, Inclusive, and Courageous—we foster a culture that inspires our people to achieve common goals. Together, let's build a brighter, healthier future for all.
The Data Analytics Engineer at My Phuoc Plant is responsible for using analytical tools to uncover meaningful insights, trends, and patterns that directly contribute to informed business decisions.
Design, build, and maintain robust, scalable, and efficient ETL/ELT pipelines for manufacturing data from various systems (MES, Historian, PLCs, SAP, IoT, Google drive, Snowflake) into data lakes/warehouses/analytical platforms.
Develop and optimize data models within platforms like Snowflake, Databricks, or Google BigQuery to support analytics, reporting, and advanced analytics. Ensure data integrity, performance, and scalability for manufacturing data.
Implement automated data quality checks, validation, and monitoring processes in data pipelines for accurate, complete, and consistent manufacturing data. Embed data governance and security best practices.
Collaborate with Data Scientists and Analysts to operationalize analytical and machine learning models by building feature engineering pipelines, deploying models, and monitoring performance/data drift.
Evaluate, select, implement, and manage underlying data infrastructure and tooling (e.g., Airflow, streaming platforms, cloud services, computation engines) for efficient manufacturing data processing and analytics.
Identify and resolve data-related issues, pipeline failures, and performance bottlenecks. Optimize data processing and queries for efficiency, reduced latency, and cost management.
Serve as a technical expert, collaborating with OT, IT, production, engineering, and data consumption teams to understand data requirements, provide solutions, and ensure data availability/reliability.
Responsible for relevant EHS/Quality/FP&R performance expectations as defined in each standard's RACI matrix.
Requirements
Bachelor's degree in Data Analytics, Computer Science, Data Science, Information Technology, or a related discipline.
Minimum of 2 years of hands-on experience with business intelligence tools for data analysis and processing, such as Data Studio, Tableau, Domo, and Sigma.
Proficiency in data fields like SQL, Python, or R, and cloud BI tools such as Domo.
Interpersonal skills of communication, collaboration and problem solving
A strong grasp of supply chain principles, including production planning, inventory, logistics, procurement, and quality management, is vital. Understanding their interaction within the supply chain is essential for data analysis and process optimization.
Proactively seeks and adapts to new technologies with an open mindset.
Our Commitment to Inclusion
Our journey begins with our people—developing strong talent with diverse backgrounds and perspectives to best serve our consumers around the world and fostering an inclusive environment where everyone feels a true sense of belonging. We are dedicated to ensuring that each individual can be their authentic self, is treated with respect, and is empowered by leadership to contribute meaningfully to our business.
Equal Opportunity Employer
Colgate is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, colour, religion, gender, gender identity, sexual orientation, national origin, ethnicity, age, disability, marital status, veteran status (United States positions), or any other characteristic protected by law.
Reasonable accommodation during the application process is available for persons with disabilities. Please complete this request form should you require accommodation."
Data Engineer,ĐÚNG NGƯỜI ĐÚNG VIỆC Community,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-%C4%91%C3%BAng-ng%C6%B0%E1%BB%9Di-%C4%91%C3%BAng-vi%E1%BB%87c-community-4311175213?position=17&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=jUY0SoMBqU9i2gMzcHr2oQ%3D%3D,"Mô tả công việc
Xây dựng & tối ưu hóa hệ thống xử lý dữ liệu
Xây dựng công cụ, framework giúp quản lý, tích hợp nhiều nguồn dữ liệu khác nhau, tạo nền tảng khai thác dữ liệu hiệu quả cho nhóm Data Science và Business.
Phát triển data warehouse hiệu suất cao, nền tảng BI phục vụ phân tích dữ liệu lớn
Kiểm tra, làm sạch và tối ưu hóa dữ liệu, đảm bảo độ chính xác, khả dụng của dữ liệu cho người dùng cuối.
Phát triển nền tảng dữ liệu & giám sát hệ thống
Đánh giá và phát triển công nghệ, giải pháp mới nhằm nâng cao độ tin cậy, khả năng mở rộng và sẵn sàng cao cho hạ tầng dữ liệu.
Làm việc cùng Data Scientist để thiết kế mô hình Machine Learning, phân tích hành vi khách hàng, cải thiện trải nghiệm người dùng.
Thiết kế hệ thống giám sát, cảnh báo, phát hiện lỗi hệ thống, bất thường dữ liệu và tự động hóa quy trình xử lý sự cố.
Xây dựng hệ thống ETL & xử lý dữ liệu
Hỗ trợ nền tảng dữ liệu & tự động hóa
Phối hợp với các bộ phận có liên quan để xây dựng công cụ tự động giúp tối ưu quy trình và tăng tốc pipeline xử lý dữ liệu.
Phát triển phương pháp giải quyết các vấn đề dữ liệu trong toàn bộ vòng đời dữ liệu/dịch vụ.
Xây dựng dashboard, hệ thống giám sát và tự động hóa để phát hiện, xử lý lỗi hệ thống một cách thông minh.
Yêu cầu công việc
Tốt nghiệp cao đẳng, đại học trở lên
Ưu tiên Tốt nghiệp chuyên ngành Khoa học máy tính, Hệ thống thông tin, Kỹ thuật phần mềm hoặc lĩnh vực liên quan.
Giao tiếp tốt, ngoại ngữ tốt, phối hợp công việc tốt
Trên 3 năm kinh nghiệm ở vị trí Data Engineering hoặc vai trò tương tự.
Có kinh nghiệm làm việc với SQL, hệ thống dữ liệu lớn, hệ thống cơ sở dữ liệu, hệ thống phân tán. Kinh nghiệm tối ưu hóa hiệu suất và mở rộng hệ thống.
Thành thạo một hoặc nhiều ngôn ngữ lập trình: Python, Java, Scala, Net, .Net,...
Kinh nghiệm triển khai hệ thống dữ liệu lớn trên Hadoop Ecosystem (Spark, Hadoop, Clickhouse, ELK…).
Có kiến thức về ETL, Data Warehouse (BigQuery, Snowflake, Redshift).
Biết sử dụng các công cụ hỗ trợ quản lý sản phẩm: Jira, Azure, Confluence,...
Tại sao bạn yêu thích làm việc tại đây
Thu nhập: Thỏa thuận theo năng lực
Đầy đủ các chế độ theo luật lao động hiện hành.
Chính sách phúc lợi theo quy định của Công ty đa dạng: Chăm sóc sức khỏe định kì hàng năm; Gói bảo hiểm sức khỏe chuyên biệt (FPT Care – Khám chữa bệnh miễn phí tại tất cả các bệnh viện); Các hoạt động tri ân, chăm lo đời sống tinh thần CBNV và Thân nhân...
Môi trường làm việc thân thiện, cởi mở
Dự án trọng điểm, ứng dụng những công nghệ mới nhất.
Nhiều cơ hội phát triển và thăng tiến.
Văn hóa Doanh nghiệp đặc sắc, sinh động bậc nhất với nhiều các hoạt động hấp dẫn: Tân binh, 72 giờ trải nghiệm, teambuilding, thi trạng, hội làng, hội diễn Sao Chổi, sinh nhật FPT, ngày 08/03, ngày 20/10,....
Địa điểm làm việc
Hà Nội: Toà FPT, số 10 Phạm Văn Bạch, Cầu Giấy
Thời gian làm việc
Thứ 2 - Thứ 6 (từ 08:30 đến 18:00)"
Data Engineer,ĐÚNG NGƯỜI ĐÚNG VIỆC Community,"Phường Chí Minh, Hai Duong, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-%C4%91%C3%BAng-ng%C6%B0%E1%BB%9Di-%C4%91%C3%BAng-vi%E1%BB%87c-community-4313646946?position=18&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=e%2FddGXSclqzUmhFRh5E6KQ%3D%3D,"Mô tả công việc
Làm việc với đội BA để nghiên cứu, phân tích các yêu cầu
Phân tích yêu cầu, thiết kế giải pháp và viết các tài liệu kỹ thuật trong quá trình phát triển
Chủ động nghiên cứu các công nghệ, lĩnh vực, kỹ thuật được phân công hoặc liên quan đến công việc
Địa điểm làm việc
Hồ Chí Minh: Tòa nhà 678, số 67 Hoàng Văn Thái, phường Tân Phú, Quận 7
Thời gian làm việc
Thứ 2 - Thứ 6 (từ 08:30 đến 18:00)
Thứ 7 (từ 08:30 đến 12:00)
Yêu cầu công việc
Trên 3 năm kinh nghiệm làm việc với các công cụ process data: batching (Spark, Pandas) hoặc streaming (Flink, Storm, Spark-Streaming)
Trên 3 năm kinh nghiệm làm việc với (một trong) các công cụ tạo, quản lý data pipeline: Azure Data Factory, Azkaban, Luigi, Airflow v.v.
Có kinh nghiệm làm việc với Linux, Git
Có kinh nghiệm với SQL và NoSQL DB: bao gồm Postgres, sql server, mongodb và elasticsearch.
Tại sao bạn yêu thích làm việc tại đây
Làm việc tại công ty hàng đầu Việt Nam, luôn không ngừng phát triển với nhiều cơ hội thăng tiến bản thân.
Được đào tạo chuyên nghiệp, hoàn toàn miễn phí trước khi làm việc.
Môi trường làm việc trẻ, năng động và thân thiện.
Tham gia đầy đủ các chế độ BHYT, BHXH, BHTN.
Thu nhập hấp dẫn, phù hợp năng lực bản thân.
Lương tháng 13 và thưởng theo hiệu quả kinh doanh.
Khám sức khỏe định kỳ.
Thường xuyên tổ chức các chương trình hội thao, hội diễn văn nghệ, tân niên, tất niên…"
Data Engineer,Hitachi Digital Services,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-hitachi-digital-services-4304541383?position=19&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=5qpE3YdCBF2bGA%2FUfxCjyg%3D%3D,"Our Company
Imagine the sheer breadth of talent it takes to bring a better tomorrow closer to today. We don’t expect you to ‘fit’ every requirement – your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.
Design, implement, maintain data models
Support data model and ETL solutions in production.
Build and maintain data pipelines using tools such as NIFI, Airflow, etc.
Evaluate and define KPIs to monitor and manage data quality
Tuning and improving the performance of DB/DWH.
Support data scientists, data analyst to explore data in Data warehouse.
Build and maintain integration data flow provided to other departments.
Maintain update-to-date documentation of data catalog, ETL flows, data dictionary.
Practice sustainable incident response and blameless postmortems.
Participate in end to end engineering solutions with regard to data processing and data delivery and integration, including data processing job/pipeline tool suite, batch framework and platform, micro-service framework and platform.
BASIC QUALIFICATIONS
Skill in one of the following languages Python / Java / Kotlin, and target in Big Data career.
Skill in one of the following technologies: PySpark / Spark / Flink / Airflow
PREFERRED QUALIFICATIONS
Experience in Design, ETL, data modeling and developing SQL database solutions.
Good understanding of data management - data lineage, meta data, data quality, data governance
Have basic understanding about big data, and big data platform.
Knowledge with S3/ Spark/ Jupyter/ Flink is a plus point.
Ability to debug and optimize code and automate routine tasks.
Skills in task and time management, proactive problem solver.
Teamwork and communication skills.
Championing diversity, equity, and inclusion
How We Look After You
We want to help you take care of your today and tomorrow – at home and at work. Which is why we offer industry-leading benefits that go far beyond compensation. That means support, services, and resources that also take care of your holistic health and wellbeing. We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. Here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent).
We’re a global team of innovators, together, we harness engineering excellence and passion for creating meaningful solutions to complex challenges. We turn organizations into industry leaders that can a make positive impact on their industries and society.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.
Fostering innovation through diverse perspectives
Hitachi is a global company operating across a wide range of industries and regions. One of the things that sets Hitachi apart is the diversity of our business and people, which drives our innovation and growth.
We are committed to building an inclusive culture based on mutual respect and merit-based systems. We believe that when people feel valued, heard, and safe to express themselves, they do their best work.
How We Look After You
We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status
or any other protected characteristic.
Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success."
Data Engineer,GFT Technologies,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/data-engineer-at-gft-technologies-4300632198?position=20&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=KRA%2BRijFOLSlQcDJD8bd9Q%3D%3D,"GFT Technologies is driving the digital transformation of the world’s leading financial institutions. Other sectors, such as industry and insurance, also leverage GFT’s strong consulting and implementation skills across all aspects of pioneering technologies, such as cloud engineering, artificial intelligence, the Internet of Things for Industry 4.0, and blockchain.
With its in-depth technological expertise, strong partnerships and scalable IT solutions, GFT increases productivity in software development. This provides clients with faster access to new IT applications and innovative business models, while also reducing risk.
We’ve been a pioneer of near-shore delivery since 2001 and now offer an international team spanning 16 countries with a global workforce of over 9,000 people around the world. GFT is recognised by industry analysts, such as Everest Group, as a leader amongst the global mid-sized Service Integrators and ranked in the Top 20 leading global Service Integrators in many of the exponential technologies such as Open Banking, Blockchain, Digital Banking, and Apps Services.
Sign-on Bonus:
Eligible for candidates who are currently employed elsewhere and able to join GFT
within 30 days
of offer acceptance.
Role Summary
As a Data Engineer at GFT, you will play a pivotal role in designing, maintaining, and enhancing various analytical and operational services and infrastructure crucial for the organization's functions. You'll collaborate closely with cross-functional teams to ensure the seamless flow of data for critical decision-making processes.
Key Activities
Data Infrastructure Design and Maintenance:
Architect, maintain, and enhance analytical and operational services and infrastructure, including data lakes, databases, data pipelines, and metadata repositories, to ensure accurate and timely delivery of actionable insights.
AWS Glue Development:
Experience in AWS Glue data pipeline development (not drag-and-drop).
Collaboration: Work closely with data science teams to design and implement data schemas and models, integrate new data sources with product teams, and collaborate with other data engineers to implement cutting-edge technologies in the data space.
Data Processing:
Develop and optimize large-scale batch and real-time data processing systems to support the organization's growth and improvement initiatives.
Workflow Management: Utilize workflow scheduling and monitoring tools like Apache Airflow and AWS Batch to ensure efficient data processing and management.
Quality Assurance: Implement robust testing strategies to ensure the reliability and usability of data processing systems.
Continuous Improvement:
Stay abreast of emerging technologies and best practices in data engineering, and propose and implement optimizations to enhance development efficiency.
Required Skills
4-6 years of experience as a Data Engineer.
Professional
developer with design pattern knowledge.
Experience with production-grade code.
Able to develop quickly, work overtime, and perform under pressure.
Technical Expertise: Proficient in Unix environments, distributed and cloud computing, Python frameworks (e.g., pandas, pyspark), version control systems (e.g., git), and workflow scheduling tools (e.g., Apache Airflow).
Database Proficiency: Experience with columnar and big data databases like Athena, Redshift, Vertica, and Hive/Hadoop.
Cloud Services: Familiarity with
or other cloud services like Glue, EMR, EC2, S3, Lambda, etc.
Containerization: Experience with container management and orchestration tools like Docker, ECS, and Kubernetes.
CI/CD: Knowledge of CI/CD tools such as Jenkins, CircleCI, or AWS CodePipeline.
Nice-to-have Requirements
Programming Languages: Familiarity with JVM languages like Java or Scala.
Database Technologies: Experience with RDBMS (e.g., MySQL, PostgreSQL) and NoSQL databases (e.g., DynamoDB, Redis).
BI Tools: Exposure to enterprise BI tools like Tableau, Looker, or PowerBI.
Data Science Environments: Understanding of data science environments like AWS Sagemaker or Databricks.
Monitoring and Logging: Knowledge of log ingestion and monitoring tools like ELK stack or Datadog.
Data Privacy and Security: Understanding of data privacy and security tools and concepts.
Messaging Systems: Familiarity with distributed messaging and event streaming systems like Kafka or RabbitMQ.
Due to the high volume of applications we receive, we are unable to respond to every candidate individually. If you have not received a response from GFT regarding your application within 10 workdays, please consider that we have decided to proceed with other candidates. We truly appreciate your interest in GFT and thank you for your understanding."
Data Engineer,Sending Labs,Vietnam,https://vn.linkedin.com/jobs/view/data-engineer-at-sending-labs-4288680181?position=21&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=emMefPvN6xXLZmJsCLuTbw%3D%3D,"Building the Web3 communications stack. Sending.Network is the first Web3 native communications protocol supporting fully decentralized, private community chat. Built on Sending.Network, Sending.Me is the next gen communications app for communities large and small
You Will Be Responsible For
Data Pipeline Development: Build and maintain scalable ETL/ELT pipelines for real-time and batch crypto market data; integrate data from APIs, trading venues, blockchain nodes, and third-party platforms.
Infrastructure & Architecture: Design and manage databases, warehouses, and data lakes; leverage cloud-native and distributed processing tools (e.g., AWS, Spark, Flink) to support large-scale workloads.
Operations & Tooling: Monitor, troubleshoot, and optimize pipelines to ensure reliability, performance, and cost efficiency; implement data governance, security, and access control best practices.
Research & Trading Support: Curate, label, and validate datasets for model training, backtesting, and strategy evaluation; prepare high-quality datasets and reports to support quantitative research and trading teams.
Collaboration & Continuous Improvement: Work with researchers and traders to translate requirements into technical solutions; stay current with new technologies, industry developments, and AI/data science applications in trading.
Ideal Profile
Bachelor’s or Master’s degree in Computer Science, Data Science, Engineering, Finance, Economics, or a related technical field.
Experience in data engineering, data science, or related fields (crypto/finance/Web3 experience a strong plus).
Strong programming skills in Python and SQL (R, Rust, or Java is a plus).
Proven experience building and maintaining ETL/ELT pipelines, with exposure to real-time data streaming.
Hands-on experience with cloud platforms (AWS, GCP, or Azure) and cloud-native services.
Familiarity with on-chain analytics tools, crypto data APIs, and financial datasets.
Strong understanding of data governance, quality control, and reproducibility in research workflows.
Excellent attention to detail, problem-solving, and ability to work independently in a fast-paced environment.
Bonus: familiarity with quantitative research, backtesting, and trading workflows, knowledge of machine learning data pipelines and evaluation processes.
What's on Offer?
Excellent career development opportunities
Competitive compensation (salary + research/performance bonuses)
Access to advanced tools and unique datasets
Growth and learning opportunities with a leading-edge research team"
ETL/Data Engineer,LG CNS Vietnam,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/etl-data-engineer-at-lg-cns-vietnam-4303862330?position=22&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=m9ociiSmrHiWD5KTWVukVQ%3D%3D,"Job DescriptionDevelop in web application development in different areas such as: AI Machine learning / Data Learning Solution and other company solutionsEnsure the best possible performance, quality and responsiveness of back-end applicationsAlways keep up to date with emerging technologies
Requirement[Required]Bachelor’s degree in information technology, or related fields.More than 3 years of experience in python developExperience designing data analytics pipelinesExperience in using various data analysis toolsExperience storing data in various formats (Parquet, CSV, Iceberg, etc.)Memory Optimization ExperienceExperience in gaining visibility/traceability of data analytics pipelinesExperience analyzing data in Python, Spark environmentsExperience using data analytics services such as SparkExperience in developing cloud (AWS) environmentsExperience in developing with RDBMS (MySQL) SQLExperience with collaboration tools such as Git, Jira, Confluence, etc.Understand and leverage CI/CD
[Nice to have]Team leading experienceAble to communicate in Korean
OpportunityAttractive salary and bonus will be discussed after going through CV & InterviewReview capacity annually and adjust salary increases according to work performance.Health care: Premium health insurance, Annual health check-upYoung working environmentGood career development opportunities with interesting and challenging projects.English, Korean, technical, soft skills training courses.Opportunity to learn special courses from LG CNS, new technology and security.Gifts on holidays (April 30th - May 1st, September 2nd, Tet, etc.)Outdoor activities with company support: sports clubs, team building, happy hour parties, birthdays, travel, employee and family events, etc.Working hours: 8 hours from Monday - Friday (8 hours/day)
Location15th Floor, Keangnam Landmark 72, Me Tri, Nam Tu Liem, Hanoi"
Data Engineer (Good English),Hitachi Digital Services,Ho Chi Minh City Metropolitan Area,https://vn.linkedin.com/jobs/view/data-engineer-good-english-at-hitachi-digital-services-4273263045?position=23&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=jf0GDc%2Bsns9kmivfUaI9Nw%3D%3D,"Imagine the sheer breadth of talent it takes to inspire the future. We don’t expect you to ‘fit’ every requirement – your life experience, character, perspective, and passion for achieving great things in the world are equally important to us.
What You’ll Bring
3+ years of experience in data engineering, analytics, or BI roles.
Proficiency in SQL and Strong experiences in Python.
Experience with cloud data platforms (AWS, Azure).
Have experiences with PowerBI and familiar with Tableau, AWS Quicksight, Qlik,.. is a plus
Excellent communication and stakeholder management skills.
Championing diversity, equity, and inclusion
How We Look After You
We want to help you take care of your today and tomorrow – at home and at work. Which is why we offer industry-leading benefits that go far beyond compensation. That means support, services, and resources that also take care of your holistic health and wellbeing. We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. Here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent).
We’re a global team of innovators, together, we harness engineering excellence and passion for creating meaningful solutions to complex challenges. We turn organizations into industry leaders that can a make positive impact on their industries and society.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.
Fostering innovation through diverse perspectives
Hitachi is a global company operating across a wide range of industries and regions. One of the things that sets Hitachi apart is the diversity of our business and people, which drives our innovation and growth.
We are committed to building an inclusive culture based on mutual respect and merit-based systems. We believe that when people feel valued, heard, and safe to express themselves, they do their best work.
How We Look After You
We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status
or any other protected characteristic.
Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success."
Data Engineer,Galaxy FinX,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-galaxy-finx-4307876798?position=24&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=4d%2FC%2BvgdqQlQQRtPlzUJTA%3D%3D,"As a Data Engineer, you'll lead initiatives optimizing data flow, ensuring quality, and implementing security measures.
Job Description:
Data Engineering
Design and implement scalable and secure data pipelines for collecting, processing, and storing data.
Develop and maintain datasets, ensuring accuracy, completeness, and compliance with security standards.
Work closely with data scientists and analysts to understand data needs and implement solutions that meet compliance requirements.
Implement robust data quality checks and validation processes to identify and rectify inconsistencies, ensuring data integrity.
Optimize data processing and storage for efficiency, cost-effectiveness, and compliance.
Identify and implement solutions to enhance the performance of data pipelines, with a focus on security best practices.
Job Requirements:
Bachelor's degree in Computer Science, Information Technology, or a related field.
2 or more years of experience as a Data Engineer, ETL Developer, or similar roles, with expertise in ETL processes and tools, preferably in banking or financial services.
Experience in data modeling, data lake, and data warehousing
Strong proficiency in programming languages such as Python, Java, or Scala.
Strong proficiency in Airflow for designing and managing complex workflows and dependencies.
Solid understanding of data analytics and flow analysis for mobile app user interfaces.
Familiarity with database systems (e.g., SQL, NoSQL) and data integration tools.
Strong analytical and problem-solving skills with a detail-oriented mindset.
Excellent communication and collaboration skills to work effectively with cross-functional teams.
Knowledge of cloud platforms (e.g., AWS, GCP) and related data technologies is a plus.
Knowledge of security practices in the context of data handling.
Familiarity with version control systems (e.g., Git) and agile software development practices.
Good communication skills & ability to explain your findings to business/technical stakeholders in an efficient manner.
Be able to manage time effectively & deliver high-quality work.
Ability to work independently and manage multiple tasks within deadlines.
Possess a proactive mindset & positive attitude, able to bring in good influences to your teammates & stakeholders."
Python Developer (All Level),Saigon Technology - Accelerate Software Development,"Da Nang, Da Nang City, Vietnam",https://vn.linkedin.com/jobs/view/python-developer-all-level-at-saigon-technology-accelerate-software-development-4312583862?position=25&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=DXVBkh96VpckxAGwK0ndzA%3D%3D,"Job Description
Work closely with our clients to develop product and configure the application to their business environment
Work within a team & communicate effectively across teams
Deal with technical challenges, programing tasks while managing client expectations and building long-term customer relations
Implement assigned tasks from client/manager
Job Requirements
MUST HAVE SKILLS:
From 2+ years of experience in Python
Experience with any Python Web Framework: Django/FastAPI/Flask
Experience working with Jinja2
Experience with Python ORM: SQLAlchemy, Pydantic
Experience working with Docker, Git, Git flow, CI/CD
Experience with Unit Test, Intergration Test
Experience with Scaling and Building Stable Systems
Have knowledge with Caching, DB, DB design
Good at English
NICE TO HAVE
Experience with AI/ML/DL, especially ChatGPT and OpenAI... integration
Experience with Data Engineer, Big Data/ ETL/ Airflow
Experience working with micro-service projects
Experience with NoSQL (MongoDB/DynamoDB), Vector Database (OpenSearch, Qdrant...)
Experience with cloud services: AWS/Azure/GCP
Skills Tags
: Python, Django, DB
Benefits
Competitive Salary and Brilliant Health Benefits
Attractive salary (13th-month salary, salary review twice/year) and project bonus
Bonus programs for candidate referral, technical article writing
Interest-free loan support for personal plan
Allowance for sickness, maternity, paternity and periodic health examination
PVI health care program
The staff of the quarter and year reward
Progressive and Fun Working Environment
A professional English-speaking working environment with Agile – Scrum model
Hybrid Working Model: Flexible working time and WFH support.
Surrounded with friendly, open-minded, young and supportive colleagues
Annual company trip and regular team-building parties, party celebration (Christmas, Birthday, Mid-autumn,...), Sports clubs (football, badminton, swimming …)
Valuable Training
Sponsor examination fee for professional certificates (AWS, Azure, IELTS, PMP, Scrum Master,...)
Sponsor fee for joining any technical training sessions and courses.
Free English workshops"
Data Engineer,GMO-Z.com RUNSYSTEM JSC.,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-gmo-z-com-runsystem-jsc-4293589144?position=26&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=IPa2OIdJxHL3aBWn1TPYYg%3D%3D,"Level: Experienced, Junior, Middle
Khối: DX Solution & Product
Trạng thái: Inactive
Số lượng tuyển: 1
Thu nhập: upto 20 triệu
Địa điểm: Hà Nội
Nội dung công việc
Phân tích và thiết kế kho dữ liệu
Tối ưu hiệu năng của hệ thống
Tham gia phát triển báo cáo
Tham gia cài đặt và cấu hình môi trường
Tư vấn các vấn đề liên quan tới chất lượng dữ liệu, nghiên cứu và đưa ra phương pháp để tối ưu việc kết xuất, chuyển đổi dữ liệu
Yêu cầu công việc
Tốt nghiệp hoặc sắp tốt nghiệp chuyên ngành Khoa học Máy tính, Công nghệ Thông tin, Toán Tin hoặc các ngành liên quan.
Kinh Nghiệm
Từ 1-2 năm kinh nghiệm trong phát triển Python và Data Engineer. Kinh nghiệm với AI/ML là một lợi thế lớn.
Có kinh nghiệm trong phát triển phần mềm bằng python, html, css, javascript
Có kinh nghiệm với RDBMS (SQL Server, MySQL, PostgreSQL); sử dụng môi trường Linux (bash scripts, firewall,...); Đã làm việc với các công cụ Git, Kafka, Spark, Hadoop, Airflow, MageAI
Có kinh nghiệm visualize data sử dụng metabase, superset hoặc các công cụ tương tự
Hiểu biết về khái niệm Data Lake và Data Warehouse.
Kỹ Năng
Thành thạo ngôn ngữ lập trình Python.
Thành thạo sử dụng các công cụ AI như chatgpt, github copilot
Có kỹ năng về Crawler, Backend là một lợi thế.
Có kỹ năng sử dụng cơ sở dữ liệu SQL, NoSQL..
Có kỹ năng phân tích và xử lý dữ liệu cơ bản.
Khả năng sử dụng tiếng Anh thành thạo.
Tinh thần học hỏi, khả năng làm việc nhóm và chịu được áp lực công việc.
Chế độ đãi ngộ
Range lương: upto 20.000.000 VNĐ; 13 tháng lương/năm
Phụ cấp ăn trưa 730.000 VNĐ, thưởng 300.000 VNĐ quà sinh nhật và các loại thưởng khác (Thưởng thành tích, thưởng cá nhân xuất sắc, thưởng đạt chứng chỉ ngoại ngữ/chuyên môn từ 1.000.000 VNĐ - 5.000.000 VNĐ).
Hỗ trợ chi phí khóa học ôn, lệ phí thi chứng chỉ ngoại ngữ/chuyên môn.
Được làm việc trực tiếp với các khách hàng Nhật Bản
Được hưởng các phúc lợi khác như: Team-building hàng tháng, nghỉ mát hàng năm, khám sức khỏe định kỳ,...
Tất cả các chế độ theo quy định của Luật Lao động Việt Nam (BHXH, BHYT, BHTN,...).
Nghỉ phép năm và các ngày nghỉ lễ, Tết khác theo quy định của Nhà nước Việt Nam.
Môi trường làm việc trẻ trung, năng động, có lộ trình và cơ hội thăng tiến
Gửi CV kèm ảnh và tiêu đề email theo cấu trúc: [Tên vị trí_Tên ứng viên]
Email: hr@runsystem.vn
Website: https://runsystem.net Hoặc liên hệ trực tiếp với HR:
HR : Phương Thủy - Zalo: 0971024576 | Email: thuyntp@runsystem.net
HR: Thanh Huyền - Zalo: 0965003031 | Email: huyentt2@runsystem.net"
Data Engineer,GemCommerce,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/data-engineer-at-gemcommerce-4312661538?position=27&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=sQNcNu2U10DXQcp%2BcD470A%3D%3D,"Why we're looking for you
We are on a mission to create the best possible user experiences, focusing on convenience, delight, and world-class design. With a team of talented, motivated individuals committed to continuous learning and innovation, we need someone like you to help us achieve our goals. By joining us, you will have the opportunity to build, collaborate, learn, and grow alongside some of the most dedicated professionals in the field. This role calls for a Data Engineer passionate about turning data into actionable insights, driving excellence, and contributing to products that make a difference.
Data Engineering: Design, develop, and maintain robust ETL pipelines to extract, transform, and load data from various sources such as Google Analytics, Facebook Ads, Google Ads, and Shopify Ads...
Data Warehousing: Build and optimize data warehouses to store and manage large datasets efficiently.
Data Transformation: Transform raw data into a consumable format for data analysts and scientists.
Data Quality: Ensure data quality, accuracy, and consistency throughout the data lifecycle.
Data Infrastructure: Collaborate with the team to design and implement a scalable data infrastructure.
Data Reporting: Develop and deliver compelling data visualizations and reports to present insights and recommendations.
Data Governance: Adhere to data governance policies and ensure data security.
Cross-functional Collaboration: Work closely with data analysts, data scientists, and business stakeholders to understand their data needs.
3+ years of experience in Data Engineering or a similar role in a product-based company, experience in the e-commerce domain or SaaS is a plus.
Proficiency in SQL and programming languages like Python.
Expertise in ETL tools and cloud services such as Airflow, DBT, AWS DMS, AWS Glue.
Strong understanding of data warehousing concepts and tools.
Experience with data visualization tools like Power BI.
Knowledge of database management systems and optimization techniques.
Excellent problem-solving and analytical skills.
Strong communication skills, both written and verbal.
Ability to work independently and as part of a team.
Passion for data and a drive to learn new technologies.
Salary range: Negotiate
Performance review and competency evaluation: 2 times per year.
Lunch meal + Parking fees provided.
Annual performance bonus & 13th – month salary: up to 3-month salary (based on your devotion and business efficiency)
Holiday bonus & Company trip
Remote working days: 12 days per year
Annual leaves: 12 days per year
Treatment: annual health check-up at top clinic in VN, weekly shoulder massage treatment, monthly team bonding, annual company trip,…
Training program: ensure to cover product training for all newbie and other training programs according to the Company’s policy
Why you’ll love working here
Learn product-thinking and customer-centric mindset.
Collaborative and supportive environment
Young and passionate colleagues
Professional and creative office view
Clear & Scientific Agile Framework on the whole company workflow & culture.
Join a dynamic team to design and develop high-performance E-commerce products. Take on ambitious and challenging objectives by contributing to groundbreaking business models in the market.
Flexible working time:
Morning: 8:00 am -12:00 pm (check in from 8:00 am to 8:45 am)
Afternoon: 1:00 pm - 5:45 pm (check out from 5:00 pm to 5:45 pm)"
Data Engineer (Open for Mid/Senior),Home Credit Vietnam,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-open-for-mid-senior-at-home-credit-vietnam-4266108193?position=28&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=FZfNkUkvWe3suh5p850uxA%3D%3D,"JOB DESCRIPTION
Collaboration in Virtual Domain-aligned Team:​
Work closely with the Product Owner, Data Scientists/Analysts and other colleagues in agile way towards shared product goals​
Ensure and promote proper data model design and feasibility of data products including AI/ML model training/serving​
Continuous Improvement, Technical Excellence and Accountability:​
Follow and enrich data product building blocks and code templates, review and approve changes from contributors​
​Automated Testing, Code Quality and Documentation:​
Implement and maintain quality standards for data products​ (well-documented, reusable, scalable, performing, DRY)
​Ensure regular testing and validation are implemented as vital part of automated CI/CD​
Monitor and improve the performance and efficiency of data flows​
KEY REQUIREMENTS
Strong experience
in Data Engineer, skills in
Spark, Kafka, Python, Dbt, Iceberg, Airflow OR Cloud Technologies (AWS/GCP/Azure)
Demonstrated experience designing dimensional data model, effective algorithms to process data, software engineering principles
Proficiency in SQL and Python
Good English & communication
Good stakeholder management skills
Nice to have:
Finance/Banking/Ecommerce domain knowledge, especially in Risk, Antifraud, CRM models
Microservice architecture with Docker and Kubernetes development experience
COMPENSATION & BENEFITS
13th salary Fixed and KPI Bonus
Premium Health Care
24/7 Accidental Insurance
100% Social Insurance
Meal + Phone Allowance
Yearly Medical Checkup
15/18 Annual Leaves
Professional and Transparent Working Environment"
Data Engineer,"SOTATEK., JSC","Quận Cầu Giấy, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-sotatek-jsc-4263496601?position=29&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=nEKWaaWQPTEf4PrAGjWPgA%3D%3D,"Job Description
Design, develop, and maintain scalable data pipelines and ETL processes.
Implement and optimize data storage solutions, including data warehouses and data lakes.
Collaborate with data scientists and analysts to understand data requirements and provide efficient data access methods.
Ensure data quality, consistency, and security across all data platforms.
Develop and maintain data models, schemas, and data dictionaries.
Implement data governance policies and procedures.
Optimize query performance and data retrieval processes.
Integrate various data sources and APIs into our data ecosystem.
Automate data workflows and implement monitoring and alerting systems.
Stay up-to-date with emerging technologies and recommend improvements to our data architecture.
Job Requirements
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
5+ years of experience in data engineering or similar roles.
Fluent in English
Certifications in relevant data platforms or cloud technologies (AWS, GG, Azure, Databrick, Dataiku, Snowflake)
Strong programming skills in Python, Scala, or Java.
Extensive experience with SQL, NoSQL, Vector databases
Experience with ELT/ETL open source frameworks (DBT, Fivetran)
Proficiency in big data technologies (e.g., Hadoop, Spark, Hive).
Experience with cloud platforms (e.g., AWS, Azure, GCP) and their data services.
Experience with data modeling, data warehousing concepts, and dimensional modeling.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git) and CI/CD pipelines.
Experienced in working with large ERP systems and has participated in building ETL pipelines from the analysis and design phases.
Flexible, with strong problem-solving skills and attention to detail.
Nice To Have
Agile mindset
Experience with streaming data processing (e.g., Kafka, Flink).
Knowledge of machine learning pipelines and MLOps practices.
Familiarity with containerization and orchestration tools (e.g., Docker, Kubernetes).
Familiarity with other data analytics tools such as Power BI, Tableau, etc.
Experience with data governance and compliance requirements (e.g., GDPR, CCPA).
Experience with database migration projects
Certifications in relevant data platforms or cloud technologies (AWS, GG, Azure, Databrick, Dataiku)
Compensation & Benefits
🎯Flexible working regime and health care:
Flexible timekeeping (from 8:00 - 9:00 to 17:30 - 18:30)
Minimum 14 paid leaves per annum for all employees after probation
01-day remote work per month
A flexitime allowance of 90-180 minutes per month for employees
01 hour paid leave per day for women having children under 12 months
Social insurance, health insurance, unemployment insurance and MIC care insurance
🎯Transparent And Fair Benefits
Saturday & Sunday OFF, Overtime pay is 150%, 200%, 300% as per labor law;
Work performance review 2 times/ year (in April and October)
13th-month salary
Bonus Policy: Public holidays (2/9, 30/4, 1/5, 1/1,...); Personal Performances; Excellent Team; Performance bonus in Token of the project;..
Men’s Day, Women’s Day, Children’s Day, Mid-Autumn Festival and other benefits under the provisions of the company
🎯Dynamic Environment And Open Culture
Year-end party, sports day, yearly company trip and quarterly team building,...with a generous budget
Socialize with colleagues through monthly Happy Hour
Monthly allowance when joining clubs: Soccer, Swimming, Yoga, Music,...
Nice & modern working space with young, dynamic & friendly colleagues and free coffee, tea, drinks,...
Flat, open and sharing culture with friendly management team; outsourcing company with product mindset
🎯Strong Learning Culture
Free training courses for technical and soft skills (presentation skills, communication skills, foreign language courses,...)
Account to log in to our online learning system, which contains thousands of valuable lectures (LMS)
Participate in workshops, seminars, tech talk,... with sharing from experts inside and outside the company
Working opportunities with technical gurus who built and operated world-class applications with millions of users."
Data Engineer,WorldQuant,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-worldquant-4311373770?position=30&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=nwvt7xfBKHI3SmIsHowG7w%3D%3D,"WorldQuant develops and deploys systematic financial strategies across a broad range of asset classes and global markets. We seek to produce high-quality predictive signals (alphas) through our proprietary research platform to employ financial strategies focused on market inefficiencies. Our teams work collaboratively to drive the production of alphas and financial strategies – the foundation of a balanced, global investment platform.
WorldQuant is built on a culture that pairs academic sensibility with accountability for results. Employees are encouraged to think openly about problems, balancing intellectualism and practicality. Excellent ideas come from anyone, anywhere. Employees are encouraged to challenge conventional thinking and possess an attitude of continuous improvement.
Our goal is to hire the best and the brightest. We value intellectual horsepower first and foremost, and people who demonstrate an outstanding talent. There is no roadmap to future success, so we need people who can help us build it.
Technologists at WorldQuant research, design, code, test and deploy firmwide platforms and tooling while working collaboratively with researchers. Our environment is relaxed yet intellectually driven. We seek people who think in code and are motivated by being around like-minded people.
We are looking for an experienced Data Engineer who will help us to build and maintain an ecosystem for processing multiple datasets from different sources (both internal and external) vital to the firm’s investment operations.
What You'll Do
Creating automated data processing system and monitoring/maintaining it
Integrating multiple data sources and databases into one system
Developing interfaces and micro services in Python
Preprocessing and cleansing of semi-structured or unstructured data
Developing efficient algorithms for data processing
Testing and integrating external APIs
Supporting Business Analysts team
What You’ll Bring
A bachelor / master's degree in a technical or quantitative field from top university
At least 3 years of experience as a data engineer or software developer
Excellent programming skills
Experience with data processing using Python
Experience with building databases
Experience with Containers and Kubernetes
Scripting skills in UNIX environment: shell, python Fire, etc.
Experience with code versioning tools (e.g. Git), issue tracking tools (e.g. Jira)
Debugging skills, eye for detail and identifying problems
Strong problem-solving skills and an analytical mindset
A passion for working with data
What We Offer
Competitive and attractive compensation package with clear career road-map – where you feel challenged everyday
We offer a strong culture of learning and development: training courses, library, speakers, share and learn events
Learn from who sits next to you! Working in WQ you are surrounded by smart and talented people
Premium Health Insurance and Employee Assistance Program
Generous time-off policy, re-creation sabbatical leave (based on tenure), Trade Union benefits for staff and family
Team building activities every month: Local engagement events, monthly team lunch – Employee clubs: football, ping-pong, badminton, yoga, running, PS5, movies, etc.
Annual company trip and occasional global conferences – opportunity to travel and connect with our global teams
Happy-hour with tea break, snacks and meals every day in the office!
By submitting this application, you acknowledge and consent to terms of the WorldQuant Privacy Policy. The privacy policy offers an explanation of how and why your data will be collected, how it will be used and disclosed, how it will be retained and secured, and what legal rights are associated with that data (including the rights of access, correction, and deletion). The policy also describes legal and contractual limitations on these rights. The specific rights and obligations of individuals living and working in different areas may vary by jurisdiction.
WorldQuant is an equal opportunity employer and does not discriminate in hiring on the basis of race, color, creed, religion, sex, sexual orientation or preference, age, marital status, citizenship, national origin, disability, military status, genetic predisposition or carrier status, or any other protected characteristic as established by applicable law."
Data Engineer,DataHouse Vietnam,Da Nang Metropolitan Area,https://vn.linkedin.com/jobs/view/data-engineer-at-datahouse-vietnam-4306346137?position=31&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=k6Uu%2FzzZ23sgEX9bmyYpOw%3D%3D,"We're looking for a
Medior Data Engineer
to join our team in
Da Nang / Binh Duong / HCMC.
In this role, you will build and maintain data pipelines that power our analytics and business intelligence. Work with our team to ensure data quality and accessibility across the organization. You will support the ingestion, transformation, and storage of data from various sources, contributing to reliable and scalable data infrastructure. This role offers a great opportunity to grow your technical skills while collaborating closely with data analysts, scientists, and engineers to enable data-driven decision-making.
MAIN ACCOUNTABILITIES:
Build and manage data workflows to extract, transform, and load data into storage systems.
Write scripts to clean, format, and enrich raw data for analytics use.
Track ETL pipeline performance and quickly address failures or delays.
Work with analysts and data scientists to deliver relevant and usable data.
Maintain clear documentation of data workflows, logic, and troubleshooting steps.
Understand and document clients’ business needs and data flows to identify where analytical processes can add value and prescribe appropriate solutions.
Collect, manipulate, and store data from various sources to construct streamlined data pipelines used to integrate with analytical software dashboards.
JOB QUALIFICATIONS:
Education:
Graduated in a related field (Information Management/Informatics, Computer Science/ Engineering, Data Science).
Experience & Skills:
Over 3 years of experience in databases, data modeling, data collection, data warehousing, and ETL processes.
Hands-on experience with ETL orchestration tools for building and scheduling data workflows.
Proficient in writing and optimizing SQL queries for data extraction, transformation, and analysis.
Strong understanding of modern data storage architectures and their role in supporting analytical workloads.
Skilled in using Python for data processing, automation, and scripting tasks.
Proficient in Git for version control collaborative development.
Self-motivated, proactive, and able to work effectively both independently and within a team environment.
Good English communication skills.
Familiarity with AWS services like S3, Redshift, or Glue in a data engineering context.
Practical experience working with large-scale data storage and querying systems.
Comfortable navigating and executing commands in a Linux or Unix environment.
Understanding of containerization concepts and ability to use Docker for setting up isolated environments.
JOB BENEFITS:
With human-centric spirit, we offer a total compensation package that ranks among the best in the industry. It consists of competitive pay, bonuses, as well as, benefit programs which include health and wellbeing, learning and development, benefits at work.
1. Salary & bonuses
Competitive take-home cash together with monthly allowances.
Annual performance appraisal and salary review.
13th salary and accountability bonus twice a year based on company/individual performance and profitability.
Monthly entertainment/team building allowances & project/team celebrations for certain milestones (applied for project teams and back office).
Working tools: laptop and widescreen provided.
2. Life-long learning & development:
Opportunities to work alongside a fun and global team with the international/US clients from multi-cities (Da Nang/Binh Duong/Ho Chi Minh).
On-site program to explore the regional and US cultures for all levels.
Seniority reward from one-year intervals and up.
Extensive learning budget along with rewards for any individuals to gain some certain certificates as defined.
Tons of internal and external training sessions (online learning resources, English, wellness & tech-share workshops).
3. Fun vibes at work:
Weekly cafeteria expenses with coffee beans, milks, noodles and snacks available at offices.
Public holiday gifts and celebrations on Tet, Women’s days, Men’s Day, Children’s Day & Mid-Autumn Day, Programmer’s Day, Teacher/Trainer’s Day, Christmas Day.
Sport activities and clubs.
4. Health & well-being benefits:
Statutory insurances (social, health and unemployment) based on full salary according to the laws of Vietnam.
Annual health-check with advanced package at a quality medical facility.
National coverage healthcare card and seasonal flu shot.
12 annual leaves + 4 family days-off
Company trip once a year and team outing trips.
Visiting and caring on some occasions: birthday, marriage, pregnancy and baby shower, university graduation, hospitalization and funeral (for self-employees and direct family members)
Join our exceptional team and enjoy these attractive benefits. Explore our open positions and take your career to new heights with us!"
Senior/Lead Data Engineer,TymeX,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/senior-lead-data-engineer-at-tymex-4299945154?position=32&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=gen4N8lmgsnHtPQv23NGog%3D%3D,"TymeX is building some of world's fastest growing digital banks and the data team plays a key role in driving the bank's vision of creating a platform that stimulates economic participation and facilitates broader financial inclusion by implementing creative best in class data and analytic solutions to achieve success in providing quality services and products to our customers and optimising the business.
Senior/Lead Data Engineer
you will contribute to the mission by creating solutions that will directly support informed decision-making and innovation by providing clean, protected, quality and auditable data from various sources into fit for purpose data products.
In this role, you can expect to:
Design, develop, test, deploy and monitor data pipelines in Databricks on AWS from a wide variety of data sources
Design, develop, test, deploy and monitor scalable code with PySpark and SQL in Databricks
Identify opportunities to improve internal process through code optimisation and automation
Build data quality dashboards, lineage flows / and or monitoring tools to utilize the data pipeline, providing active monitoring and actionable insight into overall data quality and data governance
Assist in migrating data from legacy systems onto newly developed solutions
Follow and lead best practices on all data security, retention, and privacy policies
Requirements
Bachelor's degree
3+ years' experience of building ETL/ELT pipelines
Proven competency in solution design, development, implementation, reporting and analysis
Proficiency in Apache-Spark, Python and SQL languages
Proficiency in working with Text, Delta, Parquet, JSON, CSV, and XML data formats.
Working knowledge of Spark structured streaming
AWS infrastructure experience, specifically working with S3
Solid understanding of git-based version control, DevOps, and CI/CD. Experience of working on Atlassian stack a plus
Knowledge of common web API frameworks and web services
Strong teamwork, relationship, and client management skills, and the ability to influence peers and senior management to accomplish team goals
Willingness to embrace modern technology, best practice, and ways of work
Benefits
Performance bonus up to 2 months
13th month salary pro-rata
15-day annual leave+ 3-day sick leave + 1 birthday leave + 1 Christmas leave
Meal and parking allowance are covered by the company
Full benefits and salary rank during probation
Insurances as Vietnamese labor law and premium health care for you and your family without seniority compulsory
SMART goals and clear career opportunities (technical seminar, conference, and career talk) - we focus on your development
Values-driven, international working environment, and agile culture
Overseas travel opportunities for training and working related
Internal Hackathons and company's events (team building, coffee run, blue card...)
Work-life balance 40-hr per week from Mon to Fri"
"Data Engineer, Specialist",AIA Vietnam,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-specialist-at-aia-vietnam-4306363010?position=33&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=0EiPEa9sMypMnOf1qCl5yQ%3D%3D,": Manager, Data Engineer
Ho Chi Minh
: Customer and Information Technology |
: Information Technology
: Individual Contributor
THE OPPORTUNITY:
We are currently looking for
Data Engineer, Specialist
who is responsible for developing and maintaining data pipelines, infrastructure, and systems that support advanced analytics initiatives across various departments and support data integration within the Data Warehouse platform as enterprise golden source.
ROLES AND RESPONSIBILITIES:
Develop and implement data pipelines, ETL processes, and set up routines for data from various data sources.
Optimize the data processing and query performance to ensure scalable and efficient data operations.
Implement data quality controls, data governance policies and securities requirement to safeguard sensitive insurance data
Collaborate with data scientist, data analysts and other business stakeholders to understand data needs
Develop technical and training manuals and comply to other company data life cycle standards
Stay up to date with emerging technologies, trends, and best practice in data engineers to drive continuous improvement and innovation.
JOB REQUIREMENTS:
Education & Experience
Education: BSc degree in computer science. Data Engineering, Information Systems, Mathematics or relevant fields
Experience 3-5 years of experience as a Data Engineer or similar role within the life insurance industry or other relevant financial services sector.
Technical skill
Experience with Data warehouse: Databrick, Spark.
Experience with Business Intelligence: Power BI
Expert-level SQL: SQL Server, SQL Databrick.
Experience in programming language – Python, Spark
Experience with Git source control.
Experience with building data model special on insurance domain.
Hands-on experience of working with Data projects that have a complete CI/CD system.
Experience with ETL tool: Azure Data Factory, Airflow.
Soft skill:
Ability to influence both technical and business peers and stakeholders.
Lead by example – jump 'on the tools' when required to support the team.
Good communication skills with the ability to convey complex technical concepts in clear and concise language for various audiences, English communication is required.
Demonstrated problem-solving skills and a results-oriented mindset."
Data Analyst,ĐÚNG NGƯỜI ĐÚNG VIỆC Community,"Phường Chí Minh, Hai Duong, Vietnam",https://vn.linkedin.com/jobs/view/data-analyst-at-%C4%91%C3%BAng-ng%C6%B0%E1%BB%9Di-%C4%91%C3%BAng-vi%E1%BB%87c-community-4313606086?position=34&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=Wc90aUWdoq2QtXo2DWx%2Bkw%3D%3D,"Mô tả công việc
Key Responsibilities
Engage with business stakeholders to understand reporting needs, KPIs, and pain points. Translate business questions into analytical frameworks
Develop dashboards for Pilot metrics (EFOS, OSA, NPD availability, visibility compliance, and in-store discipline using geo-fencing and SOP adherence) & Intraday dashboard platform from design to roll out
Automate reporting for Pods, SteerCo, and regional squads.
Ensure data quality & pipeline integrity across multiple sources (e.g., execution, stock, offtake, NIV)
Scale dashboards to national roll-out with high adoption rate
Support AI image recognition pilots and integrate external data sources
Deliverables / OKRs
Pilot dashboards live as per required lead-time (both Power BI & Intraday one)
≥90% usage & adoption rate.
Weekly SteerCo pack report delivered on-time 100%.
Regional dashboards launched by Jan 2026.
Training delivered to regional teams for dashboard usage.
Yêu cầu công việc
3+ years in data/business analysis (preferably FMCG/retail).
Strong dashboarding & visualization (Power BI, Tableau, Looker, or Google Data Studio).
Solid SQL/ETL knowledge; experience managing data pipelines.
Strong problem-solving skills; ability to turn data into insights.
Excellent communication skills to present insights clearly.
Tại sao bạn yêu thích làm việc tại đây
Bảo hiểm xã hội, Bảo hiểm y tế
Nhân viên được hưởng lương tháng 13 và thưởng các dịp lễ, Tết - theo chính sách công ty từng năm
Tăng lương định kỳ: Review lương dựa trên năng lực và kết quả công việc hàng năm
Đánh giá minh bạch, công bằng
Trang bị hiện đại: Laptop làm việc tiên tiến, hỗ trợ công cụ đầy đủ
Địa điểm làm việc
Hồ Chí Minh: Quận 7
Thời gian làm việc
Thứ 2 - Thứ 6 (từ 08:30 đến 17:30)
Từ thứ 2 đến thứ 6 (8h/ngày)"
Data Engineer,Ban Vien Corporation,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-ban-vien-corporation-4310311711?position=35&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=xMS3mo8%2Bt8ZOYEZwdsLRPA%3D%3D,"JOB DESCRIPTION
We are looking for Data Engineer to join us:
• Responsible for implementing robust data pipeline.
• Responsible for creating reusable and scalable data pipelines.
• Responsible for using Databrick to build model in Datalake/Azure Synapse.
• Data engineers’ provision and set up data platform/Streaming data technologies that are on-premises and in the cloud.
• The data platforms include relational databases, nonrelational databases, data streams, and file stores.
REQUIREMENTS
• Bachelor’s degree in Computer Science, Computer Engineering, or a related technical discipline.
• At least 3 years of experience in Data Engineering, particularly in Data Integration.
Technical Proficiency:
Hands-on experience with Flink/Spark + Kafka/Airflow
Have experience with Databricks/ Snowflake / InfluxDB
Having experience with CI/CD and Kubernetes is a plus
Strong skills in SQL and Python, Java programming.
Familiarity with Big Data technologies such as Spark or Hadoop/MapReduce
Practical knowledge of Azure services like HDInsight and Azure Databricks would be advantageous
Knowledge of hosting solutions, container-based deployments, and storage architectures is beneficial
Nice to have:
Understanding of cloud platforms, including one of Azure/AWS/Google Cloud Platform
COMPANY BENEFITS
• 13th Salary + Performance Bonus.
• Pass probation Bonus.
• Premium healthcare insurance benefits (PVI Insurance package) and family medical benefit (based on the level of experience).
• Provide the famous e-learning platform-Udemy, to encourage continuous learning to adapt to the T-shape model.
• Flexible working time: only 8 hours required as continual working-time at the office.
• Annual leave up to 17 days: 12 days paid leave + 5 days’ sick leave.
• Professional and Personal Development Training Programs.
• 4 Stars standard company trip in summer and a big annual Year-End-Party.
• Coffee and snacks provided.
• Holiday celebrations and parties for team members and family."
BigData Engineer,Viettel AI,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/bigdata-engineer-at-viettel-ai-4312322565?position=36&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=a1ZGOEtFSOn%2B3UXA7ZDlTw%3D%3D,"Attractive remuneration with competitive income and 6 months/year salaries/ efficient production and business.
A welfare package of $2000 per year including Tet bonus, holiday bonus and resort.
Being a colleague of a team of leading technology experts in fields such as: DPI, BigData, Machine Learning, AI, Data Mining, Speech Recognition, Virtual Assistant…
Improve capacity through intensive training courses, study support and professional certification exams.
Consider raising the minimum salary once a year. Access to attractive promotion opportunities.
12 days holiday, 12 vacation days and 3 annual vacation days.
Always enjoy to the fullest full benefits of social insurance, health insurance, unemployment insurance and statutory leave and sick leave after signing the labor contract
Join the comprehensive health care MIC insurance package.
Relax, spark creativity with Happy Time every day, modern workspace.
Enjoy a delicious lunch, selected by nutritionists.
Working in key projects of the Group with large system scale.
Experience the multiculturalism when working with Viettel's personnel in more than 10 countries.
Job Descriptions
Building and maintaining a data collection and processing system
Integrating data from various sources.
Designing a Data Lake, Data system Warehouse, building an ETL system on the basis of big data.
Job requirements
General Requirements
Graduated from a regular university with good grade or higher, majoring in the right position
Having experience of 02 years or more (accepting the whole process of working experience) while undergrad)
TOEIC 550/990
Required Requirements
Experience working with one of the relational databases: MySQL, Postgres, Oracle
Knowledge of data processing techniques, ETL process
Minimum 1 year of Java or Python programming
Programming experience Scala
Experience working with NoSQL databases such as Elasticsearch, Redis, HBase, Cassandra
Experience working with graph databases like Neo4J, JanusGraph
Knowledge of distributed computing principles, Big Data ecosystem such as Hadoop, MapReduce, Spark
Experience in integrating data from multiple sources different
Programming experience with Spark, Spark Streaming, Kafka, is an advantage"
Data Engineer,Techcom Securities,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-techcom-securities-4311824920?position=37&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=cjcxQER0YjF68eB5gb5t3A%3D%3D,"we are redefining how financial services are delivered — using technology to create smarter, faster, and more inclusive financial solutions. Data is at the core of everything we do, from personalized financial products to real-time fraud detection. We're looking for a
Data Engineer
with a passion for building scalable and secure data infrastructure to help power our next generation of fintech products.
Data Engineer
, you will work closely with data scientists, backend engineers, and product teams to build and optimize our end-to-end data platform. You’ll be responsible for architecting and maintaining robust, scalable, and high-performance
data pipelines and lakehouse infrastructure
to support our analytics, risk models, and customer experiences.
Key Responsibilities
Design and develop
ETL/ELT pipelines
for structured and unstructured financial data across internal and third-party systems.
Develop and manage large-scale
data infrastructure
Apache Spark
Apache Kafka
Apache Flink
data lakehouse architecture
services such as:
Collaborate with analytics, product, and engineering teams to deliver reliable and scalable data solutions for reporting and real-time applications.
Collaborate with analytics, ML, and engineering teams to integrate data products into core financial systems.
Monitor and improve the performance and reliability of pipelines and data services.
Required Qualifications
of experience as a Data Engineer or in a similar backend/data-focused engineering role.
Proficient in
, with working knowledge of
Solid understanding of
distributed data processing
streaming architectures
Strong experience with
big data tools
(Spark, Kafka, Flink, Hive, etc.).
Production experience with
AWS data stack
(S3, Glue, EMR, Kinesis, Redshift, Athena).
Strong SQL skills and experience with data modeling and data warehousing concepts.
Familiarity with CI/CD, Docker, Git, and automation in data workflows.
Nice to Have
Experience with
open data lakehouse
Apache Iceberg
Apache Hudi
Hands-on experience with
Familiarity with
data cataloging
data lineage
tools (e.g., AWS Glue, Apache Atlas, Great Expectations).
Fintech domain knowledge (payments, risk, lending, compliance) is a strong plus.
What You’ll Get
Competitive salary with performance-based bonuses.
Opportunity to work on cutting-edge data architecture in a high-growth fintech environment.
Ownership and influence in a data-driven organization.
Learning budget and access to leading cloud and data tools."
Data Engineer,WorldQuant,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-worldquant-4288906692?position=38&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=hKa54klNK447N6LDh5yTPw%3D%3D,"WorldQuant develops and deploys systematic financial strategies across a broad range of asset classes and global markets. We seek to produce high-quality predictive signals (alphas) through our proprietary research platform to employ financial strategies focused on market inefficiencies. Our teams work collaboratively to drive the production of alphas and financial strategies – the foundation of a balanced, global investment platform.
WorldQuant is built on a culture that pairs academic sensibility with accountability for results. Employees are encouraged to think openly about problems, balancing intellectualism and practicality. Excellent ideas come from anyone, anywhere. Employees are encouraged to challenge conventional thinking and possess an attitude of continuous improvement.
Our goal is to hire the best and the brightest. We value intellectual horsepower first and foremost, and people who demonstrate an outstanding talent. There is no roadmap to future success, so we need people who can help us build it.
Technologists at WorldQuant research, design, code, test and deploy firmwide platforms and tooling while working collaboratively with researchers and portfolio managers. Our environment is relaxed yet intellectually driven. We seek people who think in code and are motivated by being around like-minded people.
We are seeking for an exceptionally talented candidate to join our rapidly growing team as a Data Engineer. The Data Engineer will partner with our close-knit team of quantitative researchers, technologists and data sourcing colleagues to analyze and enrich a broad range of structured and unstructured large-scale data.
Job Responsibilities Include, But Not Limited To The Followings
Enriching a wide range of structured and unstructured data into datasets for quantitative analysis and financial engineering.
Enhancing data quality & integrity by developing validation tools to measure the effectiveness of data enrichment.
Becoming a domain expert on different deep learning and machine earning applications, analyzing & understanding the underlying dynamics and behaviors within the data.
Develop insights based on the data and collaborate with the research team to generate signals.
Developing the utility tools that can further automate the software development, testing and deployment workflow.
What You’ll Bring
Strong academic background – minimum of a bachelor’s degree in a technical or quantitative field.
Practical experience with and understanding of deep neural networks and other machine learning techniques.
Demonstrated ability to implement data science pipelines and real-time applications in Python (C++ is a plus).
3+ years of relevant experience as a Data Engineer or similar roles.
Proficiency with python based tools like Jupyter notebook, coding standards like pep8.
Experience with LLM/AI for data processing is a plus
Exceptional analytical & problem solving abilities, with a strong attention to detail.
Good command of English
Excellent software development skills: ability to convert rough overall use-cases to a working codebase.
Motivated by a deep curiosity and passion to learn is a plus.
Past experience as a data scientist or data engineer in finance or investment profile is a plus.
Experience with Linux/Unix shell and Git is a plus.
What We Offer
Competitive and attractive compensation package with clear career road-map – where you feel challenged everyday
We offer a strong culture of learning and development: training courses, library, speakers, share and learn events
Learn from who sits next to you! Working in WQ you are surrounded by smart and talented people
Premium Health Insurance and Employee Assistance Program
Generous time-off policy, re-creation sabbatical leave (based on tenure), Trade Union benefits for staff and family
Team building activities every month: Local engagement events, monthly team lunch – Employee clubs: football, ping-pong, badminton, yoga, running, PS5, movies, etc.
Annual company trip and occasional global conferences – opportunity to travel and connect with our global teams
Happy-hour with tea break, snacks and meals every day in the office!
By submitting this application, you acknowledge and consent to terms of the WorldQuant Privacy Policy. The privacy policy offers an explanation of how and why your data will be collected, how it will be used and disclosed, how it will be retained and secured, and what legal rights are associated with that data (including the rights of access, correction, and deletion). The policy also describes legal and contractual limitations on these rights. The specific rights and obligations of individuals living and working in different areas may vary by jurisdiction.
WorldQuant is an equal opportunity employer and does not discriminate in hiring on the basis of race, color, creed, religion, sex, sexual orientation or preference, age, marital status, citizenship, national origin, disability, military status, genetic predisposition or carrier status, or any other protected characteristic as established by applicable law."
System Engineer,The Grand Ho Tram,Ho Chi Minh City Metropolitan Area,https://vn.linkedin.com/jobs/view/system-engineer-at-the-grand-ho-tram-4300106098?position=39&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=vQoOaYYbvdETzH5nDt0T7w%3D%3D,"Information Technology
Work Location Ho Chi Minh (Ho Tram Commune)
Job Level Experienced (Non - Manager)
Job Type Permanent
Qualification College
Experiences 1 - 3 Years
Salary Competitive
Industry IT - Hardware / Network, Restaurant / Hotel
Contact Person HR
Job Description
Perform the day to day operations, management and administration to protect the integrity, confidentiality, and availability of information assets and technology infrastructure of the organization.
Identify security issues and risks, monitor and test application performance for potential bottlenecks, identify possible solutions and develop mitigation plans
Performing analysis of system security needs and contributes to design, integration, and installation of hardware and software.
Analyzing, troubleshooting and correcting system problems remotely and on-site.
Architect, design, implement, support, and evaluate security-focused tools and services including project leadership roles
Develop and interpret security policies and procedures
Mentor junior members of the team
Participate in security compliance efforts
Develop and deliver training materials and perform general security awareness and specific security technology training
Acquisition and vendor risk assessment due diligence
Evaluate and recommend new and emerging security products and technologies
Participate in tier 2 and tier 3 security operations support
Participate in incident handling
Participate in projects that develop new intellectual property
Manage and monitor all installed systems and infrastructure
Write and maintain custom scripts to increase system efficiency and lower the human intervention time on any tasks
Job Requirement
BS/MBS in Computer Science or equivalent desired
Emerging company-wide reputation in the field of information security
Consistent implementation of security solutions at the business unit level
At least 3 years of experience in infrastructure or application-level vulnerability testing and auditing
At least 3 years of system, network and/or application security experience
Strong experience and detailed technical knowledge in security engineering, system and network security, authentication and security protocols, cryptography, and application security
Knowledge of network and web related protocols (e.g., TCP/IP, UDP, IPSEC, HTTP, HTTPS, routing protocols)
Job tags: System Engineer IT Specialist Chuyên viên IT Nhân viên IT IT Engineer IT Assistant IT Executive Kỹ sư hệ thống"
Data Engineer,FPT Software Career,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-fpt-software-career-4309475547?position=40&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=LQHaPHtgumEzvfSlxgNjCA%3D%3D,"Job Description :
Design and implement scalable data pipelines to process large and complex datasets.
Build and maintain reports and dashboards to provide actionable insights across key business domains.  Analyze data to identify trends, patterns, and opportunities for business improvement.
Collaborate with cross-functional teams to gather business requirements and translate them into technical solutions.
Recommend and integrate data sources to improve data completeness and decision making.
Develop and maintain data models, dictionaries, and documentation to support BI initiatives.
Implement data visualization techniques and tools to communicate findings effectively.
Contribute to the definition and execution of BI strategy and roadmap.
Stay current with the latest advancements in data engineering, cloud platforms, and analytics technologies.
Job Qualification :
Bachelor’s degree in Computer Science, Engineering, or related field.
Excellent verbal and written communication skills in English.
3+ years of hands-on experience building and operationalizing data pipelines.
Strong experience with GCP, particularly BigQuery and related services (e.g., Cloud Storage, Dataflow, Pub/Sub).
Proficiency in SQL, ETL development, and data warehousing concepts.
Experience with BI tools such as Power BI, Tableau (optional), or other reporting solutions.
Solid understanding of data modeling, data integration, and data governance.
Familiarity with source control and DevOps practices using Git and CI/CD tools.
Hands-on experience working with various data sources: SQL databases, flat files (CSV/XML), Web APIs, etc. Strong understanding of storage technologies: Data Lake, Relational DBs, NoSQL, and Graph databases.  Comfortable working in Agile and fast-paced environments."
Senior Data Engineer,SCC,Ho Chi Minh City Metropolitan Area,https://vn.linkedin.com/jobs/view/senior-data-engineer-at-scc-4306455952?position=41&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=1SV9izealif%2F6Dwfl%2F6oKg%3D%3D,"The Application Development and Maintenance department is looking for a skilled and experienced Data Warehouse Developer to join our Business Intelligence development team. As part of our team, you will help design and implement complex DW and BI solutions for our customers, leveraging both on-premises and cloud technologies (primarily using Microsoft technology stack). We deliver end-to-end BI services, from architecting DW/BI solutions and designing & developing Data Warehouses, to implementing ETL processes and developing the presentation layer.
Responsibilities
Take part in implementing complex BI solutions for SCC customers
Participate in the design and development of Data Warehouse systems
Design, implement, fine tune Azure Synapse or Fabric pipelines for big data.
Establish data governance and metadata management practices.
Design and develop SQL stored procedures, functions, views, triggers
Design, implement and maintain database objects (tables, views, indexes) as part of the solutions we develop
Work closely with our Data Architects as part of DW solution design and development
Maintain library of model documents, templates, or other reusable BI technical knowledge assets
Estimate effort for assigned coding tasks
Qualifications
Advanced knowledge of relational databases & SQL with a strong understanding of database structures/Data Modeling concepts
Strong understanding of Data Warehousing/ETL strategy and best practices as well as the ability to develop ETL processes using T-SQL/SSIS/Data Factory/Synapse Analytics/Microsoft Fabric
Hands-on experience working with Azure Data Platform technologies, including Azure SQL, Azure Data Factory, and Azure Synapse
Experience working with Microsoft Fabric and understanding its concepts and layers in a data management context
Working knowledge of business system analysis and requirements definition
Ability to translate business questions into data requirements and business intelligence
Experience in database development, SQL queries, stored procedures
Ability to adhere to coding standards and participate in peer code reviews
Knowledge and understanding of the software development life cycle
Experience with software documenting and version control
Analytical and troubleshooting skills with challenging technical subjects and tasks.
Strong written and verbal communication skills (English)
Preferred Skills
Expertise in Azure Data Stack (Synapse, ADF, Databricks).
Mastery of DAX optimization and query folding.
Experience with scripting for advanced analytics.
Experience in designing and developing complex reports using Power BI
Ability to create & maintain Power BI data models and use DAX expressions"
Data Engineer,Ecomobi PTE,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/data-engineer-at-ecomobi-pte-4312619334?position=42&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=Fyy%2FdOCMZqjjVa1%2FrYVPMA%3D%3D,"Tổng quan công việc
Ecomobi đang tìm kiếm một Data Engineer cấp cao để thiết kế, xây dựng và tối ưu hóa kiến trúc, hệ thống dữ liệu phục vụ cho các hoạt động phân tích dữ liệu chiến dịch Affiliate, sản phẩm dữ liệu và hỗ trợ các đội ngũ Data Scientist, Analyst. Bạn sẽ đóng vai trò then chốt trong việc đảm bảo dữ liệu luôn chính xác, tin cậy và sẵn sàng cho toàn bộ hệ thống downstream.
Trách nhiệm
Thiết kế, xây dựng và tối ưu hóa hệ thống
pipeline dữ liệu
(batch & streaming) phục vụ phân tích và sản phẩm dữ liệu.
Quản lý và phát triển
Data Lake, Data Warehouse, Data LakeHouse
Đảm bảo dữ liệu chính xác, tin cậy và sẵn sàng cho Data Scientist, Analyst, và các hệ thống downstream.
Đề xuất, lựa chọn và triển khai công nghệ big data phù hợp:
Spark, Kafka, Airflow, dbt, Snowflake, Redshift, BigQuery
Tham gia xây dựng
best practices
data governance, data quality, metadata management, lineage
Làm việc với đội ngũ cross-functional (
Product, Data Science, BI
) để hiểu nhu cầu dữ liệu và triển khai giải pháp.
Mentor và review code
cho các Data Engineer cấp dưới (Nếu có).
Đưa ra quyết định nhanh trong tình huống bất ngờ dựa trên quy định bộ phận.
Phân biệt công việc quan trọng/cấp bách, lập kế hoạch ưu tiên hiệu quả.
Kỹ năng Yêu cầu
Tối thiểu 3-5 năm kinh nghiệm
trong lĩnh vực
Data Engineering
SQL nâng cao
và một ngôn ngữ lập trình (ưu tiên
Python/Scala
Kinh nghiệm với
ETL/ELT pipelines
và các công cụ workflow orchestration (
Airflow, Luigi, dbt
kiến trúc dữ liệu
: Data Lake, Data Warehouse, Data LakeHouse, Data Mart.
Trải nghiệm với hệ thống xử lý dữ liệu phân tán (
Spark, Hadoop, Kafka, Flink
Hiểu biết tốt về cơ sở dữ liệu quan hệ (
) và NoSQL (
MongoDB, Cassandra, Redis
Kinh nghiệm triển khai trên
cloud (AWS/GCP/Azure)
, sử dụng dịch vụ liên quan đến compute, storage, database, streaming.
CI/CD, IaC (Terraform, CloudFormation)
tư duy hệ thống, kỹ năng giải quyết vấn đề
, và khả năng
làm việc độc lập
Giao tiếp cởi mở, tôn trọng, truyền đạt kỳ vọng rõ ràng với nhóm.
Ra quyết định phức tạp dựa trên quy chuẩn, tham vấn cấp trên khi cần.
Kỹ năng Ưu tiên
Ưu tiên có kinh nghiệm về
data security, privacy (GDPR, data masking, encryption)
Hiểu biết về mô hình đề xuất hoặc nhắm mục tiêu thương hiệu.
Kinh nghiệm làm việc với dữ liệu từ Shopee, Lazada, TikTok.
Quyền lợi
Thu nhập hấp dẫn, nhận 100% lương trong 2 tháng thử việc.
Phụ cấp: ăn trưa 40.000 VND/ngày, gửi xe 250.000 VNĐ/tháng.
Cấp laptop/PC hoặc trợ cấp 400.000 VND/tháng (nếu dùng laptop cá nhân).
Thưởng lễ Tết: tháng lương 13, Tết dương lịch, 08/03, 01/05, 02/09, 20/10, Trung thu, sinh nhật, thưởng quý, Year-End bonus, v.v.
Đánh giá hiệu quả và xem xét tăng lương ít nhất 1 lần/năm.
Quỹ học tập dành cho toàn nhân viên.
Thời gian làm việc: 9h–18h (thứ Hai đến thứ Sáu).
Nghỉ phép năm: 17 ngày/năm (12 ngày nghỉ, 1 ngày sinh nhật, 4 ngày Caring days).
Bảo hiểm xã hội, sức khỏe (PVI); khám sức khỏe định kỳ 1 lần/năm.
Du lịch summer trip, year-end party, teambuilding, v.v.
Thông tin khác:
Thời gian làm việc: 9AM – 6PM (7,5 hours/day From Monday to Friday)
Địa điểm: 174 Thai Ha, Dong Da district, Ha Noi."
Analytics Engineer,Golden Gate Restaurant Group,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/analytics-engineer-at-golden-gate-restaurant-group-4305155162?position=43&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=yAugqnNibPTS5wjLs60%2FBw%3D%3D,"About Golden Gate Group
Founded in 2005, Golden Gate is recognized as a leading restaurant chain operator in Vietnam in terms of sales, number of concepts, outlets, and professionalism.
Golden Gate currently operates over 21 concepts with nearly 400 restaurants nationwide, serves 17 million customers each year, and has planned for further development.
Philosophy: More quality for life
Mission: Happy team members – Happy customer
Vision: Be the first F&B choice
Core Values: Integrity – Humanity – High efficiency
Job Description
Build and maintain data models following Medallion Architecture using SQL, dbt, Git, Trino, Iceberg.
Build ETL pipelines/transformations using Airflow, dbt, Git.
Design and optimize semantic layers, intermediate models, and data marts to improve query performance and reusability.
Integrate and manage metadata and data lineage for visibility and traceability across the data stack.
Build dashboards in Power BI/Superset when needed.
Requirements
2+ years of experience as an Analytics Engineer / Data Engineer / Data Analyst.
Bachelor’s degree in Computer Science, Information Systems, Software Engineering, or a related field.
Strong understanding of dimensional data modeling (fact/dimension tables, SCD, KPI logic, metric standardization, date granularity & calendar table design).
Proficient in SQL, dbt, Python, Airflow, Git.
Good knowledge of Lakehouse toolset (MinIO, Iceberg, Hive, Trino…).
Experience with Power BI / Superset.
Familiarity with Data Governance, including access control, quality, and standardization.
Benefits
Competitive salary and performance-based bonus.
13th-month salary and annual bonus.
Training and development opportunities to enhance professional skills.
Dynamic, professional environment at one of the leading F&B corporations in Vietnam.
Employee discounts across the Golden Gate Group restaurant system."
Data Engineer (AWS - HCM),TechX Corp.,"District 3, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-aws-hcm-at-techx-corp-4304070191?position=44&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=zIZztseXh587Zusa2TRq8g%3D%3D,"Company Overview:
, we are pioneers in delivering cutting-edge solutions that empower businesses to thrive in today’s digital landscape. With a strong focus on
Cloud Transformation (AWS)
Data Modernization
Generative AI
, we bring unparalleled expertise to drive innovation, efficiency, and growth for our clients.
We specialize in
banking and financial services, retail, manufacturing, and transportation
, delivering impactful solutions that address industry-specific challenges and significant market shifts.
Together, we’ll build a future where innovation knows no limits.
Job Description:
As a Data Engineer at TechX, you'll be at the forefront of delivering world-class data platforms, data modernization and data analytics solutions, utilizing the most advanced architecture and cloud services for our esteemed customers.
Key Responsibilities:
Collaborate with TechX Data Architect, Data Engineer, Data Analyst, and Data Scientist to design, implement, and tackle data warehouse, data analytics and Data Lake challenges on a massive scale, by leveraging modern microservices architecture and AWS data services.
Focus on automation and optimization for all aspects of data platform maintenance and deployment, with a special emphasis on data pipeline, big data processing, data platform design, and self-service analytics.
Key Requirements:
Bachelor's degree in computer science, Info Systems, Business, or related field.
Minimum of 2 years in Data Engineering
Experience with one or more query languages (SQL), schema definition languages (e.g., DDL), and scripting languages (Python and Terraform) to build innovative data solutions.
Proficiency or first experience in technical architectures, data modeling, infrastructure components, ETL/ELT, reporting/analytic tools, and extracting value from vast datasets.
Exposure to distributed system concepts, particularly from a data storage and computing perspective (e.g., data lake architectures).
Initial hands-on experience with AWS Big Data Services such as S3, Glue, Athena, Lambda, EMR, RDS, and Redshift.
Familiarity with at least one business intelligence reporting tool (Tableau, Quicksight, PowerBI).
Basic knowledge of DBA tasks and SQL performance tuning.
What We Offer:
Innovative Environment: A dynamic and collaborative work environment where innovation is encouraged.
Professional Growth: Opportunities for professional growth and development through continuous learning and exposure to cutting-edge technologies.
Competitive Package: Competitive salary and benefits package
Impactful Work: The chance to work on cutting-edge projects with industry-leading clients, making a tangible impact on their business success.
Due to the high volume of applicants, only shortlisted candidates will be contacted. We apologize for this inconvenience!"
Data Engineer,WorldQuant,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-worldquant-4311362995?position=45&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=oFbiJ8OEdKwI4pm1U3spvA%3D%3D,"WorldQuant develops and deploys systematic financial strategies across a broad range of asset classes and global markets. We seek to produce high-quality predictive signals (alphas) through our proprietary research platform to employ financial strategies focused on market inefficiencies. Our teams work collaboratively to drive the production of alphas and financial strategies – the foundation of a balanced, global investment platform.
WorldQuant is built on a culture that pairs academic sensibility with accountability for results. Employees are encouraged to think openly about problems, balancing intellectualism and practicality. Excellent ideas come from anyone, anywhere. Employees are encouraged to challenge conventional thinking and possess an attitude of continuous improvement.
Our goal is to hire the best and the brightest. We value intellectual horsepower first and foremost, and people who demonstrate an outstanding talent. There is no roadmap to future success, so we need people who can help us build it.
Technologists at WorldQuant research, design, code, test and deploy firmwide platforms and tooling while working collaboratively with researchers. Our environment is relaxed yet intellectually driven. We seek people who think in code and are motivated by being around like-minded people.
We are looking for an experienced Data Engineer who will help us to build and maintain an ecosystem for processing multiple datasets from different sources (both internal and external) vital to the firm’s investment operations.
What You'll Do
Creating automated data processing system and monitoring/maintaining it
Integrating multiple data sources and databases into one system
Developing interfaces and micro services in Python
Preprocessing and cleansing of semi-structured or unstructured data
Developing efficient algorithms for data processing
Testing and integrating external APIs
Supporting Business Analysts team
What You’ll Bring
A bachelor / master's degree in a technical or quantitative field from top university
At least 3 years of experience as a data engineer or software developer
Excellent programming skills
Experience with data processing using Python
Experience with building databases
Experience with Containers and Kubernetes
Scripting skills in UNIX environment: shell, python Fire, etc.
Experience with code versioning tools (e.g. Git), issue tracking tools (e.g. Jira)
Debugging skills, eye for detail and identifying problems
Strong problem-solving skills and an analytical mindset
A passion for working with data
What We Offer
Competitive and attractive compensation package with clear career road-map – where you feel challenged everyday
We offer a strong culture of learning and development: training courses, library, speakers, share and learn events
Learn from who sits next to you! Working in WQ you are surrounded by smart and talented people
Premium Health Insurance and Employee Assistance Program
Generous time-off policy, re-creation sabbatical leave (based on tenure), Trade Union benefits for staff and family
Team building activities every month: Local engagement events, monthly team lunch – Employee clubs: football, ping-pong, badminton, yoga, running, PS5, movies, etc.
Annual company trip and occasional global conferences – opportunity to travel and connect with our global teams
Happy-hour with tea break, snacks and meals every day in the office!
By submitting this application, you acknowledge and consent to terms of the WorldQuant Privacy Policy. The privacy policy offers an explanation of how and why your data will be collected, how it will be used and disclosed, how it will be retained and secured, and what legal rights are associated with that data (including the rights of access, correction, and deletion). The policy also describes legal and contractual limitations on these rights. The specific rights and obligations of individuals living and working in different areas may vary by jurisdiction.
WorldQuant is an equal opportunity employer and does not discriminate in hiring on the basis of race, color, creed, religion, sex, sexual orientation or preference, age, marital status, citizenship, national origin, disability, military status, genetic predisposition or carrier status, or any other protected characteristic as established by applicable law."
Data Engineer (Good English),Hitachi Consulting ( formerly Information Management Group),"Đà Nang, Da Nang City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-good-english-at-hitachi-consulting-formerly-information-management-group-4301556380?position=46&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=KJ%2FF%2BYyFA1p9aJoZWNHkGg%3D%3D,"Imagine the sheer breadth of talent it takes to inspire the future. We don’t expect you to ‘fit’ every requirement – your life experience, character, perspective, and passion for achieving great things in the world are equally important to us.
What You’ll Bring
3+ years of experience in data engineering, analytics, or BI roles.
Proficiency in SQL and Strong experiences in Python.
Experience with cloud data platforms (AWS, Azure).
Have experiences with PowerBI and familiar with Tableau, AWS Quicksight, Qlik,.. is a plus
Excellent communication and stakeholder management skills.
Championing diversity, equity, and inclusion
How We Look After You
We want to help you take care of your today and tomorrow – at home and at work. Which is why we offer industry-leading benefits that go far beyond compensation. That means support, services, and resources that also take care of your holistic health and wellbeing. We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. Here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent).
We’re a global team of innovators, together, we harness engineering excellence and passion for creating meaningful solutions to complex challenges. We turn organizations into industry leaders that can a make positive impact on their industries and society.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.
Fostering innovation through diverse perspectives
Hitachi is a global company operating across a wide range of industries and regions. One of the things that sets Hitachi apart is the diversity of our business and people, which drives our innovation and growth.
We are committed to building an inclusive culture based on mutual respect and merit-based systems. We believe that when people feel valued, heard, and safe to express themselves, they do their best work.
How We Look After You
We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status
or any other protected characteristic.
Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success."
Data Engineer (AI Conversation Team),FPT Smart Cloud,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-ai-conversation-team-at-fpt-smart-cloud-4265594399?position=47&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=kMHf2N5HueDx9lNmS%2BtOpg%3D%3D,"FPT Smart Cloud is hiring a Data Engineer
to join our AI Conversation Team, where you’ll design and optimize data pipelines that fuel intelligent conversational AI systems. This is a unique opportunity to work on large-scale AI products and collaborate with leading experts in natural language technologies.
About FPT Smart Cloud
FPT Smart Cloud (FCI)
– a member of FPT Corporation, pioneers AI & Cloud solutions in Vietnam. FCI was founded with the mission to generating an immense leap in productivity and agility in business operations.
FPT Smart Cloud aims at leading the industry by focusing on building a firm technological foundation, developing diversified ecosystem products, and reaching global connectivity.
Customized to specific needs: Providing cloud-based products and solutions customized to each industry.
All-in-one Platform: Consolidating FPT Smart Cloud technology and diverse business solutions all in one platform. AI & Cloud services are a Unify eco-system.
Local market leadership: Outstanding Cloud and AI technology infrastructure and platform to help local businesses grow their products and services online.
Deliver the future: Help customers achieve business outcomes faster by integrating world-class processes and technology.
Key Responsibilities
Collaborate with team members to develop data platform projects in department
Build, maintain and automate data pipeline architecture (data ingestion, moving though to data storage, data processing, data delivery, …)
Optimize data delivery, infrastructure for greater scalability
Build data quality metrics, data validation and monitoring tools for tracking data, alert, utilizing the data pipeline
Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs.
Participate in community group’s activities to research new technology, sharing knowledge between teams
Participate in 1-1 review with team leader / line manager for progress reporting and feedback and career development
Contribute to team documents including (technical documents, detailed design documents, process-related documents, onboarding documents…)
Requirements
3–5 years of experience in a Data Engineer role, ideally supporting AI/ML teams or products.
Proficient in at least one programming language (Python or Java).
Strong SQL skills and hands-on experience with both relational (MySQL, PostgreSQL) and NoSQL databases (MongoDB).
Experience with big data processing frameworks such as Apache Spark, Apache Kafka.
Hands-on experience with orchestration tools like Apache Airflow.
Familiar with version control and DevOps practices (Git, CI/CD, Docker, Kubernetes).
In-depth knowledge of cloud-based storage and caching systems (e.g., S3, Redis, Elasticsearch).
Experience working with at least one cloud platform (AWS, GCP, or Azure).
Familiarity with messaging systems such as Kafka or RabbitMQ.
Bonus Points
Experience working with unstructured data (text, conversations, transcripts).
Familiarity with ML data workflows or AI model lifecycle.
Exposure to vector databases or embedding-based data systems is a plus.
Soft Skills
Understanding and experience working with Agile/Scrum methodologies.
Strong communication skills, able to work in cross-functional teams with engineers, researchers, and product owners in a dynamic, fast-paced environment.
Top Benefits
Salary: Competitive, pay according to ability. Negotiation during the interview.
Social insurance and health insurance according to labor laws.
Creative, open-minded working environment that respects individuals
FPT Premium Care package
Activities and culture with FCI and FPT Corporation
Study support package for children of FCI union
Sponsor related courses and certifications
Working environment
Working Location:
Site Hanoi: 7th Floor, FPT Tower, no. 10 Pham Van Bach Street, Dich Vong ward, Cau Giay district, Hanoi.
Working hours:
8h30 AM – 12h00 PM
1h00 PM – 5h30 PM
Contact Person
Pham Thi Ha My (Ms.) – Talent Acquisition Team Lead
Email: Mypth3@fpt.com
Phone: 0962456194
FPT Smart Cloud (FCI) Co., LTD
Address: 7th Floor, FPT Tower, No. 10 Pham Van Bach, Cau Giay Dist, Hanoi
Websites: FPT Cloud | FPT AI
AI Engineer
Xem thêm các vị trí nổi bật khác tại đây
: https://fptsmartcloud.com/co-hoi-nghe-nghiep"
Python Software Engineer,HCLTech Vietnam,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/python-software-engineer-at-hcltech-vietnam-4303316638?position=48&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=5scLuISST6JD6pIF6N8UZA%3D%3D,"Python Software Engineer (CLOUD) – VERY OPEN TO NEGOTIATE
We highly appreciate your interest in this position of HCLTECH Vietnam. After reviewing all applications, only qualified candidates will be contacted for the next steps within 15 days from date of submission.
Python Software Engineer (CLOUD)
JOB LOCATION
ABOUT HCLTECH VIETNAM COMPANY LIMITED
HCLTECH Vietnam Company Limited belongs to HCLTech.
HCLTech is a global technology company, home to more than 223,400 people across 60 countries,
delivering industry-leading capabilities centered around digital, engineering, cloud, and AI, powered by a broad portfolio of technology services and products. We work with clients across all major verticals, providing industry solutions for Financial Services, Manufacturing, Life Sciences and Healthcare, Technology and Services, Telecom and Media, Retail and CPG, and Public Services. Consolidated revenues as of 12 months ending June 2023 totaled $12.8 billion. To learn how we can supercharge progress for you, visit https://www.hcltech.com/.
Our Vietnam-based offices:
• Hanoi Office: Level 13-17, Leadvisors Tower, 643 Pham Van Dong Street, Co Nhue Ward, North
Tu Liem District, Hanoi
• HCMC Office: Level 11, Five Star Tower, 28Bis Mac Dinh Chi Street, Da Kao Ward, District 1, HCMC
DUTIES & RESPONSIBILITIES:
Our purpose is to shape a world where people and communities thrive. We are making this happen by improving our customers’ financial wellbeing so they can achieve incredible things – buying a home, building a business, or saving for big or small things. We are looking for people who are passionate about transformational change and to help us redefine banking for the future. Be a part of building the ideal data ecosystem from scratch. Ensuring the right data is generated from all
applications and sourced at the right velocity with the complete depth and breadth to ensure complete coverage and reuse. This is your opportunity to build new, not fix old.
What will your day look like:
• Building and running the data processing pipeline on Google Cloud Platform
• Work with Implementation teams from concept to operations to provide deep technical expertise for successfully deploying large scale data solutions in the enterprise and use modern
data/analytics technologies on GCP
• Design pipelines and architectures for data processing
• Implement methods for DevOps automation of all parts of the built data pipelines to deploy from development to production
• Formulate business problems as technical data problems while ensuring that key business drivers
are captured in collaboration with product management
• Extract, load, transform, clean and validate data
• Support and debug data pipelines
JOB REQUIREMENTS
We are trying to reimagine the way we help and interact with our customers, so we are looking for
candidates with creativity, an open mind and positive energy. Our requirements are as below:
Bachelor’s degree in computer science, Software Engineering, or a related field
At least 2 Years of Experience working on developing backend / platform applications using modern programming languages such as Python, Java, JS, Go, Rust.
Experience with provisioning services on top of Cloud Platforms (AWS, GCP, Azure) & Infrastructure as Code (Terraform, CDK, Pulumi...)
Experience with constructing CI/CD workflows.
Experience with (or having knowledge on) building data intensive applications.
Excellent problem-solving skills and the ability to analyze and resolve complex technical issues.
Working proficiency in English, both verbal and writing.
Nice to have:
Experience in developing big-data pipelines (batch, streaming processing) is a huge plus
Experience in developing automation processes and Observability is a huge plus.
Interested in working with data platforms / eager to learn more about data is a huge plus.
Familiar with Docker, K8S
WHY YOU SHOULD JOIN US:
• Very challenging project with ANZ Bank where latest technology are applied and talented engineers are gathered
• Attractive package including base salary + 13th month salary + Performance Bonus.
• Insurance based on full base salary and Medical Benefit (Bao Viet Insurance Package) for
Employee and Family
• 100% of full salary and benefits as an official employee from the 1st day of working
• 100% salary during probation time
• Working in a fast paced, flexible, and multinational working environment with opportunity to
travel onsite (in 60 countries)
• Internal Training (Technical & Functional & English)
• Working with outstanding colleagues coming from top universities and top companies in Vietnam and foreign countries
• 18 paid leaves per year
• Hybrid Mode (3 working days at office, flexible time)
We highly appreciate your interest in this position of HCLTECH Vietnam. After reviewing all applications, only qualified candidates will be contacted for the next steps within 15 days from date of submission."
Data Engineer (Data Suite),FPT Smart Cloud,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-data-suite-at-fpt-smart-cloud-4312970554?position=49&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=y1xa5wk0GSbi%2B4JB2rjWZA%3D%3D,"FPT Smart Cloud is hiring a Data Engineer for the Data Suite project
to design, build, and maintain the data infrastructure powering our AI-driven analytics and software solutions. In this role, you will be responsible for developing scalable data pipelines, managing data platforms, and ensuring the reliability, performance, and efficiency of data processing systems across both development and production environments.
About FPT Smart Cloud
FPT Smart Cloud (FCI)
– a member of FPT Corporation, pioneers AI & Cloud solutions in Vietnam. FCI was founded with the mission to generating an immense leap in productivity and agility in business operations.
FPT Smart Cloud aims at leading the industry by focusing on building a firm technological foundation, developing diversified ecosystem products, and reaching global connectivity.
Customized to specific needs: Providing cloud-based products and solutions customized to each industry.
All-in-one Platform: Consolidating FPT Smart Cloud technology and diverse business solutions all in one platform. AI & Cloud services are a Unify eco-system.
Local market leadership: Outstanding Cloud and AI technology infrastructure and platform to help local businesses grow their products and services online.
Deliver the future: Help customers achieve business outcomes faster by integrating world-class processes and technology.
Key Responsibilities
Design, develop, and maintain high-performance, scalable data pipelines to support data processing needs in both batch and real-time modes.
Implement and maintain data models, data marts, and data warehouses using standard methodologies such as Kimball and Slowly Changing Dimensions (SCD) solutions.
Collaborate with stakeholders to identify data requirements, build ETL/ELT processes, optimize data ingestion workflows, and develop reports.
Work with both cloud and on-premise data sources (e.g., AWS Redshift, Google BigQuery, ClickHouse, Druid) to collect data for ETL processes.
Develop solutions for data streaming and real-time processing using tools such as Apache Spark, Flink, Kafka, or similar.
Ensure data quality through monitoring, logging, and implementing data governance practices.
Work closely with Data Analysts to align on data architecture and reporting requirements.
Automate and optimize data workflows using CI/CD pipelines and Kubernetes infrastructure.
Stay up-to-date with big data technology trends and industry best practices.
Requirements
5+ years of experience in software development, with at least 3 years in Data Engineering.
Strong SQL skills with hands-on experience in common databases (Oracle, MSSQL, MySQL).
Experience with cloud data warehouses such as Redshift or Google BigQuery.
Experience with on-premise data warehouse solutions like ClickHouse, Postgres, Druid, or Kylin.
Familiarity with NoSQL databases (e.g., MongoDB, Redis, Cassandra, HBase).
In-depth understanding of data modeling principles such as Kimball methodology.
Advanced Python programming skills; Scala experience is a plus.
Experience with big data frameworks such as Hadoop, Spark, Trino, and Druid.
Experience with data streaming platforms (e.g., Spark Streaming, Flink).
Experience building data pipeline monitoring and data quality control systems.
Proficiency in building data platforms in cloud environments (Azure, AWS).
Experience with CI/CD and Git for Kubernetes deployments.
Understanding of DevOps is a plus.
Knowledge of BI tools and techniques for optimizing query latency and data platform cost.
Top Benefits
Salary: Competitive, pay according to ability. Negotiation during the interview.
Social insurance and health insurance according to labor laws.
Creative, open-minded working environment that respects individuals
FPT Premium Care package
Activities and culture with FCI and FPT Corporation
Study support package for children of FCI union
Sponsor related courses and certifications
Working Environment
Working Location:
Site HCMC: 3rd floor, PJICO Tower, no. 186 Dien Bien Phu, Ward 6, District 3, HCMC.
Working hours:
8h30 AM – 12h00 PM
1h00 PM – 5h30 PM
Working days
: Monday to Friday (weekends off)
Contact Person
Pham Thi Ha My (Ms.) – Talent Acquisition Team Lead
Email: Mypth3@fpt.com
Phone: 0962456194
FPT Smart Cloud (FCI) Co., LTD
Address: 7th Floor, FPT Tower, No. 10 Pham Van Bach, Cau Giay Dist, Hanoi
Websites: FPT Cloud | FPT AI
#Data Engineer
Xem thêm các vị trí nổi bật khác tại đây
: https://fptsmartcloud.com/co-hoi-nghe-nghiep"
Data Engineer,NCCPLUS VIETNAM,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-nccplus-vietnam-4312690120?position=50&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=CnoDYC4nfIHEtsv68LGN8w%3D%3D,"NCCPLUS Vietnam Joint Stock Company is a software firm and IT outsourcing service provider located in Vietnam. With 10 years of experience, we specialize in mobile and web application development, cloud and microservices development, game development, blockchain development, software testing, AI and IoT, and VoIP development. We work with clients in various fields such as public administration, education, finance & banking, insurance, healthcare, retail & ecommerce, sports, and entertainment. Our mission is to constantly enhance knowledge and capabilities to meet the divergent needs of the market and become a symbol of the IT outsourcing field.
1. JOB DESCRIPTION
Building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Design, develop, and maintain data pipelines, warehouses, datalake.
Build the data products that technical users will depend on for business intelligence and ad-hoc access.
Work side by side with our Data Science to build and automate data pipeline, data ETL, etc. on distributed data processing platforms such as Spark.
Prepare data inputs for a generic blueprint ""model builder"".
Build production data pipeline for daily ETL and model retraining.
End-to-end data processing, troubleshooting, and problem diagnosis.
2. YOUR SKILLS AND EXPERIENCE
At least 3 years of working experience with Python, with a strong focus on ETL processes and Cloud Functions development.
Proficiency in Google Cloud Platform (GCP) stack, including Cloud Functions, Workflows, Pub/Sub, Dataform, and BigQuery.
Strong skills in data modeling and SQL optimization for efficient data processing and querying.
Experience with orchestration tools (e.g., Airflow, Dataform) and CI/CD pipelines for data workflows.
Solid understanding of event-driven architectures and their implementation in data pipelines.
Good at multi-threading, atomic operations, and computation frameworks such as Spark (Dataframe, DBT, SQL, etc.), distributed storage, and distributed computing.
Good at communication & teamwork.
Being open-minded, and willing to learn new things.
Requires proficiency in English, working directly with foreign partners.
3. WHY YOU'LL LOVE WORKING HERE
Hybrid Policy: Work from home up to 3 days per week.
Total Income = Net salary + Performance bonus (>14 months salary).
Review salary twice per year base on your performance and output.
Holiday bonus on 1/1, 30/4, 1/5, 2/9 and personal birthday.
Lunch allowance of 1,000,000 VND per month.
PVI Care comprehensive health insurance package for yourself and your family, annual health check.
NCC women receive full salary maternity allowance.
Participating in Knowledge Sharing sessions with OpenTalk, MiniTalk weekly.
Be equipped with modern PC + Laptop equipment.
Enjoy all of our activities such as Company Trip, Company's Birthday, YEP, Sport/Game/Code War Contest,...
We have these clubs for you to join: Football, Table football, Music, English, media and more.
The office is pleasant, and fully equipped with an attractive pantry with snacks, tea, coffee, and milo,...
Great promotion opportunities and career paths.
Working hours: Morning: 8h30 – 12h00; Afternoon: 13h00 – 17h30. Monday to Friday (Having to attend 2 OpenTalk sessions per month).
Address: 8th Floor, St.Moritz Tower, 1014 Pham Van Dong, Hiep Binh, Ho Chi Minh.
Website: https://ncc.plus/
Facebook: https://www.facebook.com/nccplusvietnam and
https://www.facebook.com/TuyendungNCCSoft"
Senior/Lead Data Engineer (Sign-on Bonus),GFT Technologies,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/senior-lead-data-engineer-sign-on-bonus-at-gft-technologies-4284769768?position=51&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=w%2BdE78stKZ7Wv2RCFbwiSw%3D%3D,"What do we do?
GFT Technologies is driving the digital transformation of the world’s leading financial institutions. Other sectors, such as industry and insurance, also leverage GFT’s strong consulting and implementation skills across all aspects of pioneering technologies, such as cloud engineering, artificial intelligence, the Internet of Things for Industry 4.0, and blockchain.
With its in-depth technological expertise, strong partnerships and scalable IT solutions, GFT increases productivity in software development. This provides clients with faster access to new IT applications and innovative business models, while also reducing risk.
We’ve been a pioneer of near-shore delivery since 2001 and now offer an international team spanning 16 countries with a global workforce of over 9,000 people around the world. GFT is recognised by industry analysts, such as Everest Group, as a leader amongst the global mid-sized Service Integrators and ranked in the Top 20 leading global Service Integrators in many of the exponential technologies such as Open Banking, Blockchain, Digital Banking, and Apps Services.
Role Summary
As a Senior/ Data Engineer at GFT, you will be responsible for managing, designing, and enhancing data systems and workflows that drive key business decisions. The role is focused 75% on data engineering, involving the construction and optimization of data pipelines and architectures, and 25% on supporting data science initiatives through collaboration with data science teams for machine learning workflows and advanced analytics. You will leverage technologies like Python, Airflow, Kubernetes, and AWS to deliver high-quality data solutions.
Sign-on Bonus
: Eligible for candidates who are currently employed elsewhere and able to join GFT
within 30 days
of offer acceptance.
Key Activities
Architect, develop, and maintain scalable data infrastructure, including data lakes, pipelines, and metadata repositories, ensuring the timely and accurate delivery of data to stakeholders
Work closely with data scientists to build and support data models, integrate data sources, and support machine learning workflows and experimentation environments
Develop and optimize large-scale, batch, and real-time data processing systems to enhance operational efficiency and meet business objectives
Leverage Python, Apache Airflow, and AWS services to automate data workflows and processes, ensuring efficient scheduling and monitoring
Utilize AWS services such as S3, Glue, EC2, and Lambda to manage data storage and compute resources, ensuring high performance, scalability, and cost-efficiency
Implement robust testing and validation procedures to ensure the reliability, accuracy, and security of data processing workflows
Stay informed of industry best practices and emerging technologies in both data engineering and data science to propose optimizations and innovative solutions
Required Skills
Core Expertise: Proficiency in Python for data processing and scripting (pandas, pyspark), workflow automation (Apache Airflow), and experience with AWS services (Glue, S3, EC2, Lambda)
Containerization & Orchestration: Experience working with Kubernetes and Docker for managing containerized environments in the cloud
Data Engineering Tools: Hands-on experience with columnar and big data databases (Athena, Redshift, Vertica, Hive/Hadoop), along with version control systems like Git
Cloud Services: Strong familiarity with AWS services for cloud-based data processing and management
CI/CD Pipeline: Experience with CI/CD tools such as Jenkins, CircleCI, or AWS CodePipeline for continuous integration and deployment
Data Engineering Focus (75%): Expertise in building and managing robust data architectures and pipelines for large-scale data operations
Data Science Support (25%): Ability to support data science workflows, including collaboration on data preparation, feature engineering, and enabling experimentation environments
Nice-to-have requirements
Advanced Data Science Tools: Experience with AWS Sagemaker or Databricks for enabling machine learning environments
Big Data & Analytics: Familiarity with both RDBMS (MySQL, PostgreSQL) and NoSQL (DynamoDB, Redis) databases
BI Tools: Experience with enterprise BI tools like Tableau, Looker, or PowerBI
Messaging & Event Streaming: Familiarity with distributed messaging systems like Kafka or RabbitMQ for event streaming
Monitoring & Logging: Experience with monitoring and log management tools such as the ELK stack or Datadog
Data Privacy and Security: Knowledge of best practices for ensuring data privacy and security, particularly in large data infrastructures
What can we offer you?
Competitive salary
13th-month salary guarantee
Performance bonus
Professional English course for employees
Premium health insurance
Extensive annual leave
Due to the high volume of applications we receive, we are unable to respond to every candidate individually. If you have not received a response from GFT regarding your application within 10 workdays, please consider that we have decided to proceed with other candidates. We truly appreciate your interest in GFT and thank you for your understanding."
Data Engineer,CMC CONSULTING,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-cmc-consulting-4270951204?position=52&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=qn7JDfilJ%2F8PqEV2B%2Bo04g%3D%3D,"CMC Consulting
Build and maintain ETL/ELT processes, data streaming, and integration with the SAP ecosystem, Cloud data platform. Deep knowledge of data engineering and the ability to work with structured and unstructured data.
Key Responsibilities
Build scalable ETL/ELT processes for structured and unstructured SAP data
Establish and maintain data lakehouse architecture and real-time integration with SAP
Collaborate with analysts and data scientists to ensure data accessibility and quality
Optimize data processing workflows to enhance performance and scalability
Implement Feature Engineering solutions for ML/AI models from SAP data
Requirements
Minimum 3 years of experience working with ETL/ELT and data streaming
Experience with big data technologies such as Spark, Kafka, or Databricks
Experience with at least one public cloud platform (AWS, Azure, GCP)
Strong programming skills with Python, SQL, and related data processing technologies
Technical Skills
ETL/ELT Other: Airflow, dbt, Fivetran, Talend
Streaming: Kafka, Spark Streaming, DataFlow
Cloud: AWS (Glue, EMR), Azure (Data Factory, Synapse), GCP (Dataflow, BigQuery)
Programming languages: Python, SQL, ABAP (basic)
Data lakehouse: Databricks, Delta Lake, Snowflake
Feature Store Technologies: Feast, Tecton, or equivalent
Benefits
Competitive base salary and KPI bonus.
Attractive bonus program according to Company’s regulation.
100% of gross offer during probation period.
Join full of insurance benefits as prescribed by Vietnamese Labor law after signing labor contract.
Young, dynamic, professional working environment with its own corporate culture.
Annually summer trip, teambuilding and company events.
CMC Healthcare programs.
5 working days per week, from Monday – Friday.
Opportunity to self-development training, encourage creativity and innovation
A team of leaders and managers who value talent, helping employees maximize their capacity."
Software Engineer,Microsoft,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/software-engineer-at-microsoft-4288381091?position=53&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=Qn1CQH2t5MT%2BE8y6QeF34A%3D%3D,"M365 Big Data Optimization team is hiring a
Software Engineer
to work on big data efficiency improvements for M365 worldwide.
Hundreds of millions of global customers use M365 products every day. The engineering, telemetry and signals, datasets behind this are at world’s largest scale: 800+ PB storage and a million CPU cores. There are various kinds of data platforms and systems working with each other to deliver efficient services. We provide large scale and unified datasets, span our portfolio for M365 world-class products and applications, collaborate with data scientists/product teams/sales/marketing teams to gain more product insights, maximize customer value, and reduce cost.
Our team manages this huge and critical big data capacity and platform, we continuously optimize this giant storage & processing resources for M365 worldwide. We have achieved 200 million dollars cost reduction in the past few years and have accumulated a wealth of technical experience in distributed storage and processing fields. By joining us, you will also become experts on big data efficiency, and share your best practices with thousands of M365 engineers all over the world. There are lots of opportunities and challenges that you can show your passion and talents in big data, improve the efficiency and reliability of the systems, pursue engineering excellence, and make tremendous influence!
Microsoft’s mission is to empower every person and every organization on the planet to achieve more. As employees we come together with a growth mindset, innovate to empower others, and collaborate to realize our shared goals. Each day we build on our values of respect, integrity, and accountability to create a culture of inclusion where everyone can thrive at work and beyond.
Responsibilities
Design and build highly efficient, lower cost dataset for storage and pipeline for processing.
Research, analyze, and investigate optimization opportunities on Office365 Big Data platform.
Ensure Office365 capacity stay in allocated quota by optimization in both storage and processing.
Design tools, analyze and leverage data mining, generate BI reports to improve the efficiency of data capacity & resource management.
Qualifications
Required Qualifications:
Bachelor's Degree in Computer Science, or related technical discipline with proven experience coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python
OR equivalent experience.
Solid Computer Science fundamentals.
Solid coding skills (no language restrictions).
Experience on design/development with C#/SQL Server/ASPX/JavaScript/Azure.
Quick learning and solid problem solving, design, coding, and debugging skills.
Communication in English: reading, writing, listening, and speaking.
Cross-team collaboration skills.
Accountable and proactive.
Data pipeline and optimization knowledge is a plus.
Experience with distributed system is a plus.
Experience or strong interest in leveraging AI to improve engineering efficiency, such as automating workflows, enhancing tooling, or identifying optimization opportunities.
Other Requirements
Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include but are not limited to the following specialized security screenings:
Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.
Preferred Qualifications
Bachelor's Degree in Computer Science
OR related technical field AND 1+ year(s) technical engineering experience with coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python
OR Master's Degree in Computer Science or related technical field with proven experience coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python
OR equivalent experience.
Microsoft is an equal opportunity employer. Consistent with applicable law, all qualified applicants will receive consideration for employment without regard to age, ancestry, citizenship, color, family or medical care leave, gender identity or expression, genetic information, immigration status, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran or military status, race, ethnicity, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable local laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application process, read more about requesting accommodations."
Software Engineer - Dashboard,PLS,Vietnam,https://vn.linkedin.com/jobs/view/software-engineer-dashboard-at-pls-4206937002?position=54&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=rLNYVDoP%2FBGLYxxtsd5YAg%3D%3D,"PLS Pte Ltd is a dynamic company committed to delivering innovative solutions across diverse industries. Our vision is to lead globally by creating impactful solutions, while our mission is to provide exceptional value through expertise, collaboration, and creativity. We offer various services, including custom software solutions, cutting-edge technology, consulting, project management, and quality assurance. Our strengths lie in innovation, expertise, customer-centricity, and integrity. Join us to transform challenges into opportunities and drive growth together.
You Will Be Responsible For
Design, develop, and maintain web applications using C#, React.js, and MS SQL Server.
Collaborate with cross-functional teams to gather and analyze requirements.
Write clean, efficient, and well-documented code.
Troubleshoot and debug applications to ensure optimal performance.
Participate in code reviews and contribute to best practices and continuous improvement.
Ideal Profile
Minimum 3 years of hands-on experience in software development with C#, React.js, and MS SQL Server.
Strong understanding of software development principles and design patterns.
Experience with REST APIs, and agile development methodologies.
Self-motivated, responsible, and able to work independently in a remote environment.
Good communication skills and a collaborative mindset.
What's on Offer?
Work alongside & learn from best in class talent
A role that offers a breadth of learning opportunities
Fantastic work culture"
Senior Data Engineer (Upto $4000),"BLUE BELT TECHNOLOGY CO.,LTD",Hanoi Capital Region,https://vn.linkedin.com/jobs/view/senior-data-engineer-upto-%244000-at-blue-belt-technology-co-ltd-4311254160?position=55&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=ypczbfSls1x2%2B%2F9%2FyiBCVw%3D%3D,"Introduction
With over a decade of experience in IT and fintech, Blue Belt has become a leading software development company, delivering innovative technology solutions to a diverse global clientele. We specialize in developing web, mobile, payment, and blockchain applications that offer seamless user experiences. Headquartered in Tokyo, Japan, with a state-of-the-art Technology Hub in Hanoi, Vietnam, Blue Belt operates in more than ten countries, including Japan, Thailand, Indonesia, the Philippines, Malaysia, Taiwan, and Brazil. Our team of over 200 professionals brings a wealth of expertise to drive our global operations.
We are seeking a
Senior Data Engineer
to take ownership of building and scaling our data infrastructure. This role is ideal for someone who is deeply experienced with the modern data stack and is hands-on with designing, implementing, and maintaining robust ETL/ELT pipelines. You will play a pivotal role in shaping the company’s data strategy, enabling data-driven decision-making across the organization, and mentoring junior team members.
Job Description
Lead the design, implementation, and optimization of scalable ETL/ELT pipelines across various data sources.
Architect and manage data platforms and warehouse solutions (e.g., BigQuery, Snowflake, Redshift).
Champion best practices in data modeling, pipeline orchestration, and transformation workflows (e.g., with dbt, Airflow).
Collaborate closely with Product, Engineering, and Analytics teams to align data architecture with business goals.
Ensure data quality, lineage, governance, and compliance across all systems.
Drive the adoption of modern data tools and processes, including streaming data (Kafka, Spark), real-time analytics, and observability.
Troubleshoot complex data issues and ensure high reliability and uptime of production pipelines.
Mentor and provide technical guidance to other engineers and data practitioners.
Key requirements for this position include:
5+ years of experience in data engineering, preferably in fast-paced environments.
Bachelor's degree in computer science, Information Technology or other technical field preferred from TOP UNIVERSITY specializing in Information Technology
Proficient in Python, SQL, and data pipeline orchestration tools like Airflow, Prefect, or Dagster.
Hands-on experience with cloud data warehouses (e.g., BigQuery, Snowflake) and data transformation tools (e.g., dbt).
Strong understanding of data warehousing concepts, dimensional modeling, and data architecture patterns.
Comfortable working with large volumes of structured and semi-structured data (JSON, Parquet, etc.).
Familiarity with stream processing technologies (Kafka, Spark Streaming).
Knowledge of CI/CD, version control, and infrastructure-as-code for data workflows.
Excellent communication skills and ability to collaborate cross-functionally."
Principal Data Engineer,Sleek,Vietnam,https://vn.linkedin.com/jobs/view/principal-data-engineer-at-sleek-4303999783?position=56&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=Q%2B20sBQM95N8A6NOFeIYMw%3D%3D,"Through proprietary software and AI, along with a focus on customer delight, Sleek makes the back-office easy for micro SMEs.
We give Entrepreneurs time back to focus on what they love doing - growing their business and being with customers. With a surging number of Entrepreneurs globally, we are innovating in a highly lucrative space.
We operate 3 business segments:
Corporate Secretary
: Automating the company incorporation, secretarial, filing, Nominee Director, mailroom and immigration processes via custom online robots and SleekSign. We are the market leaders in Singapore with :5% market share of all new business incorporations
Accounting & Bookkeeping
: Redefining what it means to do Accounting, Bookkeeping, Tax and Payroll thanks to our proprietary SleekBooks ledger, AI tools and exceptional customer service
FinTech payments
: Overcoming a key challenge for Entrepreneurs by offering digital banking services to new businesses
Sleek launched in 2017 and now has around 15,000 customers across our offices in Singapore, Hong Kong, Australia and the UK. We have around 500 staff with an intact startup mindset.
Backed by world-class investors, we are on track to be one of the few cash flow positive, tech-enabled unicorns based out of Singapore.
Requirements
We are looking for a Principal Data Engineer that is excited about the below Mission and Outcomes over the next 6-12 months.
Work closely with cross-functional teams to translate our business vision into impactful data solutions. Drive the alignment of data architecture requirements with strategic goals, ensuring each solution not only meets analytical needs but also advances our core objectives. Be pivotal in bridging the gap between business insights and technical execution by tackling complex challenges in data integration, modeling, and security, and by setting the stage for exceptional data performance and insights. Shape the data roadmap, influence design decisions, and empower our team to deliver innovative, scalable, high-quality data solutions every day.
Architecture & Design:
Define the overall greenfield data architecture (batch + streaming) using GCP - BigQuery.
Establish best practices for ingestion, transformation, data quality, and governance
Data Ingestion & Processing:
Lead the design and implementation of ETL/ELT pipelines:
Ingestion: Datastream, Pub/Sub, Dataflow, Airbyte, Fivetran, Rivery
Storage & Compute: BigQuery, GCS
Transformations: dbt, Cloud Composer (Airflow), Dagster
Ensure data quality and reliability with dbt tests, Great Expectations/Soda, and monitoring
Governance & Security:
Implement Dataplex & Data Catalog for metadata, lineage, and discoverability
Define IAM policies, row/column-level security, DLP strategies, and compliance controls
Monitoring, Observability & Reliability:
Define and enforce SLAs, SLOs, and SLIs for pipelines and data products
Implement observability tooling:
Cloud-native: Cloud Monitoring, Logging, Error Reporting, Cloud Trace
Third-party (nice-to-have): Monte Carlo, Datafold, Databand, Bigeye
Build alerting and incident response playbooks for data failures and anomalies
Ensure pipeline resilience (idempotency, retries, backfills, incremental loads)
Establish disaster recovery and high availability strategies (multi-region storage, backup/restore policies)
Analytics Enablement:
Partner with BI/analytics teams to deliver governed self-service through Looker, Looker Studio, and other tools
Support squad-level data product ownership with clear contracts and SLAs
Team Leadership
Mentor a small data engineering team; set coding, CI/CD, and operational standards
Collaborate with squads, product managers, and leadership to deliver trusted data
To do this you will have:
10+ years experience in data engineering, architecture, or platform roles
Strong expertise in GCP data stack: BigQuery, GCS, Dataplex, Data Catalog, Pub/Sub, Dataflow
Hands-on experience building ETL/ELT pipelines with dbt + orchestration (Composer/Airflow/Dagster)
Deep knowledge of data modeling, warehousing, partitioning/clustering strategies
Experience with monitoring, reliability engineering, and observability for data systems
Familiarity with data governance, lineage, and security policies (IAM, DLP, encryption)
Strong SQL skills and solid knowledge of Python for data engineering
Nice-to-Have
Experience with Snowflake, Databricks, AWS (Redshift, Glue, Athena), or Azure Synapse
Knowledge of open-source catalogs (DataHub, Amundsen, OpenMetadata)
Background in streaming systems (Kafka, Kinesis, Flink, Beam)
Exposure to data observability tools (Monte Carlo, Bigeye, Datafold, Databand)
Prior work with Looker, Hex, or other BI/analytics tools
Startup or scale-up experience (fast-moving, resource-constrained environments)
Behavioural fit is also important at Sleek, and we will be looking for candidates that have a proven track record of embodying the below attributes in their recent roles:
This shows reliability and helps build trust within the team. We move fast and need to know that everyone will see things through to completion and proactively help to get things back on track when challenges arise. Accountability is really important to us.
There is so much we don't know. Humility allows for open-mindedness to feedback and a willingness to learn from others. It paves the way for collaboration and creates a positive work environment. It is a key ingredient of self awareness and emotional intelligence.
Structured Thinking
: Our business is complex with many layers (many services, many countries, many cultures). Regardless of whether you're more analytical or creative in nature, being able to show sound judgement is important to us. It ensures solutions are pragmatic and balance the needs of the organisation, team and customers.
Attention to detail
: You'll be managing multiple, complex workstreams from numerous stakeholders. It will be important for you to keep track of everything and notice when information is missing or inconsistent.
Excellent listener and clear communicator
: We have a variety of nationalities and for many people, English isn't their first language. For you to excel, you'll need to be present in your calls and make sure you properly receive and thoughtfully send messages to others in the business.
About The Interview Process
The successful candidate will participate in the below interview stages (note that the order might be different to what you read below).
We anticipate the process to last no more than 3 weeks from start to finish. Whether the interviews are held over video call or in person will depend on your location and the role.
Screening call
A :30 minute chat with our Talent Acquisition team to learn more about you, your background, and what you're looking for.
Technical screening
A :60 minute call with a Senior Data Engineer to cover some high-level technical concepts and ensure alignment on core skills.
Technical competency panel
Behavioural panel interview
A :60 minute conversation with two of our business leaders, where we'll dive into recent work situations to understand how you collaborate and operate day-to-day.
Final interview
A closing conversation with our CTO, giving you the opportunity to ask any final questions and hear more about our vision and priorities.
Offer + references
We'll make a non-binding offer verbally or by email, followed by a couple of short phone or video calls with references that you provide.
Requirement for background screening
Please be aware that Sleek is a regulated entity and as such is required to perform different levels of background checks on staff depending on their role.
This may include using external vendors to verify the below:
Your education
Any criminal history
Any political exposure
Any bankruptcy or adverse credit history
We will ask for your consent before conducting these checks. Depending on your role at Sleek, an adverse result on one of these checks may prohibit you from passing probation.
By submitting a job application, you confirm that you have read and agree to our Data Privacy Statement for Candidates, found at sleek.com.
Benefits
Some other great things about working at Sleek...
Humility and kindness
: Humility is a core attribute we hire for, which means we have a culture of not taking ourselves too seriously and being able to laugh. Kindness is also incredibly important. We are committed to creating and nurturing a diverse and inclusive environment.
Flexibility
: You'll be able to work from home 5 days per week. If you need to start early or start late to cater to your family or other needs, we don't mind, so long as you get your work done and proactively communicate. You can also work fully remote from anywhere in the world for 1 month each year
Financial benefits
: We pay competitive market salaries and provide staff with generous paid time off and holiday schedules. Certain staff at Sleek are also eligible for our employee share ownership plan and can share in the upside of our stellar growth trajectory as we work toward listing on a prominent stock exchange in the Asia Pacific region.
Personal growth
: You'll get a lot of responsibility and autonomy at Sleek - we move at a fast pace so you'll be making decisions, making mistakes and learning. There's also a range of internal and external facing training programmes we run. We're also at the forefront of utilising AI in our space and are developing a regional centre of AI excellence. It is our intention that if you leave Sleek, you leave as a more well-rounded person and professional."
Software Engineer (Frontend - Pluto Connect),Pluto,Vietnam,https://vn.linkedin.com/jobs/view/software-engineer-frontend-pluto-connect-at-pluto-4157199203?position=57&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=HloXBPpC58Pr3txT3YvHzw%3D%3D,"Company Overview
Pluto is a modern financial operating system for global businesses. We combine global payments infrastructure with intelligent workflow automation to provide businesses with unparalleled insights, control, and automation over their spending.
Pluto is a venture-capital funded startup backed by top-tier Silicon Valley investors and built by folks from some of the best technology institutions in the world -- Google, Shopify, Square, Uber, PayPal, Cash App, Digital Ocean, Y Combinator, and more
We are reinventing corporate finance for the modern age and are solving challenging problems in the B2B payments space. With a stronghold in Dubai and sights set on expansion, we're now looking for a seasoned frontend engineer to help shape our next chapter
As a Frontend Engineer, you will be developing cutting-edge web applications and libraries for Pluto Connect - our modern embedded spend management platform for banks and financial institutions. This role involves collaborating with high-performing cross-functional teams and external stakeholders to build and ship high-impact integration projects.
Key Responsibilities
Develop and maintain complex software applications using React, Typescript, and GraphQL
Lead the development lifecycle of features and work streams from design to deployment
Collaborate with product managers, designers, and backend engineers to deliver world-class products
Become a voracious problem solver who can tackle difficult issues on tight timelines, manage cross-functional dependencies and set an exemplary bar for engineering within the organization
Write and maintain clean, maintainable and testable code, leveraging design patterns and architecture where appropriate
Participate in code reviews, pair-programming sessions and provide constructive feedback to peers
Develop a strong understanding of the domain and bring a deep product and design mindset while building solutions
Candidate Profile
Bachelor’s or Master’s degree in Computer Science, Software Engineering or related field
Strong fundamentals in computer science, algorithms and data structures
3+ years of experience in software development
Experienced in React and Typescript, familiar with GraphQL
Strong understanding of web fundamentals, web performance, software design patterns and architecture
Experience with modern front-end development tools and frameworks
Excellent problem-solving and analytical skills
Excellent communication and collaboration skills
Experience working with nimble, cross-functional teams with high shipping velocity
Our Tech Stack
Frontend: React, TypeScript, GraphQL
Backend: Kotlin, GraphQL, Temporal
Database: Postgres
Infrastructure: AWS, Azure, Kubernetes, Terraform
Observability: Datadog, Cloudwatch, Lightstep
At Pluto, we live by the following core values:
Work like a lion, not like a cow 🦁
Directly Responsible Individuals (DRI): We empower team members with autonomy to own their projects and outcomes fully, fostering a culture of accountability, clarity and outsized impact. As the DRI you are the driver of success for your projects — proactively identifying, communicating and mitigating risks and resolving blockers to ensure success.
Clear Communication: At Pluto, we emphasize clear, transparent communication as fundamental to our success. We strive for clarity, succinctness and transparency in all interactions, ensuring that every message is articulated clearly and understood universally across our teams. This commitment to clarity reduces misunderstandings and fosters a more productive environment especially for distributed teams.
Systems Thinking & First Principles Reasoning: We love to deconstruct problems and innovate from the ground up. By understanding complex systems and questioning assumptions, we are able to develop creative solutions to difficult problems.
Fact-centric: We are intentional about questioning our assumptions, and bringing the facts to focus by diving into the data, quick-fire experimentation & swift prototyping. We make room for varied perspectives & calibrate our goals with metrics that track our progress, and accurately reflect reality.
Benefits
Competitive compensation & stock options so you have skin in the game
Remote-first work environment
We'll ship you a new MacBook
Monthly fitness benefit
Home-office budget
Discretionary learning budget"
Junior Software Engineer (Backend),Upskills,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/junior-software-engineer-backend-at-upskills-4304533833?position=58&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=DSq4%2FLzCrysbiH1pJ%2Fua5Q%3D%3D,"Upskills provides expert financial software consulting for investment banks and leading financial institutions in Asia Pacific, Middle East, and Europe region. With a strong from Front to Back expertise of the cash and derivatives markets, coupled by an in-depth knowledge of financial markets technologies, we provide smart, business-wise and efficient solutions.
We are seeking a highly motivated Backend Software Engineer with strong experience in Data Management, Data Flows, Reports and Interface Development.
The successful candidate will integrate a fast growing team and get direct exposure to the technical challenges of international financial institutions. You will take part of important software implementation and be responsible for, and not limited to:
Interacting with our international clients and our business analysts in Singapore, and Hong Kong
Designing reports and interfaces following client requirements
Developing high quality solutions and maintaining code delivered to our clients
Working in an agile environment and delivering timely and tailor made solutions
Taking initiatives to build internal tools
Promoting best DevOps practices for the team
Requirements
Master's or Bachelor's Degree in Computer Science, Information Technology, or relevant discipline
At least 2 years of experience with the Software Development Life Cycle: product specification, design, implementation, QA, release
Strong proficient in programming languages, such as Python, NodeJS, or Java
Knowledge of SQL, preferably PostgreSQL
Knowledge of JavaScript, Angular, or React is a strong plus
Knowledge of multiple frameworks, such as Flask, FastAPI, Django
Experience working in an Agile/Scrum development process
Strong passion for learning and adapting to new technologies
Strong analytical and problem-solving skills
Keen interest in financial markets. Working knowledge of financial products a plus
Fluent English and good communication skills"
Data Engineer,OPSWAT,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-opswat-4309619010?position=59&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=5yBm70w1Us3wjt1XvWn4PA%3D%3D,"OPSWAT, a global leader in IT, OT, and ICS critical infrastructure cybersecurity, delivers an end-to-end platform that gives public and private sector organizations and enterprises the critical advantage needed to protect their complex networks, secure their devices, and ensure compliance. Over the last 20 years our commitment to innovative technology has earned the trust of more than 1,700 organizations, governments, and institutions globally, solidifying our role in protecting the world’s critical infrastructure and securing our way of life.
The Position
The Data Engineering team is a vital component of OPSWAT's technology division, responsible for building and maintaining the foundational data infrastructure that supports the entire organization. The team's mission is to create a scalable, reliable, and secure data platform on Microsoft Azure, enabling data-driven decision-making across all business units. This involves designing and managing data warehouses and data lakes, developing robust data integration pipelines, and ensuring the quality and accessibility of data assets. A key strategic initiative for the team is the development and implementation of a Master Data Management (MDM) layer, which will play a critical role in enhancing data consistency and accuracy across various systems and applications.
What You Will Be Doing
Design and Maintenance of Azure Data Warehouse/Data Lake:
Ensuring the performance, reliability, and security of the data platform, requiring proactive monitoring and optimization.
Evaluate and implementing strategies to optimize the data warehouse and data lake for cost-effectiveness and efficiency, aligning with best practices for Azure data management
Development of ETL/ELT pipelines on Azure, utilizing tools such as Azure Data Factory and Azure Databricks, to ingest and transform data from a variety of sources.
Implementing data quality rules and processes to ensure the accuracy and reliability of the data within the platform
Optimize data storage & computing resources for cost efficiency
Proactively identify pipeline bottlenecks and performance issues
Development and Implementation of Master Data Management (MDM) Layer: A key responsibility is to research and recommend suitable MDM tools and technologies available within the Azure ecosystem. Our goal is to establish a ""single source of truth"" for critical data entities, ensuring consistency and quality across all connected systems.
Collaboration and Communication: The individual will work closely with product teams, sales and revenue operations, customer experience, and other stakeholders across the organization to understand their data requirements and deliver effective solutions that meet their needs. This requires the ability to explain complex technical concepts in a layman terms to non-technical audiences effectively.
Risk management: ensure data warehouse availability & resiliency (during service outage, network issues, unexpected high loads, external attacks, disaster recovery, etc.)
Define effective and measurable KPIs for the team and individual team members
Team resource management & Key Man Dependency mitigations
Strategic planning (at least 3 quarters in advance on team roadmap and vision)
Study new technologies & industry trends and propose appropriate solutions for company’s need
What We Need From You
Bachelor's degree in Computer Science, Data Engineering, or a related field.
Minimum of 3 years of proven experience in data warehouse/data lake design, implementation. This experience should include hands-on work with core Azure data services such as Azure Data Lake, Azure Synapse Analytics, and Azure SQL Database.
Extensive experience with ETL/ELT tools within the Azure ecosystem, including Azure Data Factory and Azure Databricks.
A strong understanding of database architecture principles, encompassing both relational databases (e.g., Azure SQL Database, Azure Synapse Analytics) and the concepts of NoSQL databases for handling unstructured data.
Project management skills - able to lead a project from beginning to end
Experience in building and managing a data engineering team
Experience in integrating data from multiple SaaS applications, including Salesforce, Hubspot, Netsuite, HRIS (e.g., Workday, ADP), and ZoomInfo.
Experience in researching and implementing Master Data Management (MDM) solutions.
Strong problem-solving, analytical, and troubleshooting skills are essential for this role.
It Would be Nice if You Had
Good written and verbal communication skills, including the ability to collaborate effectively with cross-functional teams, are required.
Experience with real-time data streaming technologies such as Azure Event Hubs or Kafka.
Experience with DevOps practices and tools for automating data pipelines, such as Azure DevOps and CI/CD pipelines.
Experience working in a Scrum Agile team.
OPSWAT is an equal opportunity employer. We celebrate diversity and are committed to providing an environment where equal employment opportunities are extended to all employees and applicants, free of discrimination and harassment of any type. All employment decisions are based on individual qualifications, job requirements, and business needs without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other category protected by federal, state, or local laws.
Recruiting Agencies: we do not accept unsolicited resumes from third party agencies for any of our open positions. To submit resumes for our jobs, there must be a recruiting contract approved by our legal team and endorsed by both parties. We are currently not accepting additional 3rd party agencies at this time."
Data Engineer (Middle),NCCPLUS VIETNAM,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/data-engineer-middle-at-nccplus-vietnam-4309766084?position=60&pageNum=0&refId=HiBikBBYXWw7RDczMXrY1Q%3D%3D&trackingId=qrjYuoJBhSBJ%2BSoWs0gGNw%3D%3D,"NCCPLUS Vietnam Joint Stock Company is a software firm and IT outsourcing service provider located in Vietnam. With 10 years of experience, we specialize in mobile and web application development, cloud and microservices development, game development, blockchain development, software testing, AI and IoT, and VoIP development. We work with clients in various fields such as public administration, education, finance & banking, insurance, healthcare, retail & ecommerce, sports, and entertainment. Our mission is to constantly enhance knowledge and capabilities to meet the divergent needs of the market and become a symbol of the IT outsourcing field.
1. JOB DESCRIPTION
Building and optimizing ‘big data’ data pipelines, architectures, and data sets.
Design, develop, and maintain data pipelines, warehouses, datalake.
Build the data products that technical users will depend on for business intelligence and ad-hoc access.
Work side by side with our Data Science to build and automate data pipeline, data ETL, etc. on distributed data processing platforms such as Spark.
Prepare data inputs for a generic blueprint ""model builder"".
Build production data pipeline for daily ETL and model retraining.
End-to-end data processing, troubleshooting, and problem diagnosis.
2. YOUR SKILLS AND EXPERIENCE
At least 3 years of working experience with Python, with a strong focus on ETL processes and Cloud Functions development.
Proficiency in Google Cloud Platform (GCP) stack, including Cloud Functions, Workflows, Pub/Sub, Dataform, and BigQuery.
Strong skills in data modeling and SQL optimization for efficient data processing and querying.
Experience with orchestration tools (e.g., Airflow, Dataform) and CI/CD pipelines for data workflows.
Solid understanding of event-driven architectures and their implementation in data pipelines.
Good at multi-threading, atomic operations, and computation frameworks such as Spark (Dataframe, DBT, SQL, etc.), distributed storage, and distributed computing.
Good at communication & teamwork.
Being open-minded, and willing to learn new things.
Requires proficiency in English, working directly with foreign partners.
3. WHY YOU'LL LOVE WORKING HERE
Hybrid Policy: Work from home up to 3 days per week.
Total Income = Net salary + Performance bonus (>14 months salary).
Review salary twice per year base on your performance and output.
Holiday bonus on 1/1, 30/4, 1/5, 2/9 and personal birthday.
Lunch allowance of 800,000 VND per month.
PVI Care comprehensive health insurance package for yourself and your family, annual health check.
NCC women receive full salary maternity allowance.
Participating in Knowledge Sharing sessions with OpenTalk, MiniTalk weekly.
Be equipped with modern PC + Laptop equipment.
Enjoy all of our activities such as Company Trip, Company's Birthday, YEP, Sport/Game/Code War Contest,...
We have these clubs for you to join: Football, Table football, Music, English, media and more.
The office is pleasant, and fully equipped with an attractive pantry with snacks, tea, coffee, and milo,...
Great promotion opportunities and career paths.
Working hours:
Morning: 8h30 – 12h00; Afternoon: 13h00 – 17h30. Monday to Friday (Having to attend 2 OpenTalk sessions per month).
HA NOI 1: 2nd Floor, CT3 The Pride, To Huu, Ha Dong, Ha Noi.
HA NOI 2: 7th Floor, Vinfast My Dinh Building, 8 Pham Hung, Cau Giay, Ha Noi.
HA NOI 3: 5th Floor, Hong Ha Tower, 89 Thinh Liet, Hoang Mai, Ha Noi.
https://ncc.asia/
https://www.facebook.com/nccsoft.com.vn and
https://www.facebook.com/nccplusvietnam"
Analytics Engineer (Junior to Mid Level),Holistics Data,Vietnam,https://vn.linkedin.com/jobs/view/analytics-engineer-junior-to-mid-level-at-holistics-data-4301987191?position=1&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=myGBPhL7G7cmHGdfkO7dCw%3D%3D,"Introduction
Holistics is a Business Intelligence platform built for the AI era. We are the team behind projects like the BI tool Holistics, the database diagram tool dbdiagram.io, and the open-source project dbml.org. We help data teams build a trusted, supervised self-service analytics experience for their business stakeholders.
We are searching for an
Analytics Engineer (junior to mid-level)
to help us evolve our data pipeline to support internal decisions, as well as helping our customers get more value of Holistics BI platform.
This is a full-time role based in
Ho Chi Minh City, Vietnam
, with a flexible, hybrid WFH/WFO setup.
What You Will Do
This role is a hybrid of technical analytics work and customer-facing engagements. Your time will be allocated across two core functions.
Internal data pipeline/modeling & BI:
You will actively contribute to our internal data & BI platform, ensuring our internal stakeholders get the data they need
Manage and enhance internal data pipeline: Contribute to the maintenance of our internal data ingestion, transformation, and reporting pipeline, ensuring data is timely and correct. This hands-on work will be a source of direct feedback for our product team.
Build best-practice data models: Develop and refactor internal data models using dbt, following data modeling paradigms like Kimball/Inmon and star/snowflake schemas. These models will serve as real-world examples for customer education.
Ensure data quality and integrity: Implement data tests and validation checks to ensure internal metrics are calculated correctly, translating these practices into customer-facing guidance.
Research and experiment with new tools: Test-drive emerging BI and analytics engineering tools, evaluate how they might enhance our demos or internal stack, and share your findings with the product and marketing teams.
Customer engagement:
Besides working on internal data pipeline, you’ll also get the chance to engage with our customers directly on their analytics projects
Guide customer onboarding: Work directly with new customers via shared Slack channels, review their data warehouse schemas, and coach them on data modeling best practices
Develop enablement assets: Convert modeling best practices and hard-won insights into step-by-step guides, sample Git repositories, and short video tutorials so customers and internal teams can self-serve.
Deliver scoped consulting: Spend focused time helping strategic accounts solve complex data modeling challenges, then document the patterns for future projects and content.
Our current data setup
We wrote an entire book explaining the complex analytics landscape, so it’s fair to say that we know a thing or two about setting up data stack. Our data setup is as follows:
Ingestion and Transformation: Event tracking (Snowplow) and ETLs from various data sources into our data lake (GCS) and data warehouse (BigQuery). The data is then transformed and moved to the data mart using dbt.
Serving: Reporting is served on Holistics, and reverse-ETLs are handled to push data back into our CRMs for operational purposes.
Orchestration: We use Prefect on top of the data stack to orchestrate and automate data pipelines and other operations.
On the business side, we practice what we preach. That means instead of building reports and dashboards, Data team works to maintain a common set of metric definitions and educate the business users on how to self-serve the platform effectively.
What We Look For In You
We are seeking a candidate with a unique blend of technical depth and communication skills.
A genuine interest in the data analytics / BI industry as a whole and a love for owning the entire data stack, not just one part of it.
Systems thinking: You can reason about how business operations, data collection, and system architecture interact and affect one another. When you solve a problem once, you instinctively template it for the next fifty times.
Excellent communication skills: You can communicate complex technical ideas clearly in both written and spoken English. You write like you code - clear, concise, and maintainable.
Problem-solving orientation: You have solid problem decomposition skills and are energized by live problem-solving in workshops and discovery calls.
Continuous learner: You possess a growth mindset and are excited to learn something new as part of your work every day.
Technical Skills
SQL: You have a good foundation in SQL. You can write readable queries to extract and manipulate data, and you can explain core concepts like grain and joins.
Data Modeling: You are familiar with fundamental data modeling concepts (e.g., dimensional modeling) and schema designs (star/snowflake) and understand why we transform raw data into user-friendly formats.
Programming: You have some experience with Python or an equivalent language for scripting and data manipulation tasks.
Version Control: You are familiar with the basics of Git for tracking changes and collaborating on projects.
Cloud Databases: You have exposure to, or academic experience with, a major cloud data warehouse (e.g., BigQuery, Snowflake, Redshift).
Nice-to-haves:
Exposure to dbt, Looker/LookML, Holistics, or other modern BI tools; a demonstrated interest in learning how data infrastructure empowers business decisions.
Interview Process
Round 1: A chat with the team and a light technical interview to discuss your skills, experience, and past projects.‍
Round 2: A short take-home assignment or on-site assignment to assess your technical and problem-solving skills.‍
Round 3: A culture and behavioral chat with management.
Why You'll Love Working
💕 Benefits
Flexible working arrangement, hybrid work (WFO+WFH) policies
14 annual leaves, 14 sick leaves, and childcare leave policy.
Annual budget for personal and professional growth, well-being, and interest cultivations.
24/7 healthcare insurance; periodic medical checkups.
13-month salary; annual compensation reviews
Weekly happy hours, company-organized events.
Workstations provided for maximum productivity
MacBook (or Laptop) Grant
Mechanical Keyboard + Mouse
Big-screen monitor(s)
Working time:
From Monday – Friday, from 9.00 a.m. to 6.00 p.m.
Work location:
457-459 Nguyen Dinh Chieu, Ward 5, District 3, Ho Chi Minh City, Vietnam."
Data Engineer,The Value Maximizer,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-the-value-maximizer-4309898349?position=2&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=bapSwIa%2Bib0gCVYf1LmtXQ%3D%3D,"MÔ TẢ CÔNG VIỆC
Phân tích yêu cầu nghiệp vụ cho hệ thống dữ liệu từ nguồn, phối hợp xây dựng các yêu cầu dữ liệu phục vụ báo cáo, phân tích từ các hệ thống nguồn khác nhau như T24, W4,..
Vận hành , phát triển & tối ưu hóa luồng dữ liệu - cả mảng dữ liệu & sandbox cho các team Analytics
Duy trì & phát triển hệ thống từ điển dữ liệu cho khối KHCN Yêu cầu tuyển dụng1. Trình độ Học vấn
Đại học hoặc cao hơn liên quan đến 1 trong các ngành kinh tế, tài chính, ngân hàng, hoặc công nghệ thông tin, toán tin ứng dụng, ngoại thương...2.Kiến thức/ Chuyên môn Có Liên Quan
Có kinh nghiệm ở vai trò Kỹ sư Dữ liệu hoặc vai trò tương đương
Có kinh nghiệm thiết kế ở mức schema các hệ quản trị cơ sở dữ liệu
Có kinh nghiệm cũng như kiến thức nghiệp vụ ngân hàng cũng như các hệ thống nghiệp vụ ngân hàng
Có kinh nghiệm làm việc với các hệ thống dữ liệu: Data Warehouse/ Data LakeHouse/ Data Product
Có kiến thức về các ngôn ngữ lập trình (SQL, Python, Spark)
Có kinh nghiệm triển khai thực tế với các hệ thống IBM DWH, AWS, Databricks là 1 điểm cộng
Có chứng chỉ về Data Engineer là 1 điểm cộng Quyền lợi và chế độ đãi ngộ
Mức lương: 20- 40 triệu + Thưởng dự án
Đóng đầy đủ bảo hiểm
Thưởng lễ, Tết
Xét tăng lương hàng năm (8-10 triệu/năm)
Nghỉ phép theo quy định công ty
Onsite dự án bank (dài hạn)
Thử việc 2 tháng (lương thử việc 85%)"
Data Engineer Executive,TH Food Chain,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-executive-at-th-food-chain-4309552289?position=3&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=U5CibAuWdw16G06LFVNPag%3D%3D,"Tìm việc làm tương tự:
Xem tất cả việc làm, THFC_Tất Cả Việc Làm, THFC_Công Nghệ Thông Tin, Công nghệ thông tin"
"Data Engineer (Python,SQL upto $2500)",FPT Software Innovation,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-python-sql-upto-%242500-at-fpt-software-innovation-4306343052?position=4&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=jthjfIP4LeQz0Mt9OBakvw%3D%3D,"Position: Data Engineer (Midde/Senior)
Location: District 9, Ho Chi Minh City
About the Role
We are looking for a Data Engineer with 3+ years of experience, strong expertise in
Python and SQL
, and hands-on experience with Azure Data Services. This is a great opportunity to grow technically while contributing to an enterprise-level Big Data transformation for global projects.
Key Responsibilities
Develop and maintain scalable, production-grade data pipelines.
Work extensively with
Python and SQL
to build, optimize, and maintain ETL/ELT processes.
Work with Azure components (Data Lake, Data Factory, Synapse, etc.).
Support data modeling and integration across various structured data sources.
Contribute to platform migration efforts toward Big Data architecture.
Collaborate with analysts, architects, and stakeholders to deliver reusable data assets.
Participate in Agile ceremonies (standups, sprint planning, retros, etc.).
Required Skills & Experience
Bachelor’s degree in IT, Computer Science, Data Science, or related fields.
3+ years of experience in data engineering.
Strong programming skills in Python and SQL
with solid knowledge of RDBMS (Oracle, SQL Server).
Hands-on experience with Azure Data Services: Data Lake, Data Factory, Synapse.
Good understanding of data modeling, integration patterns, and pipeline orchestration.
Good English communication and teamwork skills; ability to work cross-functionally.
Nice to have:
Experience with Databricks (including Unity Catalog).
Benefits
“FPT Care” health insurance provided by Petrolimex (PJICO), exclusive for FPT employees.
Annual summer vacation starting from May, following company policy.
Annual salary review.
International, dynamic, and friendly working environment.
Annual leave and working conditions compliant with Vietnam labor laws.
Sponsorship for international certification exams and study programs.
Loan interest support policy for Fsofter employees.
Shuttle bus service for employees.
Contact
Ms. Thu – Talent Acquisition Specialist
Email: Thuthm1@fpt.com
| Tel/Zalo: 0522930490"
Middle Data Engineer,Global Fashion Group,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/middle-data-engineer-at-global-fashion-group-4306185198?position=5&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=TG%2FpAVXfTCDxOMx40jmorw%3D%3D,"About Global Fashion Group
Global Fashion Group is the leading fashion and lifestyle destination in growth markets across LATAM, SEA and ANZ. From our people to our customers and partners, we exist to empower everyone to express their true selves through fashion. Our three e-commerce platforms: Dafiti, ZALORA and THE ICONIC connect an assortment of international, local and own brands to over 800 million consumers from diverse cultures and lifestyles. GFG’s platforms provide seamless and inspiring customer experiences from discovery to delivery, powered by art & science that is infused with unparalleled local knowledge. As part of the Group’s vision is to be the #1 online destination for fashion & lifestyle in growth markets, we are committed to doing this responsibly by being people and planet positive across everything we do.
Since launching in 2011, THE ICONIC has redefined the future of retail in Australia and New Zealand. As the leading fashion, sports and lifestyle e-commerce destination in the region, our e-commerce platforms (Retail, Marketplace and Services) provide a seamless and inspiring end-to-end customer experience through our own technology innovations. We stand for benchmark-setting customer service, delivery options, returns policies, and curation of brands.
We are a diverse and dynamic community of over 1,000 people working towards our purpose “To bring on the future of shopping”. THE ICONIC is people and planet positive, and we strive towards creating a positive impact in the world by driving genuine and meaningful change for the better of all communities involved.
For more information visit: www.global-fashion-group.com
At GFG, Technology is driven by innovation and quality is highly valued. Our data team is the driving force behind our business strategies and decisions. We are integrated into all departments to ensure all employees at THE ICONIC have access to good quality and timely data.
Our Data Engineering team solves complex problems and delivers data to propel our business forward, powering the insights that are used in every decision we make. We are the engineers, the builders, the maintenance people for all our business data.
SCOPE OF ROLE
Develop and support our enterprise data warehouses, analytical databases and infrastructure
Work with the team to re-platform our existing data architecture to next generation tooling and data architecture
Build/Maintain our data pipelines in Python and SQL to ensure that data is delivered in a timely manner
Work closely with our Data Scientists and Data Analysts to implement new insights and statistical models
Assist in developing tools/processes to enable our business to self serve
What It Takes
Both AWS and GCP
BigQuery, SQL Server and Redshift
Docker & Kubernetes
Airflow, Cloud Composer and Pentaho
Cloud Dataflow/Apache Beam
High velocity streaming data, behavioural data and well as structured and unstructured data
Extensive programming knowledge of Data processing within Python
Dependency management within Python
Iterators, producers and consumer knowledge
Airflow DAG building
CI/CD - Bamboo Deployment / Delivery Experience
Strong SQL coding skills
Advanced Data Engineering Design Skills
Excellent communication skills
Significant experience in a similar Data Engineering role
Strong data warehousing/Data Engineering experience
Experience with data modelling and complex ETL solutions
Test and QA Experience in Data Pipelines
Ways to stand out from the crowd
GCP experience
BigQuery/Redshift
Cube/SSAS Experience
DevOps/CICD
What We Offer You
The ability to cross train into DevOps/Platforms or Data Engineering
The unique opportunity to have a serious impact on a growing organisation
A dynamic working environment shaping the face of fashion e-commerce in growth markets
Work closely with a global talent pool with international mindset
#1 IT company to work for in Vietnam in 2021
Best practice scaled agile engineering
Amazing office and great culture! Massage chairs, table tennis, video game room, quarterly team events, yearly company trip, end of the year party
Hybrid working environment and work from home set up allowance
Clear career progression plan and support
English classes
Macbook or laptop when you start
Social insurance, medical insurance & AON insurance
13th month salary
15 days of annual leave, 30 days of sick leave/mental health, 1 day of occasion leave
Support for Gym membership
VND 60,000,000 referral reward for successfully referring someone to GFG
Global Fashion Group embraces diversity and equality of opportunity. We are committed to building an inclusive and diverse team representing all backgrounds, with as wide a range of perspectives as possible, and harnessing industry-leading skills. We believe that the more inclusive we are, the better our work will be. We welcome and consider applications to join our team from all qualified candidates, regardless of their characteristics. We comply with all applicable laws and regulations on non-discrimination in employment (and recruitment), as well as work authorization and employment eligibility verification requirements."
Data Engineer,Hitachi Consulting ( formerly Information Management Group),"Đà Nang, Da Nang City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-hitachi-consulting-formerly-information-management-group-4305957918?position=6&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=G%2BrJUYHMrtogzIPimzMc3g%3D%3D,"Our Company
Imagine the sheer breadth of talent it takes to bring a better tomorrow closer to today. We don’t expect you to ‘fit’ every requirement – your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.
Design, implement, maintain data models
Support data model and ETL solutions in production.
Build and maintain data pipelines using tools such as NIFI, Airflow, etc.
Evaluate and define KPIs to monitor and manage data quality
Tuning and improving the performance of DB/DWH.
Support data scientists, data analyst to explore data in Data warehouse.
Build and maintain integration data flow provided to other departments.
Maintain update-to-date documentation of data catalog, ETL flows, data dictionary.
Practice sustainable incident response and blameless postmortems.
Participate in end to end engineering solutions with regard to data processing and data delivery and integration, including data processing job/pipeline tool suite, batch framework and platform, micro-service framework and platform.
BASIC QUALIFICATIONS
Skill in one of the following languages Python / Java / Kotlin, and target in Big Data career.
Skill in one of the following technologies: PySpark / Spark / Flink / Airflow
PREFERRED QUALIFICATIONS
Experience in Design, ETL, data modeling and developing SQL database solutions.
Good understanding of data management - data lineage, meta data, data quality, data governance
Have basic understanding about big data, and big data platform.
Knowledge with S3/ Spark/ Jupyter/ Flink is a plus point.
Ability to debug and optimize code and automate routine tasks.
Skills in task and time management, proactive problem solver.
Teamwork and communication skills.
Championing diversity, equity, and inclusion
How We Look After You
We want to help you take care of your today and tomorrow – at home and at work. Which is why we offer industry-leading benefits that go far beyond compensation. That means support, services, and resources that also take care of your holistic health and wellbeing. We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. Here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent).
We’re a global team of innovators, together, we harness engineering excellence and passion for creating meaningful solutions to complex challenges. We turn organizations into industry leaders that can a make positive impact on their industries and society.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.
Fostering innovation through diverse perspectives
Hitachi is a global company operating across a wide range of industries and regions. One of the things that sets Hitachi apart is the diversity of our business and people, which drives our innovation and growth.
We are committed to building an inclusive culture based on mutual respect and merit-based systems. We believe that when people feel valued, heard, and safe to express themselves, they do their best work.
How We Look After You
We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status
or any other protected characteristic.
Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success."
Data Engineer-Upto 50M-Signing bonus 20M,SETA International,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/data-engineer-upto-50m-signing-bonus-20m-at-seta-international-4308204019?position=7&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=fiChPD4%2Fw6R6R4BCamhkYQ%3D%3D,"SETA INTERNATIONAL VIETNAM- RECRUITMENT
Job Title: Data Engineer
Responsibilities:
Automate and optimize data pipelines to ensure efficient and scalable data flow.
Perform ETL tasks for the data warehouse and optimize queries for data processing.
Conceptualize and build infrastructure to enable big data access and analysis.
Prepare and analyze raw data for manipulation by data scientists, and interpret trends and patterns.
Develop data tools to support analytics and data science teams.
Work with cloud infrastructure (preferably GCP) for deployment and scaling.
Explore and evaluate new data products and technologies.
Build and maintain analytic dashboards and reports.
Requirements:
Experience:
At least 3 years of experience as a Data Engineer.
Good communication skills in English.
Strong experience with cloud platforms (
GCP preferred
), including core services for data storage, processing, and analytics.
Big Data & Streaming:
Hands-on experience designing and implementing scalable data pipelines for both batch and real-time data. Experience with
Programming:
– must have
– nice to have
– understanding or willingness to learn for data ingestion from various sources
Strong skills in writing complex queries for data extraction, transformation, and analysis.
Workflow Orchestration:
Experience with
Apache Airflow
for building, scheduling, and maintaining data workflows.
Version Control:
Proficient with
and collaborative development workflows.
System Skills:
Comfortable working with the
Linux command line
for operations, scripting, and automation.
Familiarity with using AI tools to accelerate coding and improve productivity.
Soft Skills:
Strong leadership and self-motivation, with the ability to inspire and drive the team forward.
Strong analytical and requirement-gathering skills, able to translate business needs into technical solutions.
Critical thinking and problem-solving abilities.
Open-minded and growth-oriented: eager to learn, adapt, and accept feedback.
Detail-oriented, proactive, and able to perform well under pressure.
Benefits
Salary: Negotiate
An international, professional, young, but innovative and dynamic environment working closely with international experts and joining conferences and workshops on exciting new technologies.
Full benefits for employees according to the Vietnam Labor Laws: social and health insurance
Holidays based on the Vietnamese labor law + paid vacations
Opportunity to be onsite in the US
Contact
SETA International Viet Nam
: HL Tower, 82 Duy Tan, Ha Noi
hr@setacinq.vn
https://www.seta-international.com/
https://www.facebook.com/SETA.International.careers/"
Senior Data Engineer,Grab,Vietnam,https://vn.linkedin.com/jobs/view/senior-data-engineer-at-grab-4298873928?position=8&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=LV%2BwCWYm%2BFbXTRZb%2FjxkHA%3D%3D,"Company Description
About Grab and Our Workplace
Grab is Southeast Asia's leading superapp. From getting your favourite meals delivered to helping you manage your finances and getting around town hassle-free, we've got your back with everything. In Grab, purpose gives us joy and habits build excellence, while harnessing the power of Technology and AI to deliver the mission of driving Southeast Asia forward by economically empowering everyone, with heart, hunger, honour, and humility.
Job Description
Get to Know the Team
We maintain Grab's data infrastructure, a dependable and economical platform that supports internal data processes and company-wide data lake access. This includes managing compute systems such as Apache Spark, Trino, and Starrocks, scheduling via Airflow, and an AWS S3-based storage layer. Our storage solutions utilize modern open-source formats like Apache Iceberg and Delta, in addition to traditional Apache Hive Parquet tables.
We simplify data operations for internal users by providing managed Spark, Starrocks, Trino, and Airflow services. Furthermore, our team collaborates with Grab's Data Catalog team to offer self-service datalake capabilities powered by Datahub. We also partner closely with Grab's AI platforms, ensuring a smooth experience for users in their AI development.
Get to Know the Role
You will support the mission by maintaining and extending the platform capabilities through implementation of new features and continuous improvements. You will also explore new developments in the space and bring them to our platform there by helping the data community at Grab. You will work onsite in Grab Vietnam office, CMC Creative Space, D7, HCMC and report to Data Engineering Manager, who is based in Singapore.
The Critical Tasks You Will Perform
You will maintain and extend the Python/Go/Scala backend for Grab's Airflow, Spark, Trino and Starrocks platform
You will build, modify and extend Python/Scala Spark applications and Airflow pipelines for better performance, reliability, and cost.
You will design and implement architectural improvements for new use cases or efficiency.
You will build platforms that can scale to the 3 Vs of Big Data (Volume, Velocity, Variety)
You will follow various testing best practices and SRE best practices to ensure system stability and reliability.
Qualifications
What Essential Skills You Will Need
Software Engineering, Computer Science, or related undergraduate degree. Proficient in at least one of the following: Python, Go, or Scala and strong appetite to learn other programming languages.
You have 5 or more years of relevant professional experience
Good working knowledge in 3 or more of the following: Airflow, Spark, relational databases (ideally MySQL), Kubernetes, Starrocks, Trino, and backend API implementation and being passionate about learning the others.
Experience with AWS services (S3, EKS, IAM) and infrastructure as code tools like Terraform.
Proficiency in CI/CD tools (Jenkins, GitLab, etc.)
You are to intelligently utilize available AI resources at Grab.
Skills That Are Good To Have
Proficient in Kubernetes with hands of experience with building custom resources using frameworks like kubebuilder.
Proficient in Apache Spark, with good knowledge of resource managers like Yarn, Kubernetes and how spark interacts and work with them
Advanced understanding of Apache Airflow and its working with Celery and/or Kubernetes executor backend with exposure to Python SQLAlchemy framework.
Advanced knowledge of other query engines like Trino, Starrocks and others
Advanced knowledge of AWS Cloud
Good understanding of lakehouse table formats like Iceberg and Delta lake, how query engines work with it.
Additional Information
Life at Grab
We care about your well-being at Grab, here are some of the global benefits we offer:
We have your back with Term Life Insurance and comprehensive Medical Insurance.
With GrabFlex, create a benefits package that suits your needs and aspirations.
Celebrate moments that matter in life with loved ones through Parental and Birthday leave, and give back to your communities through Love-all-Serve-all (LASA) volunteering leave
We have a confidential Grabber Assistance Programme to guide and uplift you and your loved ones through life's challenges.
Balancing personal commitments and life's demands are made easier with our FlexWork arrangements such as differentiated hours
What We Stand For At Grab
We are committed to building an inclusive and equitable workplace that enables diverse Grabbers to grow and perform at their best. As an equal opportunity employer, we consider all candidates fairly and equally regardless of nationality, ethnicity, religion, age, gender identity, sexual orientation, family commitments, physical and mental impairments or disabilities, and other attributes that make them unique."
"Data Engineer (good EN, upto $2500)",FPT Software Innovation,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-good-en-upto-%242500-at-fpt-software-innovation-4311255057?position=9&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=aUucemcK9fxSTDSyJxr3CQ%3D%3D,"Position: Data Engineer (nice to have Databricks)
Location: HCMC
Key Responsibilities
• Develop and maintain scalable, production-grade data pipelines
• Support data modeling and integration across various structured data sources
• Contribute to platform migration efforts toward Databricks and Big Data architecture
• Collaborate with analysts, architects, and stakeholders to deliver reusable data assets
• Join Agile ceremonies (standups, sprint planning, retros, etc.)
Required Skills & Experience
Bachelor's degree in IT, Computer Science, Data Science, or related fields
At least 3 years of experience in data engineering with strong SQL and RDBMS knowledge (Oracle, SQL Server)
Advanced SQL skills for complex queries, performance tuning, and data manipulation
Solid programming skills in Python for data processing, automation, and ETL
Experience working with Spark (PySpark/Scala Spark), including building scalable distributed data processing pipelines
Nice to have experience with Databricks (including Unity Catalog is a plus)
Familiarity with Azure, AWS, or GCP cloud platforms
Understanding of data modeling, integration patterns, and pipeline orchestration
Good English communication and teamwork skills; ability to work cross-functionally
Benefits
• “FPT care” health insurance is provided by Petrolimex (PJICO) and is exclusive for FPT employees
• Annual Summer Vacation: follows the company’s policy and starts from May every year
• Salary review 1 time per year
• International, dynamic, and friendly working environment
• Annual leave and working conditions follow Vietnam labor laws
• Other benefits: sponsor for studying and taking international certification exams, sponsor loan interest policy for Fsofter
• Shuttle bus for employees
Contact: Ms. Thu – Talent Acquisition Specialist
Email: Thuthm1@fpt.com | Tel/Zalo: 052293049"
Data Engineer,BJAK,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-bjak-4313234269?position=10&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=%2FPjiMu4BJSXoGAGFEhEYwQ%3D%3D,"Transform Language Models into Real-World Applications
We’re building AI systems for a global audience. We are living in an era of AI transition - this new project team will be focusing on building applications to enable more real world impact and highest usage for the world.
This role is a global role with
remote work arrangement
. You’ll work closely with regional teams across product, engineering, operations, infrastructure and data to build and scale impactful AI solutions.
Why This Role Matters
You’ll fine-tune state-of-the-art models, design evaluation frameworks, and bring AI features into production. Your work ensures our models are not only intelligent, but also safe, trustworthy, and impactful at scale.
What You’ll Do
Collect, clean, and preprocess user-generated text and image data for fine-tuning large models
Design and manage scalable data labeling pipelines, leveraging both crowdsourcing and in-house labeling teams
Build and maintain automated datasets for content moderation (e.g., safe vs unsafe content)
Collaborate with researchers and engineers to ensure datasets are high-quality, diverse, and aligned with model training needs
What Is It Like
Likes ownership and independence
Believe clarity comes from action - prototype, test, and iterate without waiting for perfect plans.
Stay calm and effective in startup chaos - shifting priorities and building from zero doesn’t faze you.
Bias for speed - you believe it’s better to deliver something valuable now than a perfect version much later.
See feedback and failure as part of growth - you’re here to level up.
Possess humility, hunger, and hustle, and lift others up as you go.
Requirements
Proven experience preparing datasets for machine learning or fine-tuning large models
Strong skills in data cleaning, preprocessing, and transformation for both text and image data
Hands-on experience with data labeling workflows and quality assurance for labeled data
Familiarity with building and maintaining moderation datasets (safety, compliance, and filtering)
Proficiency in scripting (Python, SQL) and working with large-scale data pipelines
What You’ll Get
Flat structure & real ownership
Full involvement in direction and consensus decision making
Flexibility in work arrangement
High-impact role with visibility across product, data, and engineering
Top-of-market compensation and performance-based bonuses
Global exposure to product development
Lots of perks - housing rental subsidies, a quality company cafeteria, and overtime meals
Health, dental & vision insurance
Global travel insurance (for you & your dependents)
Unlimited, flexible time off
Our Team & Culture
We’re a densed, high-performance team focused on high quality work and global impact. We behave like owners. We value speed, clarity, and relentless ownership. If you’re hungry to grow and care deeply about excellence, join us.
BJAK is Southeast Asia’s #1 insurance aggregator with 8M+ users, fully owned by its employees. Headquartered in Malaysia and operating in Thailand, Taiwan, and Japan, we help millions of users access transparent and affordable financial protection through Bjak.com. We simplify complex financial products through cutting-edge technologies, including APIs, automation, and AI, to build the next generation of intelligent financial systems.
If you're excited to build real-world AI systems and grow fast in a high-impact environment, we’d love to hear from you."
Data Engineer,ĐÚNG NGƯỜI ĐÚNG VIỆC Community,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-%C4%91%C3%BAng-ng%C6%B0%E1%BB%9Di-%C4%91%C3%BAng-vi%E1%BB%87c-community-4313656496?position=11&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=o6iNSCVl5fMx2ae4o4vLuQ%3D%3D,"Mô tả công việc
Trách Nhiệm Thiết Yếu
Xây dựng và duy trì pipeline dữ liệu phục vụ cho mô hình tín dụng
Đảm bảo dữ liệu được thu thập, đồng bộ và quản lý trong Data Lakehouse ổn định, bảo mật và tuân thủ quy định pháp lý
Công Việc Thiết Yếu
Thiết kế pipeline ETL/ELT để tích hợp dữ liệu tài chính và phi tài chính từ nhiều nguồn
Đồng bộ dữ liệu vào Data Lakehouse phục vụ các mô hình tín dụng
Tối ưu hiệu suất pipeline, độ tươi dữ liệu và chi phí hạ tầng
Đảm bảo dữ liệu chính xác, đầy đủ và tuân thủ SLA
Yêu cầu công việc
Trình Độ Học Vấn
Cử nhân về Công nghệ Thông tin, Khoa học dữ liệu hoặc Kỹ thuật phần mềm
Kinh Nghiệm Làm Việc
3–5 năm kinh nghiệm trong Data Engineering, quản lý pipeline dữ liệu
Kinh nghiệm với hệ thống Data Lake/DWH/Lakehouse
Kiến Thức Và Kỹ Năng
Thành thạo SQL, Python, Spark và Kafka
Kinh nghiệm với cloud platforms (AWS, GCP, Azure)
Am hiểu Data Quality và Data Governance
Kinh nghiệm tích hợp dữ liệu cho mô hình tín dụng
Hiểu biết về DataOps và data pipelines
Tại sao bạn yêu thích làm việc tại đây
Tiên phong công nghệ, uy tín
MISA là doanh nghiệp CNTT xuất sắc nhất khu vực Châu Á - Châu Đại Dương. Tiên phong xuất khẩu giải pháp SaaS
TOP đầu doanh nghiệp CNTT tăng trưởng liên tục với quy mô nhân sự tăng 20%/năm, doanh thu tăng 15%/năm
Hội tụ 3000 nhân tài cùng khát vọng đưa sản phẩm công nghệ “Make In Vietnam” vươn tầm quốc tế
Xây dựng niềm tin với 270.000 khách hàng là đơn vị HCSN, doanh nghiệp, 2.5 triệu khách hàng cá nhân tại Việt Nam và 20 quốc gia
Hơn 100 giải thưởng trong ngành CNTT trong nước và quốc tế
Nền tảng vững chắc cho phát triển sự nghiệp, thăng tiến, quyền lợi
Lương cứng cạnh tranh. Thưởng năng suất dựa trên kết quả công việc từ 2 tháng lương.
Đánh giá review lương 2 lần/năm, thưởng sáng kiến...
Huấn luyện “Hổ tướng”: chương trình đào tạo quản lý tài năng, bệ phóng trở thành Chiến tướng tinh nhuệ
Giải thưởng “Gấu vàng"": nơi tôn vinh những tài năng xuất sắc nhất
Gói chăm sóc sức khỏe toàn diện tại Medlatec, cháy hết mình tại các CLB theo sở thích, chương trình teambuilding, du lịch định kỳ
Môi trường thân thiện, chia sẻ, đồng hành
Kết nối tài năng: tập trung phát triển những con người có chung lý tưởng, mục tiêu, cùng trao giá trị và nhận thành công
Tư duy đột phá: môi trường tôn trọng sự khác biệt và đề cao sáng tạo, MISA-er được tự do phát triển các ý tưởng tiến bộ, cải tiến công việc
Công nghệ cao: trang bị máy tính làm việc, tối ưu hiệu suất công việc bằng ứng dụng công nghệ, phần mềm tự động (AMIS, Jira, Power BI, AI Marketing,...)
Nơi làm việc hạnh phúc: MISA mong muốn tạo một môi trường làm việc để bạn luôn cảm thấy hạnh phúc
Thời gian làm việc
Thứ 2 - Thứ 6 (từ 08:00 đến 17:30)"
Data & CMMS Engineer,Nghi Son Refinery and Petrochemical Limited Liability Company,"Thanh Hoa, Thanh Hoa, Vietnam",https://vn.linkedin.com/jobs/view/data-cmms-engineer-at-nghi-son-refinery-and-petrochemical-limited-liability-company-4260002707?position=12&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=KePfiitOgySnyRVhWHylUg%3D%3D,"Category Data & CMMS Engineer Location Thanh Hoa Number of vacancies 1 Responsibilities
The Data and CMMS Engineer is responsible for maintaining and updating a comprehensive asset register, ensuring data accuracy in the CMMS (Maximo), and supporting maintenance strategies through the analysis and reporting of reliability and maintenance data. He also coordinates closely with planners, supervisors, and reliability teams to ensure maintenance data (PM, worklog, failure code/ mode, job card, downtime, etc,…) are input/ updated on the system timely and accurately.
Detailed responsibilities of the Data & CMMS Engineer are as follows:
Asset register:
Establish a comprehensive register (Maintenance strategies for new assets);
Perform Storage of good quality technical information for asset;
Conduct Maximo Asset data (ECA, description, specification) changes;
Implement Maximo – Maintenance process changes.
Reliability Data:
Statistic, analyze, and report Breakdown maintenance cost on CMMS system;
Statistic, analyze, and report Breakdown maintenance failure code on CMMS system;
Statistic, analyze, and report PM compliance data on CMMS system;
Statistic, analyze, and report PM cost on CMMS system;
Statistic, analyze, and report Equipment downtime;
Statistic, analyze, and report System downtime;
Conduct Pareto statistics;
Report Work priority status.
Maintenance Work execution:
Coordinate with supervisor to ensure work log and actuals work are correct. Data should input in WO within 48hour for BM;
Coordinate with planner to ensure failure code input is correct;
Coordinate with planner/ scheduler to update PM status. PM can conduct as schedule & plan;
Coordinate with supervisor to ensure work log and actuals work are correct. Data should input in WO within 48hours;
Coordinate with reliability engineer, operation & refinery planning to update equipment & process system downtime, including.
Downtime to repair as plan; Downtime due to breakdown maintenance (time to repair, time for logistic of material & service, etc.); Downtime due to system, unit trip.
Requirements
Qualification: Graduated University or above on the areas of Computer Science, Information Technology, Mechanical Engineering, or relevant.
Work Experience: Above 05 years of experience in Computer Science, Information Technology, Mechanical Engineering with high CMMS proficiency or relevant.
Skills:
Well proficient on CMMS software;
Capable to configure and optimize CMMS for data tracking and reporting;
Experienced in SQL and database management;
Capable to manage and manipulate large datasets;
Knowledgeable with reliability functions including RCM, FMECA, RCA, ECA and Asset register hierarchy;
Strong skills in data collection, validation, and analysis;
Proficiency in statistical analysis software;
Fluently read, write, and speak English.
Benefits
High limit of medical and personal accident insurance for the employee and children;
Personal Income Tax 50% deduction;
Accommodation & Transportation to work will be provided by the Company;
Free meal at site canteen (lunch)."
[Junior/Middle]_Data Engineer (GCP),MTI Technology,Ho Chi Minh City Metropolitan Area,https://vn.linkedin.com/jobs/view/junior-middle-data-engineer-gcp-at-mti-technology-4308678333?position=13&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=lWlyJrKk%2F%2Fg6ihfL1XTH0w%3D%3D,"About the job:
As a Junior Data Engineer, you will be part of a team responsible for building and maintaining our data infrastructure on the Google Cloud Platform (GCP). You will focus on assisting with the development, testing, and monitoring of data pipelines, ensuring data quality, and supporting data-driven initiatives under the guidance of senior engineers. This is an excellent opportunity to learn and grow your skills in cloud-based data engineering within a dynamic global company.
Key Responsibilities:
Assist in developing, testing, and deploying ETL/ELT processes to ingest and transform data from various sources into our GCP data warehouse/lake.
Write and optimize SQL queries for data extraction, transformation, and loading.
Monitor existing data pipelines and troubleshoot basic issues under supervision.
Support senior engineers in maintaining data quality and integrity.
Help document data pipelines, processes, and data models.
Learn and utilize various GCP data services (e.g., BigQuery, Cloud Storage, Dataflow basics).
Collaborate with team members and other IT professionals.
Required Skills and Qualifications:
Bachelor's degree in Computer Science, Engineering, Information Technology, or a related field, or equivalent practical experience.
Basic understanding of data warehousing, data lakes, and ETL/ELT concepts.
Foundational knowledge of SQL.
Familiarity with at least one programming language (e.g., Python, Java).
Eagerness to learn GCP data services and data engineering best practices.
Good problem-solving skills and attention to detail.
Ability to work effectively in a team environment.
Experience in LookML/Looker Studio is a plus
Nice to have:
Some exposure to cloud platforms, preferably GCP.
Experience with version control systems (e.g., Git).
Basic understanding of data modeling concepts.
Familiarity with the logistics or maritime industry.
Good verbal and written communication skills in English.
What We Offer
100% salary during probation
3 days of WFH per week (based on team's decision)
Lunch + Gasoline + Coffee Allowance
Health, Social and Unemployment Insurance (based on gross-based salary, according to Labor Code) and PVI Health Insurance
13th-month salary and Performance bonus
Annual salary review
12 days annual leave plus extra 02 days company leave
Company trips, sponsored team building, monthly Happy Hour, Sport Clubs (Soccer, Badminton, Pingpong, Yoga) and other joyful events;
A culture of relentless learning with free courses in specialized skills, soft skills, and English;
Yearly health-checkup;
Seniority benefits: allowance & PVI Health Insurances for family members
Technical-certificate bonus
Japanese-certificate bonus
Employee Referral Incentive"
Data Engineer-Upto 50M,SETA International Careers,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/data-engineer-upto-50m-at-seta-international-careers-4313574820?position=14&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=ytGJTy2fAHWw3IgN9cWLBw%3D%3D,"SETA INTERNATIONAL VIETNAM- RECRUITMENT
Job Title: Data Engineer
Responsibilities:
Automate and optimize data pipelines to ensure efficient and scalable data flow.
Perform ETL tasks for the data warehouse and optimize queries for data processing.
Conceptualize and build infrastructure to enable big data access and analysis.
Prepare and analyze raw data for manipulation by data scientists, and interpret trends and patterns.
Develop data tools to support analytics and data science teams.
Work with cloud infrastructure (preferably GCP) for deployment and scaling.
Explore and evaluate new data products and technologies.
Build and maintain analytic dashboards and reports.
Requirements:
Experience:
At least 3 years of experience as a Data Engineer.
Good communication skills in English.
Strong experience with cloud platforms (
GCP preferred
), including core services for data storage, processing, and analytics.
Big Data & Streaming:
Hands-on experience designing and implementing scalable data pipelines for both batch and real-time data. Experience with
Programming:
– must have
– nice to have
– understanding or willingness to learn for data ingestion from various sources
Strong skills in writing complex queries for data extraction, transformation, and analysis.
Workflow Orchestration:
Experience with
Apache Airflow
for building, scheduling, and maintaining data workflows.
Version Control:
Proficient with
and collaborative development workflows.
System Skills:
Comfortable working with the
Linux command line
for operations, scripting, and automation.
Familiarity with using AI tools to accelerate coding and improve productivity.
Soft Skills:
Strong leadership and self-motivation, with the ability to inspire and drive the team forward.
Strong analytical and requirement-gathering skills, able to translate business needs into technical solutions.
Critical thinking and problem-solving abilities.
Open-minded and growth-oriented: eager to learn, adapt, and accept feedback.
Detail-oriented, proactive, and able to perform well under pressure.
Benefits
Salary: Negotiate
An international, professional, young, but innovative and dynamic environment working closely with international experts and joining conferences and workshops on exciting new technologies.
Full benefits for employees according to the Vietnam Labor Laws: social and health insurance
Holidays based on the Vietnamese labor law + paid vacations
Opportunity to be onsite in the US
Contact
SETA International Viet Nam
: HL Tower, 82 Duy Tan, Ha Noi
hr@setacinq.vn
https://www.seta-international.com/
https://www.facebook.com/SETA.International.careers/"
Data engineer,Techvify,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-techvify-4295936255?position=15&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=6u4wsq53Hm88sRqneC7%2BrQ%3D%3D,"Địa điểm làm việc
TECHVIFY Corporation is an End to End AI-Powered Digital Transformation Partner.
At TECHVIFY, we don’t just build software.
We engineer breakthroughs.
We innovate with AI, craft with code, and scale with cloud.
We partner with startups, enterprises, and even competitors on a shared mission:
To turn bold ideas into real-world impact.
If you’re looking to lead digital transformation through intelligent software, we’re ready to build with you.
Let’s create the future – together.
Responsibilities
Design, develop, and maintain data pipelines and ETL processes to support data analytics and reporting.
Collaborate with stakeholders to understand data requirements and deliver effective solutions.
Ensure data quality and integrity through rigorous testing and validation processes.
Optimize and troubleshoot existing data workflows for performance improvements.
Participate in project planning and contribute to the development of data architecture strategies.
Stay updated with industry trends and best practices in data engineering and analytics.
Requirements
Bachelor’s degree in Computer Science, Information Technology, or a related field.
At least 2+ years in this position
Proficient in SQL and Python, with experience in Pyspark.
Understanding of data modeling, ETL processes, and data warehousing concepts.
Experience with data analysis and visualization tools.
Ability to communicate effectively in English, both written and verbal.
Familiarity with big data technologies is a plus.
Benefits
Salary: Upto 30M
Join a global team and work directly with many talents around the world.
Opportunities for onsite work in Japan, Singapore, Australia, and many other countries.
Work and grow in a dynamic, creative, and professional environment.
Healthcare: Premium Health Insurance TECHVIFY Care
13 months’ salary per year.
Annual salary evaluation.
Sponsor and encourage staff to study courses by covering tuition fees, such as Udemy, Coursera.
If you are a driven and talented Data Engineer looking for an opportunity to be part of something extraordinary, please submit your resume and cover letter for consideration.
Join our team and let’s create amazing things together! Send your updated CV to this email address:
Email: careers@techvify.com.vn
About us: Techvify | End-to-End AI-Powered Digital Transformation Partner
Vui lòng nhập họ tên của bạn
Vui lòng nhập email của bạn
Vui lòng nhập số điện thoại của bạn
Vị trí bạn quan tâm Java Engineer Data Scientist Senior Recruiter Intern AI Solution Engineer (Japanese) Senior Data engineer .Net Engineer Mid/Senior Python Engineer Data engineer AI Engineer Senior Backend Engineer (Java/Golang) Technical Leader - Python/AI Technical Lead QA Engineer Senior Recruiter (Japan Market) Automation QA Manager Software Engineer for JP market BrSE for JP Market DevOps Engineer Account Manager Senior BE in Python Django Quality Assurance Leader SRE Lead Application Managed Service L2 Senior Java Developer Senior PHP Developer Senior International Sales Executive Business Analyst Korea Sales Manager Senior PQA Lead Intern AI Solution Engineer Sales Executive (Japan Market) Devops Engineer Python Developer Mobile Developer Technical Project Manager Frontend Developer Senior Solutions Architect / Consultant Business Analyst/ Product Owner Intern Project Coordinator NodeJS Developer AI Engineer Golang Developer Senior Frontend Developer Product Quality Assurance Senior International Sales Frontend Developer Intern Sales IT Quality Assurance Engineer Senior Nodejs Developer Middle Fullstack Developer Senior C# Developer Fresher Japanese Content Creator (EB) IT Helpdesk Senior Ruby on Rails Developer Business Analyst Manual Tester Middle/Senior Auto QA Engineer Senior Frontend Developer (ReactJS) Project Manager/ Scrum Master Sales Manager Sales Manager - HN Content Marketing Specialist (Creative) Intern Sales - HCM Senior Software Engineer Project Manager Junior/Middle Quality Assurance Intern AI Engineer Senior UI/UX Designer Branding Executive (EB) Intern Fullstack/ReactJs/Python/Go Developer Intern ReactJS Developer Intern Fullstack (NodeJS, ReactJS) Developer Middle/Senior Backend Developer (NodeJS/Golang) onsite Singapore Junior/Middle PHP (Wordpress) Developer Mid-Senior Front-end Developer onsite Singapore Process Quality Assurance Manager Junior/Middle Java Developer Junior/Middle Golang Developer Junior/Middle .NET Developer Intern Employer Branding Junior/Middle NodeJs Developer Nhân viên Đào tạo Nội bộ Product Owner Support Engineering Junior/Middle Business Analyst Senior Java Developer Graphic Designer [Junior/Middle] Senior Quality Assurance Scrum Master/Project Manager [Intern/ Fresher] Marketing Manager Marketing Manager Sales Development Representative Senior Account Executive Head Of Account Management Senior Partner Sales Manager Senior Solution Sales Executive Senior Sales Executive Middle/Senior Python Developer Automation Test Engineer Talent Acquisition Specialist Talent Acquisition Intern Chuyên viên Đào tạo Nội bộ Quality Assurance [Junior/Middle] Senior Fullstack Developers IT Recruiter [Mid/Senior] Social Content Creator [thị trường Nhật] Front-end Developer [Junior/Middle] IT Recruitment Lead Chuyên viên Truyền thông Nội bộ Social Content Creator Technical Project Manager Delivery Manager [Global Market] Junior/Mid Java Engineer [Onsite] Junior AI Engineer International Sales Executive Senior React Native Engineer Junior Java Engineer Mid/Senior Java Developer Mid/Senior Java Developer Mid-Senior Nodejs Developer Junior/Mid-level .NET Software Engineer Junior /Middle Business Analyst Financial Assistant [Onsite] Junior ReactJS Developer Intern Golang Developer Senior IT Content Creator (Onsite/ Remote) Accountant Intern Office Administrator Human Resource Manager (HRM) Middle/Senior Quality Assurance ReactJS Developer Middle ReactJS Developer (Junior/Mid) Digital Marketing Executive Delivery Manager (Japan Market) Recruitment Lead Senior FullStack Developer Đà Nẵng Senior Flutter (Mobile) Golang Developer [Senior/Lead] Fresher Golang Solution Architect Digital Marketing Lead (Onsite/Remote) Sales Manager [Hanoi & HCMC] Business Development Interns [Hanoi & HCMC] Senior Sales Executive (Global Market) Senior Node.Js Developer DevOps Engineer (Azure) Quản lý chất lượng dự án phần mềm (PQA) Mid/Senior Java Developer [Japan] Senior/ Middle C# Developer [Japan] Senior/ Middle C/C++ Developer [Japan] IT Project Manager Middle/Senior Golang Developer Senior Sales Executive (Japan Market) Senior Java Developer Mid-Senior Java Developer Java Technical Leader Bridge System Engineer (BrSE) - Japan Senior .NET /C# Developer Middle/Senior Python Developer Performance Marketing Specialist Sales Assistant (Graduate - Junior) Senior Vue.Js Developer Senior ReactJs Developer PHP Team Leader Senior PHP Developer Chuyên Viên IT Recruiter Junior PHP Developer Automation QA Vị trí khác
Data engineer
Vị trí bạn quan tâm
Java Engineer
Data Scientist
Senior Recruiter
Intern AI Solution Engineer (Japanese)
Senior Data engineer
.Net Engineer
Mid/Senior Python Engineer
Data engineer
AI Engineer
Senior Backend Engineer (Java/Golang)
Technical Leader - Python/AI
Technical Lead
QA Engineer
Senior Recruiter (Japan Market)
Automation QA Manager
Software Engineer for JP market
BrSE for JP Market
DevOps Engineer
Account Manager
Senior BE in Python Django
Quality Assurance Leader
Application Managed Service L2
Senior Java Developer
Senior PHP Developer
Senior International Sales Executive
Business Analyst
Korea Sales Manager
Senior PQA Lead
Intern AI Solution Engineer
Sales Executive (Japan Market)
Devops Engineer
Python Developer
Mobile Developer
Technical Project Manager
Frontend Developer
Senior Solutions Architect / Consultant
Business Analyst/ Product Owner
Intern Project Coordinator
NodeJS Developer
AI Engineer
Golang Developer
Senior Frontend Developer
Product Quality Assurance
Senior International Sales
Frontend Developer
Intern Sales IT
Quality Assurance Engineer
Senior Nodejs Developer
Middle Fullstack Developer
Senior C# Developer
Fresher Japanese Content Creator (EB)
IT Helpdesk
Senior Ruby on Rails Developer
Business Analyst
Manual Tester
Middle/Senior Auto QA Engineer
Senior Frontend Developer (ReactJS)
Project Manager/ Scrum Master
Sales Manager
Sales Manager - HN
Content Marketing Specialist (Creative)
Intern Sales - HCM
Senior Software Engineer
Project Manager
Junior/Middle Quality Assurance
Intern AI Engineer
Senior UI/UX Designer
Branding Executive (EB)
Intern Fullstack/ReactJs/Python/Go Developer
Intern ReactJS Developer
Intern Fullstack (NodeJS, ReactJS) Developer
Middle/Senior Backend Developer (NodeJS/Golang) onsite Singapore
Junior/Middle PHP (Wordpress) Developer
Mid-Senior Front-end Developer onsite Singapore
Process Quality Assurance Manager
Junior/Middle Java Developer
Junior/Middle Golang Developer
Junior/Middle .NET Developer
Intern Employer Branding
Junior/Middle NodeJs Developer
Nhân viên Đào tạo Nội bộ
Product Owner
Support Engineering
Junior/Middle Business Analyst
Senior Java Developer
Graphic Designer [Junior/Middle]
Senior Quality Assurance
Scrum Master/Project Manager [Intern/ Fresher]
Marketing Manager
Marketing Manager
Sales Development Representative
Senior Account Executive
Head Of Account Management
Senior Partner Sales Manager
Senior Solution Sales Executive
Senior Sales Executive
Middle/Senior Python Developer
Automation Test Engineer
Talent Acquisition Specialist
Talent Acquisition Intern
Chuyên viên Đào tạo Nội bộ
Quality Assurance [Junior/Middle]
Senior Fullstack Developers
IT Recruiter [Mid/Senior]
Social Content Creator [thị trường Nhật]
Front-end Developer [Junior/Middle]
IT Recruitment Lead
Chuyên viên Truyền thông Nội bộ
Social Content Creator
Technical Project Manager
Delivery Manager [Global Market]
Junior/Mid Java Engineer [Onsite]
Junior AI Engineer
International Sales Executive
Senior React Native Engineer
Junior Java Engineer
Mid/Senior Java Developer
Mid/Senior Java Developer
Mid-Senior Nodejs Developer
Junior/Mid-level .NET Software Engineer
Junior /Middle Business Analyst
Financial Assistant [Onsite]
Junior ReactJS Developer
Intern Golang Developer
Senior IT Content Creator (Onsite/ Remote)
Accountant Intern
Office Administrator
Human Resource Manager (HRM)
Middle/Senior Quality Assurance
ReactJS Developer Middle
ReactJS Developer (Junior/Mid)
Digital Marketing Executive
Delivery Manager (Japan Market)
Recruitment Lead
Senior FullStack Developer Đà Nẵng
Senior Flutter (Mobile)
Golang Developer [Senior/Lead]
Fresher Golang
Solution Architect
Digital Marketing Lead (Onsite/Remote)
Sales Manager [Hanoi & HCMC]
Business Development Interns [Hanoi & HCMC]
Senior Sales Executive (Global Market)
Senior Node.Js Developer
DevOps Engineer (Azure)
Quản lý chất lượng dự án phần mềm (PQA)
Mid/Senior Java Developer [Japan]
Senior/ Middle C# Developer [Japan]
Senior/ Middle C/C++ Developer [Japan]
IT Project Manager
Middle/Senior Golang Developer
Senior Sales Executive (Japan Market)
Senior Java Developer
Mid-Senior Java Developer
Java Technical Leader
Bridge System Engineer (BrSE) - Japan
Senior .NET /C# Developer
Middle/Senior Python Developer
Performance Marketing Specialist
Sales Assistant (Graduate - Junior)
Senior Vue.Js Developer
Senior ReactJs Developer
PHP Team Leader
Senior PHP Developer
Chuyên Viên IT Recruiter
Junior PHP Developer
Automation QA
Vị trí khác
Vui lòng chọn vị trí tuyển dụng bạn quan tâm
Tải lên CV của bạn
(Hãy tải lên CV của bạn ở định dạng .doc .docx .pdf không quá 5MB)
Chọn file hoặc kéo thả vào đây
Vui lòng tải CV của bạn
Bạn biết đến thông tin tuyển dụng của TECHVIFY qua kênh nào?
Bạn biết đến thông tin tuyển dụng của TECHVIFY qua kênh nào?
Group facebook
Fanpage TECHVIFY Careers
Google Search
Tiktok Techvify Official
Tiktok Celine Nguyen
Tiktok Thedev_dad
Chuyên viên tuyển dụng Techvify
Youtube TECHVIFY Careers
Vui lòng chọn Bạn biết đến thông tin tuyển dụng của TECHVIFY qua kênh nào?
Gửi CV Qua Email
careers@techvify.com.vn
Liên Hệ Số Điện Thoại
02477760688"
Data Analytics Engineer,Colgate-Palmolive,"Bến Cát, Binh Duong, Vietnam",https://vn.linkedin.com/jobs/view/data-analytics-engineer-at-colgate-palmolive-4281356935?position=16&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=EIWqI7rOjGpqefgxEg%2B1tw%3D%3D,"No Relocation Assistance Offered
Job Number #168531 - Ben Cat town, Binh Duong, Viet Nam
Colgate-Palmolive Company is a global consumer products company operating in over 200 countries specialising in Oral Care, Personal Care, Home Care, Skin Care, and Pet Nutrition. Our products are trusted in more households than any other brand in the world, making us a household name!
Join Colgate-Palmolive, a caring, innovative growth company reimagining a healthier future for people, their pets, and our planet. Guided by our core values—Caring, Inclusive, and Courageous—we foster a culture that inspires our people to achieve common goals. Together, let's build a brighter, healthier future for all.
The Data Analytics Engineer at My Phuoc Plant is responsible for using analytical tools to uncover meaningful insights, trends, and patterns that directly contribute to informed business decisions.
Design, build, and maintain robust, scalable, and efficient ETL/ELT pipelines for manufacturing data from various systems (MES, Historian, PLCs, SAP, IoT, Google drive, Snowflake) into data lakes/warehouses/analytical platforms.
Develop and optimize data models within platforms like Snowflake, Databricks, or Google BigQuery to support analytics, reporting, and advanced analytics. Ensure data integrity, performance, and scalability for manufacturing data.
Implement automated data quality checks, validation, and monitoring processes in data pipelines for accurate, complete, and consistent manufacturing data. Embed data governance and security best practices.
Collaborate with Data Scientists and Analysts to operationalize analytical and machine learning models by building feature engineering pipelines, deploying models, and monitoring performance/data drift.
Evaluate, select, implement, and manage underlying data infrastructure and tooling (e.g., Airflow, streaming platforms, cloud services, computation engines) for efficient manufacturing data processing and analytics.
Identify and resolve data-related issues, pipeline failures, and performance bottlenecks. Optimize data processing and queries for efficiency, reduced latency, and cost management.
Serve as a technical expert, collaborating with OT, IT, production, engineering, and data consumption teams to understand data requirements, provide solutions, and ensure data availability/reliability.
Responsible for relevant EHS/Quality/FP&R performance expectations as defined in each standard's RACI matrix.
Requirements
Bachelor's degree in Data Analytics, Computer Science, Data Science, Information Technology, or a related discipline.
Minimum of 2 years of hands-on experience with business intelligence tools for data analysis and processing, such as Data Studio, Tableau, Domo, and Sigma.
Proficiency in data fields like SQL, Python, or R, and cloud BI tools such as Domo.
Interpersonal skills of communication, collaboration and problem solving
A strong grasp of supply chain principles, including production planning, inventory, logistics, procurement, and quality management, is vital. Understanding their interaction within the supply chain is essential for data analysis and process optimization.
Proactively seeks and adapts to new technologies with an open mindset.
Our Commitment to Inclusion
Our journey begins with our people—developing strong talent with diverse backgrounds and perspectives to best serve our consumers around the world and fostering an inclusive environment where everyone feels a true sense of belonging. We are dedicated to ensuring that each individual can be their authentic self, is treated with respect, and is empowered by leadership to contribute meaningfully to our business.
Equal Opportunity Employer
Colgate is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, colour, religion, gender, gender identity, sexual orientation, national origin, ethnicity, age, disability, marital status, veteran status (United States positions), or any other characteristic protected by law.
Reasonable accommodation during the application process is available for persons with disabilities. Please complete this request form should you require accommodation."
Data Engineer,ĐÚNG NGƯỜI ĐÚNG VIỆC Community,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-%C4%91%C3%BAng-ng%C6%B0%E1%BB%9Di-%C4%91%C3%BAng-vi%E1%BB%87c-community-4311175213?position=17&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=0YLfeXM%2Be5nNUeDOXNVIsw%3D%3D,"Mô tả công việc
Xây dựng & tối ưu hóa hệ thống xử lý dữ liệu
Xây dựng công cụ, framework giúp quản lý, tích hợp nhiều nguồn dữ liệu khác nhau, tạo nền tảng khai thác dữ liệu hiệu quả cho nhóm Data Science và Business.
Phát triển data warehouse hiệu suất cao, nền tảng BI phục vụ phân tích dữ liệu lớn
Kiểm tra, làm sạch và tối ưu hóa dữ liệu, đảm bảo độ chính xác, khả dụng của dữ liệu cho người dùng cuối.
Phát triển nền tảng dữ liệu & giám sát hệ thống
Đánh giá và phát triển công nghệ, giải pháp mới nhằm nâng cao độ tin cậy, khả năng mở rộng và sẵn sàng cao cho hạ tầng dữ liệu.
Làm việc cùng Data Scientist để thiết kế mô hình Machine Learning, phân tích hành vi khách hàng, cải thiện trải nghiệm người dùng.
Thiết kế hệ thống giám sát, cảnh báo, phát hiện lỗi hệ thống, bất thường dữ liệu và tự động hóa quy trình xử lý sự cố.
Xây dựng hệ thống ETL & xử lý dữ liệu
Hỗ trợ nền tảng dữ liệu & tự động hóa
Phối hợp với các bộ phận có liên quan để xây dựng công cụ tự động giúp tối ưu quy trình và tăng tốc pipeline xử lý dữ liệu.
Phát triển phương pháp giải quyết các vấn đề dữ liệu trong toàn bộ vòng đời dữ liệu/dịch vụ.
Xây dựng dashboard, hệ thống giám sát và tự động hóa để phát hiện, xử lý lỗi hệ thống một cách thông minh.
Yêu cầu công việc
Tốt nghiệp cao đẳng, đại học trở lên
Ưu tiên Tốt nghiệp chuyên ngành Khoa học máy tính, Hệ thống thông tin, Kỹ thuật phần mềm hoặc lĩnh vực liên quan.
Giao tiếp tốt, ngoại ngữ tốt, phối hợp công việc tốt
Trên 3 năm kinh nghiệm ở vị trí Data Engineering hoặc vai trò tương tự.
Có kinh nghiệm làm việc với SQL, hệ thống dữ liệu lớn, hệ thống cơ sở dữ liệu, hệ thống phân tán. Kinh nghiệm tối ưu hóa hiệu suất và mở rộng hệ thống.
Thành thạo một hoặc nhiều ngôn ngữ lập trình: Python, Java, Scala, Net, .Net,...
Kinh nghiệm triển khai hệ thống dữ liệu lớn trên Hadoop Ecosystem (Spark, Hadoop, Clickhouse, ELK…).
Có kiến thức về ETL, Data Warehouse (BigQuery, Snowflake, Redshift).
Biết sử dụng các công cụ hỗ trợ quản lý sản phẩm: Jira, Azure, Confluence,...
Tại sao bạn yêu thích làm việc tại đây
Thu nhập: Thỏa thuận theo năng lực
Đầy đủ các chế độ theo luật lao động hiện hành.
Chính sách phúc lợi theo quy định của Công ty đa dạng: Chăm sóc sức khỏe định kì hàng năm; Gói bảo hiểm sức khỏe chuyên biệt (FPT Care – Khám chữa bệnh miễn phí tại tất cả các bệnh viện); Các hoạt động tri ân, chăm lo đời sống tinh thần CBNV và Thân nhân...
Môi trường làm việc thân thiện, cởi mở
Dự án trọng điểm, ứng dụng những công nghệ mới nhất.
Nhiều cơ hội phát triển và thăng tiến.
Văn hóa Doanh nghiệp đặc sắc, sinh động bậc nhất với nhiều các hoạt động hấp dẫn: Tân binh, 72 giờ trải nghiệm, teambuilding, thi trạng, hội làng, hội diễn Sao Chổi, sinh nhật FPT, ngày 08/03, ngày 20/10,....
Địa điểm làm việc
Hà Nội: Toà FPT, số 10 Phạm Văn Bạch, Cầu Giấy
Thời gian làm việc
Thứ 2 - Thứ 6 (từ 08:30 đến 18:00)"
Data Engineer,ĐÚNG NGƯỜI ĐÚNG VIỆC Community,"Phường Chí Minh, Hai Duong, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-%C4%91%C3%BAng-ng%C6%B0%E1%BB%9Di-%C4%91%C3%BAng-vi%E1%BB%87c-community-4313646946?position=18&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=H2Iilq8Jw8SfnMc9RVm5ew%3D%3D,"Mô tả công việc
Làm việc với đội BA để nghiên cứu, phân tích các yêu cầu
Phân tích yêu cầu, thiết kế giải pháp và viết các tài liệu kỹ thuật trong quá trình phát triển
Chủ động nghiên cứu các công nghệ, lĩnh vực, kỹ thuật được phân công hoặc liên quan đến công việc
Địa điểm làm việc
Hồ Chí Minh: Tòa nhà 678, số 67 Hoàng Văn Thái, phường Tân Phú, Quận 7
Thời gian làm việc
Thứ 2 - Thứ 6 (từ 08:30 đến 18:00)
Thứ 7 (từ 08:30 đến 12:00)
Yêu cầu công việc
Trên 3 năm kinh nghiệm làm việc với các công cụ process data: batching (Spark, Pandas) hoặc streaming (Flink, Storm, Spark-Streaming)
Trên 3 năm kinh nghiệm làm việc với (một trong) các công cụ tạo, quản lý data pipeline: Azure Data Factory, Azkaban, Luigi, Airflow v.v.
Có kinh nghiệm làm việc với Linux, Git
Có kinh nghiệm với SQL và NoSQL DB: bao gồm Postgres, sql server, mongodb và elasticsearch.
Tại sao bạn yêu thích làm việc tại đây
Làm việc tại công ty hàng đầu Việt Nam, luôn không ngừng phát triển với nhiều cơ hội thăng tiến bản thân.
Được đào tạo chuyên nghiệp, hoàn toàn miễn phí trước khi làm việc.
Môi trường làm việc trẻ, năng động và thân thiện.
Tham gia đầy đủ các chế độ BHYT, BHXH, BHTN.
Thu nhập hấp dẫn, phù hợp năng lực bản thân.
Lương tháng 13 và thưởng theo hiệu quả kinh doanh.
Khám sức khỏe định kỳ.
Thường xuyên tổ chức các chương trình hội thao, hội diễn văn nghệ, tân niên, tất niên…"
Data Engineer,Hitachi Digital Services,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-hitachi-digital-services-4304541383?position=19&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=rQelVOJajYMBVF3q5rP7HA%3D%3D,"Our Company
Imagine the sheer breadth of talent it takes to bring a better tomorrow closer to today. We don’t expect you to ‘fit’ every requirement – your life experience, character, perspective, and passion for achieving great things in the world are equally as important to us.
Design, implement, maintain data models
Support data model and ETL solutions in production.
Build and maintain data pipelines using tools such as NIFI, Airflow, etc.
Evaluate and define KPIs to monitor and manage data quality
Tuning and improving the performance of DB/DWH.
Support data scientists, data analyst to explore data in Data warehouse.
Build and maintain integration data flow provided to other departments.
Maintain update-to-date documentation of data catalog, ETL flows, data dictionary.
Practice sustainable incident response and blameless postmortems.
Participate in end to end engineering solutions with regard to data processing and data delivery and integration, including data processing job/pipeline tool suite, batch framework and platform, micro-service framework and platform.
BASIC QUALIFICATIONS
Skill in one of the following languages Python / Java / Kotlin, and target in Big Data career.
Skill in one of the following technologies: PySpark / Spark / Flink / Airflow
PREFERRED QUALIFICATIONS
Experience in Design, ETL, data modeling and developing SQL database solutions.
Good understanding of data management - data lineage, meta data, data quality, data governance
Have basic understanding about big data, and big data platform.
Knowledge with S3/ Spark/ Jupyter/ Flink is a plus point.
Ability to debug and optimize code and automate routine tasks.
Skills in task and time management, proactive problem solver.
Teamwork and communication skills.
Championing diversity, equity, and inclusion
How We Look After You
We want to help you take care of your today and tomorrow – at home and at work. Which is why we offer industry-leading benefits that go far beyond compensation. That means support, services, and resources that also take care of your holistic health and wellbeing. We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. Here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent).
We’re a global team of innovators, together, we harness engineering excellence and passion for creating meaningful solutions to complex challenges. We turn organizations into industry leaders that can a make positive impact on their industries and society.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.
Fostering innovation through diverse perspectives
Hitachi is a global company operating across a wide range of industries and regions. One of the things that sets Hitachi apart is the diversity of our business and people, which drives our innovation and growth.
We are committed to building an inclusive culture based on mutual respect and merit-based systems. We believe that when people feel valued, heard, and safe to express themselves, they do their best work.
How We Look After You
We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status
or any other protected characteristic.
Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success."
Data Engineer,GFT Technologies,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/data-engineer-at-gft-technologies-4300632198?position=20&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=Rohpn7TgTRs%2BfLKXEt%2FYIw%3D%3D,"GFT Technologies is driving the digital transformation of the world’s leading financial institutions. Other sectors, such as industry and insurance, also leverage GFT’s strong consulting and implementation skills across all aspects of pioneering technologies, such as cloud engineering, artificial intelligence, the Internet of Things for Industry 4.0, and blockchain.
With its in-depth technological expertise, strong partnerships and scalable IT solutions, GFT increases productivity in software development. This provides clients with faster access to new IT applications and innovative business models, while also reducing risk.
We’ve been a pioneer of near-shore delivery since 2001 and now offer an international team spanning 16 countries with a global workforce of over 9,000 people around the world. GFT is recognised by industry analysts, such as Everest Group, as a leader amongst the global mid-sized Service Integrators and ranked in the Top 20 leading global Service Integrators in many of the exponential technologies such as Open Banking, Blockchain, Digital Banking, and Apps Services.
Sign-on Bonus:
Eligible for candidates who are currently employed elsewhere and able to join GFT
within 30 days
of offer acceptance.
Role Summary
As a Data Engineer at GFT, you will play a pivotal role in designing, maintaining, and enhancing various analytical and operational services and infrastructure crucial for the organization's functions. You'll collaborate closely with cross-functional teams to ensure the seamless flow of data for critical decision-making processes.
Key Activities
Data Infrastructure Design and Maintenance:
Architect, maintain, and enhance analytical and operational services and infrastructure, including data lakes, databases, data pipelines, and metadata repositories, to ensure accurate and timely delivery of actionable insights.
AWS Glue Development:
Experience in AWS Glue data pipeline development (not drag-and-drop).
Collaboration: Work closely with data science teams to design and implement data schemas and models, integrate new data sources with product teams, and collaborate with other data engineers to implement cutting-edge technologies in the data space.
Data Processing:
Develop and optimize large-scale batch and real-time data processing systems to support the organization's growth and improvement initiatives.
Workflow Management: Utilize workflow scheduling and monitoring tools like Apache Airflow and AWS Batch to ensure efficient data processing and management.
Quality Assurance: Implement robust testing strategies to ensure the reliability and usability of data processing systems.
Continuous Improvement:
Stay abreast of emerging technologies and best practices in data engineering, and propose and implement optimizations to enhance development efficiency.
Required Skills
4-6 years of experience as a Data Engineer.
Professional
developer with design pattern knowledge.
Experience with production-grade code.
Able to develop quickly, work overtime, and perform under pressure.
Technical Expertise: Proficient in Unix environments, distributed and cloud computing, Python frameworks (e.g., pandas, pyspark), version control systems (e.g., git), and workflow scheduling tools (e.g., Apache Airflow).
Database Proficiency: Experience with columnar and big data databases like Athena, Redshift, Vertica, and Hive/Hadoop.
Cloud Services: Familiarity with
or other cloud services like Glue, EMR, EC2, S3, Lambda, etc.
Containerization: Experience with container management and orchestration tools like Docker, ECS, and Kubernetes.
CI/CD: Knowledge of CI/CD tools such as Jenkins, CircleCI, or AWS CodePipeline.
Nice-to-have Requirements
Programming Languages: Familiarity with JVM languages like Java or Scala.
Database Technologies: Experience with RDBMS (e.g., MySQL, PostgreSQL) and NoSQL databases (e.g., DynamoDB, Redis).
BI Tools: Exposure to enterprise BI tools like Tableau, Looker, or PowerBI.
Data Science Environments: Understanding of data science environments like AWS Sagemaker or Databricks.
Monitoring and Logging: Knowledge of log ingestion and monitoring tools like ELK stack or Datadog.
Data Privacy and Security: Understanding of data privacy and security tools and concepts.
Messaging Systems: Familiarity with distributed messaging and event streaming systems like Kafka or RabbitMQ.
Due to the high volume of applications we receive, we are unable to respond to every candidate individually. If you have not received a response from GFT regarding your application within 10 workdays, please consider that we have decided to proceed with other candidates. We truly appreciate your interest in GFT and thank you for your understanding."
Data Engineer,Sending Labs,Vietnam,https://vn.linkedin.com/jobs/view/data-engineer-at-sending-labs-4288680181?position=21&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=Ig7QTPJQdkKXh4Eviaz%2Bqw%3D%3D,"Building the Web3 communications stack. Sending.Network is the first Web3 native communications protocol supporting fully decentralized, private community chat. Built on Sending.Network, Sending.Me is the next gen communications app for communities large and small
You Will Be Responsible For
Data Pipeline Development: Build and maintain scalable ETL/ELT pipelines for real-time and batch crypto market data; integrate data from APIs, trading venues, blockchain nodes, and third-party platforms.
Infrastructure & Architecture: Design and manage databases, warehouses, and data lakes; leverage cloud-native and distributed processing tools (e.g., AWS, Spark, Flink) to support large-scale workloads.
Operations & Tooling: Monitor, troubleshoot, and optimize pipelines to ensure reliability, performance, and cost efficiency; implement data governance, security, and access control best practices.
Research & Trading Support: Curate, label, and validate datasets for model training, backtesting, and strategy evaluation; prepare high-quality datasets and reports to support quantitative research and trading teams.
Collaboration & Continuous Improvement: Work with researchers and traders to translate requirements into technical solutions; stay current with new technologies, industry developments, and AI/data science applications in trading.
Ideal Profile
Bachelor’s or Master’s degree in Computer Science, Data Science, Engineering, Finance, Economics, or a related technical field.
Experience in data engineering, data science, or related fields (crypto/finance/Web3 experience a strong plus).
Strong programming skills in Python and SQL (R, Rust, or Java is a plus).
Proven experience building and maintaining ETL/ELT pipelines, with exposure to real-time data streaming.
Hands-on experience with cloud platforms (AWS, GCP, or Azure) and cloud-native services.
Familiarity with on-chain analytics tools, crypto data APIs, and financial datasets.
Strong understanding of data governance, quality control, and reproducibility in research workflows.
Excellent attention to detail, problem-solving, and ability to work independently in a fast-paced environment.
Bonus: familiarity with quantitative research, backtesting, and trading workflows, knowledge of machine learning data pipelines and evaluation processes.
What's on Offer?
Excellent career development opportunities
Competitive compensation (salary + research/performance bonuses)
Access to advanced tools and unique datasets
Growth and learning opportunities with a leading-edge research team"
ETL/Data Engineer,LG CNS Vietnam,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/etl-data-engineer-at-lg-cns-vietnam-4303862330?position=22&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=BSc4LhjiIj0v1HL2ekTBBg%3D%3D,"Job DescriptionDevelop in web application development in different areas such as: AI Machine learning / Data Learning Solution and other company solutionsEnsure the best possible performance, quality and responsiveness of back-end applicationsAlways keep up to date with emerging technologies
Requirement[Required]Bachelor’s degree in information technology, or related fields.More than 3 years of experience in python developExperience designing data analytics pipelinesExperience in using various data analysis toolsExperience storing data in various formats (Parquet, CSV, Iceberg, etc.)Memory Optimization ExperienceExperience in gaining visibility/traceability of data analytics pipelinesExperience analyzing data in Python, Spark environmentsExperience using data analytics services such as SparkExperience in developing cloud (AWS) environmentsExperience in developing with RDBMS (MySQL) SQLExperience with collaboration tools such as Git, Jira, Confluence, etc.Understand and leverage CI/CD
[Nice to have]Team leading experienceAble to communicate in Korean
OpportunityAttractive salary and bonus will be discussed after going through CV & InterviewReview capacity annually and adjust salary increases according to work performance.Health care: Premium health insurance, Annual health check-upYoung working environmentGood career development opportunities with interesting and challenging projects.English, Korean, technical, soft skills training courses.Opportunity to learn special courses from LG CNS, new technology and security.Gifts on holidays (April 30th - May 1st, September 2nd, Tet, etc.)Outdoor activities with company support: sports clubs, team building, happy hour parties, birthdays, travel, employee and family events, etc.Working hours: 8 hours from Monday - Friday (8 hours/day)
Location15th Floor, Keangnam Landmark 72, Me Tri, Nam Tu Liem, Hanoi"
Data Engineer (Good English),Hitachi Digital Services,Ho Chi Minh City Metropolitan Area,https://vn.linkedin.com/jobs/view/data-engineer-good-english-at-hitachi-digital-services-4273263045?position=23&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=dz9BNLA%2F0cnvw5oBEIJM4A%3D%3D,"Imagine the sheer breadth of talent it takes to inspire the future. We don’t expect you to ‘fit’ every requirement – your life experience, character, perspective, and passion for achieving great things in the world are equally important to us.
What You’ll Bring
3+ years of experience in data engineering, analytics, or BI roles.
Proficiency in SQL and Strong experiences in Python.
Experience with cloud data platforms (AWS, Azure).
Have experiences with PowerBI and familiar with Tableau, AWS Quicksight, Qlik,.. is a plus
Excellent communication and stakeholder management skills.
Championing diversity, equity, and inclusion
How We Look After You
We want to help you take care of your today and tomorrow – at home and at work. Which is why we offer industry-leading benefits that go far beyond compensation. That means support, services, and resources that also take care of your holistic health and wellbeing. We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. Here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent).
We’re a global team of innovators, together, we harness engineering excellence and passion for creating meaningful solutions to complex challenges. We turn organizations into industry leaders that can a make positive impact on their industries and society.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status or any other protected characteristic. Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success.
Fostering innovation through diverse perspectives
Hitachi is a global company operating across a wide range of industries and regions. One of the things that sets Hitachi apart is the diversity of our business and people, which drives our innovation and growth.
We are committed to building an inclusive culture based on mutual respect and merit-based systems. We believe that when people feel valued, heard, and safe to express themselves, they do their best work.
How We Look After You
We help take care of your today and tomorrow with industry-leading benefits, support, and services that look after your holistic health and wellbeing. We’re also champions of life balance and offer flexible arrangements that work for you (role and location dependent). We’re always looking for new ways of working that bring out our best, which leads to unexpected ideas. So here, you’ll experience a sense of belonging, and discover autonomy, freedom, and ownership as you work alongside talented people you enjoy sharing knowledge with.
We’re proud to say we’re an equal opportunity employer and welcome all applicants for employment without attention to race, colour, religion, sex, sexual orientation, gender identity, national origin, veteran, age, disability status
or any other protected characteristic.
Should you need reasonable accommodations during the recruitment process, please let us know so that we can do our best to set you up for success."
Data Engineer,Galaxy FinX,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-galaxy-finx-4307876798?position=24&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=G%2BIjkRFa2rpSbxWz9n8SzQ%3D%3D,"As a Data Engineer, you'll lead initiatives optimizing data flow, ensuring quality, and implementing security measures.
Job Description:
Data Engineering
Design and implement scalable and secure data pipelines for collecting, processing, and storing data.
Develop and maintain datasets, ensuring accuracy, completeness, and compliance with security standards.
Work closely with data scientists and analysts to understand data needs and implement solutions that meet compliance requirements.
Implement robust data quality checks and validation processes to identify and rectify inconsistencies, ensuring data integrity.
Optimize data processing and storage for efficiency, cost-effectiveness, and compliance.
Identify and implement solutions to enhance the performance of data pipelines, with a focus on security best practices.
Job Requirements:
Bachelor's degree in Computer Science, Information Technology, or a related field.
2 or more years of experience as a Data Engineer, ETL Developer, or similar roles, with expertise in ETL processes and tools, preferably in banking or financial services.
Experience in data modeling, data lake, and data warehousing
Strong proficiency in programming languages such as Python, Java, or Scala.
Strong proficiency in Airflow for designing and managing complex workflows and dependencies.
Solid understanding of data analytics and flow analysis for mobile app user interfaces.
Familiarity with database systems (e.g., SQL, NoSQL) and data integration tools.
Strong analytical and problem-solving skills with a detail-oriented mindset.
Excellent communication and collaboration skills to work effectively with cross-functional teams.
Knowledge of cloud platforms (e.g., AWS, GCP) and related data technologies is a plus.
Knowledge of security practices in the context of data handling.
Familiarity with version control systems (e.g., Git) and agile software development practices.
Good communication skills & ability to explain your findings to business/technical stakeholders in an efficient manner.
Be able to manage time effectively & deliver high-quality work.
Ability to work independently and manage multiple tasks within deadlines.
Possess a proactive mindset & positive attitude, able to bring in good influences to your teammates & stakeholders."
Python Developer (All Level),Saigon Technology - Accelerate Software Development,"Da Nang, Da Nang City, Vietnam",https://vn.linkedin.com/jobs/view/python-developer-all-level-at-saigon-technology-accelerate-software-development-4312583862?position=25&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=%2FlfLjUk7L%2FOiVO1C3PSwUw%3D%3D,"Job Description
Work closely with our clients to develop product and configure the application to their business environment
Work within a team & communicate effectively across teams
Deal with technical challenges, programing tasks while managing client expectations and building long-term customer relations
Implement assigned tasks from client/manager
Job Requirements
MUST HAVE SKILLS:
From 2+ years of experience in Python
Experience with any Python Web Framework: Django/FastAPI/Flask
Experience working with Jinja2
Experience with Python ORM: SQLAlchemy, Pydantic
Experience working with Docker, Git, Git flow, CI/CD
Experience with Unit Test, Intergration Test
Experience with Scaling and Building Stable Systems
Have knowledge with Caching, DB, DB design
Good at English
NICE TO HAVE
Experience with AI/ML/DL, especially ChatGPT and OpenAI... integration
Experience with Data Engineer, Big Data/ ETL/ Airflow
Experience working with micro-service projects
Experience with NoSQL (MongoDB/DynamoDB), Vector Database (OpenSearch, Qdrant...)
Experience with cloud services: AWS/Azure/GCP
Skills Tags
: Python, Django, DB
Benefits
Competitive Salary and Brilliant Health Benefits
Attractive salary (13th-month salary, salary review twice/year) and project bonus
Bonus programs for candidate referral, technical article writing
Interest-free loan support for personal plan
Allowance for sickness, maternity, paternity and periodic health examination
PVI health care program
The staff of the quarter and year reward
Progressive and Fun Working Environment
A professional English-speaking working environment with Agile – Scrum model
Hybrid Working Model: Flexible working time and WFH support.
Surrounded with friendly, open-minded, young and supportive colleagues
Annual company trip and regular team-building parties, party celebration (Christmas, Birthday, Mid-autumn,...), Sports clubs (football, badminton, swimming …)
Valuable Training
Sponsor examination fee for professional certificates (AWS, Azure, IELTS, PMP, Scrum Master,...)
Sponsor fee for joining any technical training sessions and courses.
Free English workshops"
Data Engineer,GMO-Z.com RUNSYSTEM JSC.,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-gmo-z-com-runsystem-jsc-4293589144?position=26&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=MEQZ9PN7vP3jli1VGJIAjw%3D%3D,"Level: Experienced, Junior, Middle
Khối: DX Solution & Product
Trạng thái: Inactive
Số lượng tuyển: 1
Thu nhập: upto 20 triệu
Địa điểm: Hà Nội
Nội dung công việc
Phân tích và thiết kế kho dữ liệu
Tối ưu hiệu năng của hệ thống
Tham gia phát triển báo cáo
Tham gia cài đặt và cấu hình môi trường
Tư vấn các vấn đề liên quan tới chất lượng dữ liệu, nghiên cứu và đưa ra phương pháp để tối ưu việc kết xuất, chuyển đổi dữ liệu
Yêu cầu công việc
Tốt nghiệp hoặc sắp tốt nghiệp chuyên ngành Khoa học Máy tính, Công nghệ Thông tin, Toán Tin hoặc các ngành liên quan.
Kinh Nghiệm
Từ 1-2 năm kinh nghiệm trong phát triển Python và Data Engineer. Kinh nghiệm với AI/ML là một lợi thế lớn.
Có kinh nghiệm trong phát triển phần mềm bằng python, html, css, javascript
Có kinh nghiệm với RDBMS (SQL Server, MySQL, PostgreSQL); sử dụng môi trường Linux (bash scripts, firewall,...); Đã làm việc với các công cụ Git, Kafka, Spark, Hadoop, Airflow, MageAI
Có kinh nghiệm visualize data sử dụng metabase, superset hoặc các công cụ tương tự
Hiểu biết về khái niệm Data Lake và Data Warehouse.
Kỹ Năng
Thành thạo ngôn ngữ lập trình Python.
Thành thạo sử dụng các công cụ AI như chatgpt, github copilot
Có kỹ năng về Crawler, Backend là một lợi thế.
Có kỹ năng sử dụng cơ sở dữ liệu SQL, NoSQL..
Có kỹ năng phân tích và xử lý dữ liệu cơ bản.
Khả năng sử dụng tiếng Anh thành thạo.
Tinh thần học hỏi, khả năng làm việc nhóm và chịu được áp lực công việc.
Chế độ đãi ngộ
Range lương: upto 20.000.000 VNĐ; 13 tháng lương/năm
Phụ cấp ăn trưa 730.000 VNĐ, thưởng 300.000 VNĐ quà sinh nhật và các loại thưởng khác (Thưởng thành tích, thưởng cá nhân xuất sắc, thưởng đạt chứng chỉ ngoại ngữ/chuyên môn từ 1.000.000 VNĐ - 5.000.000 VNĐ).
Hỗ trợ chi phí khóa học ôn, lệ phí thi chứng chỉ ngoại ngữ/chuyên môn.
Được làm việc trực tiếp với các khách hàng Nhật Bản
Được hưởng các phúc lợi khác như: Team-building hàng tháng, nghỉ mát hàng năm, khám sức khỏe định kỳ,...
Tất cả các chế độ theo quy định của Luật Lao động Việt Nam (BHXH, BHYT, BHTN,...).
Nghỉ phép năm và các ngày nghỉ lễ, Tết khác theo quy định của Nhà nước Việt Nam.
Môi trường làm việc trẻ trung, năng động, có lộ trình và cơ hội thăng tiến
Gửi CV kèm ảnh và tiêu đề email theo cấu trúc: [Tên vị trí_Tên ứng viên]
Email: hr@runsystem.vn
Website: https://runsystem.net Hoặc liên hệ trực tiếp với HR:
HR : Phương Thủy - Zalo: 0971024576 | Email: thuyntp@runsystem.net
HR: Thanh Huyền - Zalo: 0965003031 | Email: huyentt2@runsystem.net"
Data Engineer,GemCommerce,Hanoi Capital Region,https://vn.linkedin.com/jobs/view/data-engineer-at-gemcommerce-4312661538?position=27&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=bR0ft9ROevmcf8IAAyoXyg%3D%3D,"Why we're looking for you
We are on a mission to create the best possible user experiences, focusing on convenience, delight, and world-class design. With a team of talented, motivated individuals committed to continuous learning and innovation, we need someone like you to help us achieve our goals. By joining us, you will have the opportunity to build, collaborate, learn, and grow alongside some of the most dedicated professionals in the field. This role calls for a Data Engineer passionate about turning data into actionable insights, driving excellence, and contributing to products that make a difference.
Data Engineering: Design, develop, and maintain robust ETL pipelines to extract, transform, and load data from various sources such as Google Analytics, Facebook Ads, Google Ads, and Shopify Ads...
Data Warehousing: Build and optimize data warehouses to store and manage large datasets efficiently.
Data Transformation: Transform raw data into a consumable format for data analysts and scientists.
Data Quality: Ensure data quality, accuracy, and consistency throughout the data lifecycle.
Data Infrastructure: Collaborate with the team to design and implement a scalable data infrastructure.
Data Reporting: Develop and deliver compelling data visualizations and reports to present insights and recommendations.
Data Governance: Adhere to data governance policies and ensure data security.
Cross-functional Collaboration: Work closely with data analysts, data scientists, and business stakeholders to understand their data needs.
3+ years of experience in Data Engineering or a similar role in a product-based company, experience in the e-commerce domain or SaaS is a plus.
Proficiency in SQL and programming languages like Python.
Expertise in ETL tools and cloud services such as Airflow, DBT, AWS DMS, AWS Glue.
Strong understanding of data warehousing concepts and tools.
Experience with data visualization tools like Power BI.
Knowledge of database management systems and optimization techniques.
Excellent problem-solving and analytical skills.
Strong communication skills, both written and verbal.
Ability to work independently and as part of a team.
Passion for data and a drive to learn new technologies.
Salary range: Negotiate
Performance review and competency evaluation: 2 times per year.
Lunch meal + Parking fees provided.
Annual performance bonus & 13th – month salary: up to 3-month salary (based on your devotion and business efficiency)
Holiday bonus & Company trip
Remote working days: 12 days per year
Annual leaves: 12 days per year
Treatment: annual health check-up at top clinic in VN, weekly shoulder massage treatment, monthly team bonding, annual company trip,…
Training program: ensure to cover product training for all newbie and other training programs according to the Company’s policy
Why you’ll love working here
Learn product-thinking and customer-centric mindset.
Collaborative and supportive environment
Young and passionate colleagues
Professional and creative office view
Clear & Scientific Agile Framework on the whole company workflow & culture.
Join a dynamic team to design and develop high-performance E-commerce products. Take on ambitious and challenging objectives by contributing to groundbreaking business models in the market.
Flexible working time:
Morning: 8:00 am -12:00 pm (check in from 8:00 am to 8:45 am)
Afternoon: 1:00 pm - 5:45 pm (check out from 5:00 pm to 5:45 pm)"
Data Engineer (Open for Mid/Senior),Home Credit Vietnam,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-open-for-mid-senior-at-home-credit-vietnam-4266108193?position=28&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=KxoOpf0BIdlRSFLe5dmrnw%3D%3D,"JOB DESCRIPTION
Collaboration in Virtual Domain-aligned Team:​
Work closely with the Product Owner, Data Scientists/Analysts and other colleagues in agile way towards shared product goals​
Ensure and promote proper data model design and feasibility of data products including AI/ML model training/serving​
Continuous Improvement, Technical Excellence and Accountability:​
Follow and enrich data product building blocks and code templates, review and approve changes from contributors​
​Automated Testing, Code Quality and Documentation:​
Implement and maintain quality standards for data products​ (well-documented, reusable, scalable, performing, DRY)
​Ensure regular testing and validation are implemented as vital part of automated CI/CD​
Monitor and improve the performance and efficiency of data flows​
KEY REQUIREMENTS
Strong experience
in Data Engineer, skills in
Spark, Kafka, Python, Dbt, Iceberg, Airflow OR Cloud Technologies (AWS/GCP/Azure)
Demonstrated experience designing dimensional data model, effective algorithms to process data, software engineering principles
Proficiency in SQL and Python
Good English & communication
Good stakeholder management skills
Nice to have:
Finance/Banking/Ecommerce domain knowledge, especially in Risk, Antifraud, CRM models
Microservice architecture with Docker and Kubernetes development experience
COMPENSATION & BENEFITS
13th salary Fixed and KPI Bonus
Premium Health Care
24/7 Accidental Insurance
100% Social Insurance
Meal + Phone Allowance
Yearly Medical Checkup
15/18 Annual Leaves
Professional and Transparent Working Environment"
Data Engineer,"SOTATEK., JSC","Quận Cầu Giấy, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-sotatek-jsc-4263496601?position=29&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=2TysjKk6uxlyqW6%2BxX0ZYw%3D%3D,"Job Description
Design, develop, and maintain scalable data pipelines and ETL processes.
Implement and optimize data storage solutions, including data warehouses and data lakes.
Collaborate with data scientists and analysts to understand data requirements and provide efficient data access methods.
Ensure data quality, consistency, and security across all data platforms.
Develop and maintain data models, schemas, and data dictionaries.
Implement data governance policies and procedures.
Optimize query performance and data retrieval processes.
Integrate various data sources and APIs into our data ecosystem.
Automate data workflows and implement monitoring and alerting systems.
Stay up-to-date with emerging technologies and recommend improvements to our data architecture.
Job Requirements
Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
5+ years of experience in data engineering or similar roles.
Fluent in English
Certifications in relevant data platforms or cloud technologies (AWS, GG, Azure, Databrick, Dataiku, Snowflake)
Strong programming skills in Python, Scala, or Java.
Extensive experience with SQL, NoSQL, Vector databases
Experience with ELT/ETL open source frameworks (DBT, Fivetran)
Proficiency in big data technologies (e.g., Hadoop, Spark, Hive).
Experience with cloud platforms (e.g., AWS, Azure, GCP) and their data services.
Experience with data modeling, data warehousing concepts, and dimensional modeling.
Familiarity with data visualization tools (e.g., Tableau, Power BI).
Experience with version control systems (e.g., Git) and CI/CD pipelines.
Experienced in working with large ERP systems and has participated in building ETL pipelines from the analysis and design phases.
Flexible, with strong problem-solving skills and attention to detail.
Nice To Have
Agile mindset
Experience with streaming data processing (e.g., Kafka, Flink).
Knowledge of machine learning pipelines and MLOps practices.
Familiarity with containerization and orchestration tools (e.g., Docker, Kubernetes).
Familiarity with other data analytics tools such as Power BI, Tableau, etc.
Experience with data governance and compliance requirements (e.g., GDPR, CCPA).
Experience with database migration projects
Certifications in relevant data platforms or cloud technologies (AWS, GG, Azure, Databrick, Dataiku)
Compensation & Benefits
🎯Flexible working regime and health care:
Flexible timekeeping (from 8:00 - 9:00 to 17:30 - 18:30)
Minimum 14 paid leaves per annum for all employees after probation
01-day remote work per month
A flexitime allowance of 90-180 minutes per month for employees
01 hour paid leave per day for women having children under 12 months
Social insurance, health insurance, unemployment insurance and MIC care insurance
🎯Transparent And Fair Benefits
Saturday & Sunday OFF, Overtime pay is 150%, 200%, 300% as per labor law;
Work performance review 2 times/ year (in April and October)
13th-month salary
Bonus Policy: Public holidays (2/9, 30/4, 1/5, 1/1,...); Personal Performances; Excellent Team; Performance bonus in Token of the project;..
Men’s Day, Women’s Day, Children’s Day, Mid-Autumn Festival and other benefits under the provisions of the company
🎯Dynamic Environment And Open Culture
Year-end party, sports day, yearly company trip and quarterly team building,...with a generous budget
Socialize with colleagues through monthly Happy Hour
Monthly allowance when joining clubs: Soccer, Swimming, Yoga, Music,...
Nice & modern working space with young, dynamic & friendly colleagues and free coffee, tea, drinks,...
Flat, open and sharing culture with friendly management team; outsourcing company with product mindset
🎯Strong Learning Culture
Free training courses for technical and soft skills (presentation skills, communication skills, foreign language courses,...)
Account to log in to our online learning system, which contains thousands of valuable lectures (LMS)
Participate in workshops, seminars, tech talk,... with sharing from experts inside and outside the company
Working opportunities with technical gurus who built and operated world-class applications with millions of users."
Data Engineer,WorldQuant,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-worldquant-4311373770?position=30&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=SCuopFwcGws%2BcRfTQKNq1A%3D%3D,"WorldQuant develops and deploys systematic financial strategies across a broad range of asset classes and global markets. We seek to produce high-quality predictive signals (alphas) through our proprietary research platform to employ financial strategies focused on market inefficiencies. Our teams work collaboratively to drive the production of alphas and financial strategies – the foundation of a balanced, global investment platform.
WorldQuant is built on a culture that pairs academic sensibility with accountability for results. Employees are encouraged to think openly about problems, balancing intellectualism and practicality. Excellent ideas come from anyone, anywhere. Employees are encouraged to challenge conventional thinking and possess an attitude of continuous improvement.
Our goal is to hire the best and the brightest. We value intellectual horsepower first and foremost, and people who demonstrate an outstanding talent. There is no roadmap to future success, so we need people who can help us build it.
Technologists at WorldQuant research, design, code, test and deploy firmwide platforms and tooling while working collaboratively with researchers. Our environment is relaxed yet intellectually driven. We seek people who think in code and are motivated by being around like-minded people.
We are looking for an experienced Data Engineer who will help us to build and maintain an ecosystem for processing multiple datasets from different sources (both internal and external) vital to the firm’s investment operations.
What You'll Do
Creating automated data processing system and monitoring/maintaining it
Integrating multiple data sources and databases into one system
Developing interfaces and micro services in Python
Preprocessing and cleansing of semi-structured or unstructured data
Developing efficient algorithms for data processing
Testing and integrating external APIs
Supporting Business Analysts team
What You’ll Bring
A bachelor / master's degree in a technical or quantitative field from top university
At least 3 years of experience as a data engineer or software developer
Excellent programming skills
Experience with data processing using Python
Experience with building databases
Experience with Containers and Kubernetes
Scripting skills in UNIX environment: shell, python Fire, etc.
Experience with code versioning tools (e.g. Git), issue tracking tools (e.g. Jira)
Debugging skills, eye for detail and identifying problems
Strong problem-solving skills and an analytical mindset
A passion for working with data
What We Offer
Competitive and attractive compensation package with clear career road-map – where you feel challenged everyday
We offer a strong culture of learning and development: training courses, library, speakers, share and learn events
Learn from who sits next to you! Working in WQ you are surrounded by smart and talented people
Premium Health Insurance and Employee Assistance Program
Generous time-off policy, re-creation sabbatical leave (based on tenure), Trade Union benefits for staff and family
Team building activities every month: Local engagement events, monthly team lunch – Employee clubs: football, ping-pong, badminton, yoga, running, PS5, movies, etc.
Annual company trip and occasional global conferences – opportunity to travel and connect with our global teams
Happy-hour with tea break, snacks and meals every day in the office!
By submitting this application, you acknowledge and consent to terms of the WorldQuant Privacy Policy. The privacy policy offers an explanation of how and why your data will be collected, how it will be used and disclosed, how it will be retained and secured, and what legal rights are associated with that data (including the rights of access, correction, and deletion). The policy also describes legal and contractual limitations on these rights. The specific rights and obligations of individuals living and working in different areas may vary by jurisdiction.
WorldQuant is an equal opportunity employer and does not discriminate in hiring on the basis of race, color, creed, religion, sex, sexual orientation or preference, age, marital status, citizenship, national origin, disability, military status, genetic predisposition or carrier status, or any other protected characteristic as established by applicable law."
Data Engineer,DataHouse Vietnam,Da Nang Metropolitan Area,https://vn.linkedin.com/jobs/view/data-engineer-at-datahouse-vietnam-4306346137?position=31&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=XeDrHZnEozxqHgNACAHhgw%3D%3D,"We're looking for a
Medior Data Engineer
to join our team in
Da Nang / Binh Duong / HCMC.
In this role, you will build and maintain data pipelines that power our analytics and business intelligence. Work with our team to ensure data quality and accessibility across the organization. You will support the ingestion, transformation, and storage of data from various sources, contributing to reliable and scalable data infrastructure. This role offers a great opportunity to grow your technical skills while collaborating closely with data analysts, scientists, and engineers to enable data-driven decision-making.
MAIN ACCOUNTABILITIES:
Build and manage data workflows to extract, transform, and load data into storage systems.
Write scripts to clean, format, and enrich raw data for analytics use.
Track ETL pipeline performance and quickly address failures or delays.
Work with analysts and data scientists to deliver relevant and usable data.
Maintain clear documentation of data workflows, logic, and troubleshooting steps.
Understand and document clients’ business needs and data flows to identify where analytical processes can add value and prescribe appropriate solutions.
Collect, manipulate, and store data from various sources to construct streamlined data pipelines used to integrate with analytical software dashboards.
JOB QUALIFICATIONS:
Education:
Graduated in a related field (Information Management/Informatics, Computer Science/ Engineering, Data Science).
Experience & Skills:
Over 3 years of experience in databases, data modeling, data collection, data warehousing, and ETL processes.
Hands-on experience with ETL orchestration tools for building and scheduling data workflows.
Proficient in writing and optimizing SQL queries for data extraction, transformation, and analysis.
Strong understanding of modern data storage architectures and their role in supporting analytical workloads.
Skilled in using Python for data processing, automation, and scripting tasks.
Proficient in Git for version control collaborative development.
Self-motivated, proactive, and able to work effectively both independently and within a team environment.
Good English communication skills.
Familiarity with AWS services like S3, Redshift, or Glue in a data engineering context.
Practical experience working with large-scale data storage and querying systems.
Comfortable navigating and executing commands in a Linux or Unix environment.
Understanding of containerization concepts and ability to use Docker for setting up isolated environments.
JOB BENEFITS:
With human-centric spirit, we offer a total compensation package that ranks among the best in the industry. It consists of competitive pay, bonuses, as well as, benefit programs which include health and wellbeing, learning and development, benefits at work.
1. Salary & bonuses
Competitive take-home cash together with monthly allowances.
Annual performance appraisal and salary review.
13th salary and accountability bonus twice a year based on company/individual performance and profitability.
Monthly entertainment/team building allowances & project/team celebrations for certain milestones (applied for project teams and back office).
Working tools: laptop and widescreen provided.
2. Life-long learning & development:
Opportunities to work alongside a fun and global team with the international/US clients from multi-cities (Da Nang/Binh Duong/Ho Chi Minh).
On-site program to explore the regional and US cultures for all levels.
Seniority reward from one-year intervals and up.
Extensive learning budget along with rewards for any individuals to gain some certain certificates as defined.
Tons of internal and external training sessions (online learning resources, English, wellness & tech-share workshops).
3. Fun vibes at work:
Weekly cafeteria expenses with coffee beans, milks, noodles and snacks available at offices.
Public holiday gifts and celebrations on Tet, Women’s days, Men’s Day, Children’s Day & Mid-Autumn Day, Programmer’s Day, Teacher/Trainer’s Day, Christmas Day.
Sport activities and clubs.
4. Health & well-being benefits:
Statutory insurances (social, health and unemployment) based on full salary according to the laws of Vietnam.
Annual health-check with advanced package at a quality medical facility.
National coverage healthcare card and seasonal flu shot.
12 annual leaves + 4 family days-off
Company trip once a year and team outing trips.
Visiting and caring on some occasions: birthday, marriage, pregnancy and baby shower, university graduation, hospitalization and funeral (for self-employees and direct family members)
Join our exceptional team and enjoy these attractive benefits. Explore our open positions and take your career to new heights with us!"
Senior/Lead Data Engineer,TymeX,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/senior-lead-data-engineer-at-tymex-4299945154?position=32&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=usffvy8Jud9y3zcGJUHy5w%3D%3D,"TymeX is building some of world's fastest growing digital banks and the data team plays a key role in driving the bank's vision of creating a platform that stimulates economic participation and facilitates broader financial inclusion by implementing creative best in class data and analytic solutions to achieve success in providing quality services and products to our customers and optimising the business.
Senior/Lead Data Engineer
you will contribute to the mission by creating solutions that will directly support informed decision-making and innovation by providing clean, protected, quality and auditable data from various sources into fit for purpose data products.
In this role, you can expect to:
Design, develop, test, deploy and monitor data pipelines in Databricks on AWS from a wide variety of data sources
Design, develop, test, deploy and monitor scalable code with PySpark and SQL in Databricks
Identify opportunities to improve internal process through code optimisation and automation
Build data quality dashboards, lineage flows / and or monitoring tools to utilize the data pipeline, providing active monitoring and actionable insight into overall data quality and data governance
Assist in migrating data from legacy systems onto newly developed solutions
Follow and lead best practices on all data security, retention, and privacy policies
Requirements
Bachelor's degree
3+ years' experience of building ETL/ELT pipelines
Proven competency in solution design, development, implementation, reporting and analysis
Proficiency in Apache-Spark, Python and SQL languages
Proficiency in working with Text, Delta, Parquet, JSON, CSV, and XML data formats.
Working knowledge of Spark structured streaming
AWS infrastructure experience, specifically working with S3
Solid understanding of git-based version control, DevOps, and CI/CD. Experience of working on Atlassian stack a plus
Knowledge of common web API frameworks and web services
Strong teamwork, relationship, and client management skills, and the ability to influence peers and senior management to accomplish team goals
Willingness to embrace modern technology, best practice, and ways of work
Benefits
Performance bonus up to 2 months
13th month salary pro-rata
15-day annual leave+ 3-day sick leave + 1 birthday leave + 1 Christmas leave
Meal and parking allowance are covered by the company
Full benefits and salary rank during probation
Insurances as Vietnamese labor law and premium health care for you and your family without seniority compulsory
SMART goals and clear career opportunities (technical seminar, conference, and career talk) - we focus on your development
Values-driven, international working environment, and agile culture
Overseas travel opportunities for training and working related
Internal Hackathons and company's events (team building, coffee run, blue card...)
Work-life balance 40-hr per week from Mon to Fri"
"Data Engineer, Specialist",AIA Vietnam,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-specialist-at-aia-vietnam-4306363010?position=33&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=uCJkwKmAfRBU1y83j%2FhSYQ%3D%3D,": Manager, Data Engineer
Ho Chi Minh
: Customer and Information Technology |
: Information Technology
: Individual Contributor
THE OPPORTUNITY:
We are currently looking for
Data Engineer, Specialist
who is responsible for developing and maintaining data pipelines, infrastructure, and systems that support advanced analytics initiatives across various departments and support data integration within the Data Warehouse platform as enterprise golden source.
ROLES AND RESPONSIBILITIES:
Develop and implement data pipelines, ETL processes, and set up routines for data from various data sources.
Optimize the data processing and query performance to ensure scalable and efficient data operations.
Implement data quality controls, data governance policies and securities requirement to safeguard sensitive insurance data
Collaborate with data scientist, data analysts and other business stakeholders to understand data needs
Develop technical and training manuals and comply to other company data life cycle standards
Stay up to date with emerging technologies, trends, and best practice in data engineers to drive continuous improvement and innovation.
JOB REQUIREMENTS:
Education & Experience
Education: BSc degree in computer science. Data Engineering, Information Systems, Mathematics or relevant fields
Experience 3-5 years of experience as a Data Engineer or similar role within the life insurance industry or other relevant financial services sector.
Technical skill
Experience with Data warehouse: Databrick, Spark.
Experience with Business Intelligence: Power BI
Expert-level SQL: SQL Server, SQL Databrick.
Experience in programming language – Python, Spark
Experience with Git source control.
Experience with building data model special on insurance domain.
Hands-on experience of working with Data projects that have a complete CI/CD system.
Experience with ETL tool: Azure Data Factory, Airflow.
Soft skill:
Ability to influence both technical and business peers and stakeholders.
Lead by example – jump 'on the tools' when required to support the team.
Good communication skills with the ability to convey complex technical concepts in clear and concise language for various audiences, English communication is required.
Demonstrated problem-solving skills and a results-oriented mindset."
Data Analyst,ĐÚNG NGƯỜI ĐÚNG VIỆC Community,"Phường Chí Minh, Hai Duong, Vietnam",https://vn.linkedin.com/jobs/view/data-analyst-at-%C4%91%C3%BAng-ng%C6%B0%E1%BB%9Di-%C4%91%C3%BAng-vi%E1%BB%87c-community-4313606086?position=34&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=6%2FjtA%2BymgQnoEjayOLF64Q%3D%3D,"Mô tả công việc
Key Responsibilities
Engage with business stakeholders to understand reporting needs, KPIs, and pain points. Translate business questions into analytical frameworks
Develop dashboards for Pilot metrics (EFOS, OSA, NPD availability, visibility compliance, and in-store discipline using geo-fencing and SOP adherence) & Intraday dashboard platform from design to roll out
Automate reporting for Pods, SteerCo, and regional squads.
Ensure data quality & pipeline integrity across multiple sources (e.g., execution, stock, offtake, NIV)
Scale dashboards to national roll-out with high adoption rate
Support AI image recognition pilots and integrate external data sources
Deliverables / OKRs
Pilot dashboards live as per required lead-time (both Power BI & Intraday one)
≥90% usage & adoption rate.
Weekly SteerCo pack report delivered on-time 100%.
Regional dashboards launched by Jan 2026.
Training delivered to regional teams for dashboard usage.
Yêu cầu công việc
3+ years in data/business analysis (preferably FMCG/retail).
Strong dashboarding & visualization (Power BI, Tableau, Looker, or Google Data Studio).
Solid SQL/ETL knowledge; experience managing data pipelines.
Strong problem-solving skills; ability to turn data into insights.
Excellent communication skills to present insights clearly.
Tại sao bạn yêu thích làm việc tại đây
Bảo hiểm xã hội, Bảo hiểm y tế
Nhân viên được hưởng lương tháng 13 và thưởng các dịp lễ, Tết - theo chính sách công ty từng năm
Tăng lương định kỳ: Review lương dựa trên năng lực và kết quả công việc hàng năm
Đánh giá minh bạch, công bằng
Trang bị hiện đại: Laptop làm việc tiên tiến, hỗ trợ công cụ đầy đủ
Địa điểm làm việc
Hồ Chí Minh: Quận 7
Thời gian làm việc
Thứ 2 - Thứ 6 (từ 08:30 đến 17:30)
Từ thứ 2 đến thứ 6 (8h/ngày)"
Data Engineer,Ban Vien Corporation,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-ban-vien-corporation-4310311711?position=35&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=%2FoZNJVbSZTQd9TBK4k4Ngg%3D%3D,"JOB DESCRIPTION
We are looking for Data Engineer to join us:
• Responsible for implementing robust data pipeline.
• Responsible for creating reusable and scalable data pipelines.
• Responsible for using Databrick to build model in Datalake/Azure Synapse.
• Data engineers’ provision and set up data platform/Streaming data technologies that are on-premises and in the cloud.
• The data platforms include relational databases, nonrelational databases, data streams, and file stores.
REQUIREMENTS
• Bachelor’s degree in Computer Science, Computer Engineering, or a related technical discipline.
• At least 3 years of experience in Data Engineering, particularly in Data Integration.
Technical Proficiency:
Hands-on experience with Flink/Spark + Kafka/Airflow
Have experience with Databricks/ Snowflake / InfluxDB
Having experience with CI/CD and Kubernetes is a plus
Strong skills in SQL and Python, Java programming.
Familiarity with Big Data technologies such as Spark or Hadoop/MapReduce
Practical knowledge of Azure services like HDInsight and Azure Databricks would be advantageous
Knowledge of hosting solutions, container-based deployments, and storage architectures is beneficial
Nice to have:
Understanding of cloud platforms, including one of Azure/AWS/Google Cloud Platform
COMPANY BENEFITS
• 13th Salary + Performance Bonus.
• Pass probation Bonus.
• Premium healthcare insurance benefits (PVI Insurance package) and family medical benefit (based on the level of experience).
• Provide the famous e-learning platform-Udemy, to encourage continuous learning to adapt to the T-shape model.
• Flexible working time: only 8 hours required as continual working-time at the office.
• Annual leave up to 17 days: 12 days paid leave + 5 days’ sick leave.
• Professional and Personal Development Training Programs.
• 4 Stars standard company trip in summer and a big annual Year-End-Party.
• Coffee and snacks provided.
• Holiday celebrations and parties for team members and family."
BigData Engineer,Viettel AI,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/bigdata-engineer-at-viettel-ai-4312322565?position=36&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=xKXStNRnY3Z5Zl7LYPlmJA%3D%3D,"Attractive remuneration with competitive income and 6 months/year salaries/ efficient production and business.
A welfare package of $2000 per year including Tet bonus, holiday bonus and resort.
Being a colleague of a team of leading technology experts in fields such as: DPI, BigData, Machine Learning, AI, Data Mining, Speech Recognition, Virtual Assistant…
Improve capacity through intensive training courses, study support and professional certification exams.
Consider raising the minimum salary once a year. Access to attractive promotion opportunities.
12 days holiday, 12 vacation days and 3 annual vacation days.
Always enjoy to the fullest full benefits of social insurance, health insurance, unemployment insurance and statutory leave and sick leave after signing the labor contract
Join the comprehensive health care MIC insurance package.
Relax, spark creativity with Happy Time every day, modern workspace.
Enjoy a delicious lunch, selected by nutritionists.
Working in key projects of the Group with large system scale.
Experience the multiculturalism when working with Viettel's personnel in more than 10 countries.
Job Descriptions
Building and maintaining a data collection and processing system
Integrating data from various sources.
Designing a Data Lake, Data system Warehouse, building an ETL system on the basis of big data.
Job requirements
General Requirements
Graduated from a regular university with good grade or higher, majoring in the right position
Having experience of 02 years or more (accepting the whole process of working experience) while undergrad)
TOEIC 550/990
Required Requirements
Experience working with one of the relational databases: MySQL, Postgres, Oracle
Knowledge of data processing techniques, ETL process
Minimum 1 year of Java or Python programming
Programming experience Scala
Experience working with NoSQL databases such as Elasticsearch, Redis, HBase, Cassandra
Experience working with graph databases like Neo4J, JanusGraph
Knowledge of distributed computing principles, Big Data ecosystem such as Hadoop, MapReduce, Spark
Experience in integrating data from multiple sources different
Programming experience with Spark, Spark Streaming, Kafka, is an advantage"
Data Engineer,Techcom Securities,"Hanoi, Hanoi, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-techcom-securities-4311824920?position=37&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=4R5Ww%2BO%2F9GYfkniWJZNFvQ%3D%3D,"we are redefining how financial services are delivered — using technology to create smarter, faster, and more inclusive financial solutions. Data is at the core of everything we do, from personalized financial products to real-time fraud detection. We're looking for a
Data Engineer
with a passion for building scalable and secure data infrastructure to help power our next generation of fintech products.
Data Engineer
, you will work closely with data scientists, backend engineers, and product teams to build and optimize our end-to-end data platform. You’ll be responsible for architecting and maintaining robust, scalable, and high-performance
data pipelines and lakehouse infrastructure
to support our analytics, risk models, and customer experiences.
Key Responsibilities
Design and develop
ETL/ELT pipelines
for structured and unstructured financial data across internal and third-party systems.
Develop and manage large-scale
data infrastructure
Apache Spark
Apache Kafka
Apache Flink
data lakehouse architecture
services such as:
Collaborate with analytics, product, and engineering teams to deliver reliable and scalable data solutions for reporting and real-time applications.
Collaborate with analytics, ML, and engineering teams to integrate data products into core financial systems.
Monitor and improve the performance and reliability of pipelines and data services.
Required Qualifications
of experience as a Data Engineer or in a similar backend/data-focused engineering role.
Proficient in
, with working knowledge of
Solid understanding of
distributed data processing
streaming architectures
Strong experience with
big data tools
(Spark, Kafka, Flink, Hive, etc.).
Production experience with
AWS data stack
(S3, Glue, EMR, Kinesis, Redshift, Athena).
Strong SQL skills and experience with data modeling and data warehousing concepts.
Familiarity with CI/CD, Docker, Git, and automation in data workflows.
Nice to Have
Experience with
open data lakehouse
Apache Iceberg
Apache Hudi
Hands-on experience with
Familiarity with
data cataloging
data lineage
tools (e.g., AWS Glue, Apache Atlas, Great Expectations).
Fintech domain knowledge (payments, risk, lending, compliance) is a strong plus.
What You’ll Get
Competitive salary with performance-based bonuses.
Opportunity to work on cutting-edge data architecture in a high-growth fintech environment.
Ownership and influence in a data-driven organization.
Learning budget and access to leading cloud and data tools."
Data Engineer,WorldQuant,"Ho Chi Minh City, Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-worldquant-4288906692?position=38&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=8dUhszda2OJXawo13E3sPw%3D%3D,"WorldQuant develops and deploys systematic financial strategies across a broad range of asset classes and global markets. We seek to produce high-quality predictive signals (alphas) through our proprietary research platform to employ financial strategies focused on market inefficiencies. Our teams work collaboratively to drive the production of alphas and financial strategies – the foundation of a balanced, global investment platform.
WorldQuant is built on a culture that pairs academic sensibility with accountability for results. Employees are encouraged to think openly about problems, balancing intellectualism and practicality. Excellent ideas come from anyone, anywhere. Employees are encouraged to challenge conventional thinking and possess an attitude of continuous improvement.
Our goal is to hire the best and the brightest. We value intellectual horsepower first and foremost, and people who demonstrate an outstanding talent. There is no roadmap to future success, so we need people who can help us build it.
Technologists at WorldQuant research, design, code, test and deploy firmwide platforms and tooling while working collaboratively with researchers and portfolio managers. Our environment is relaxed yet intellectually driven. We seek people who think in code and are motivated by being around like-minded people.
We are seeking for an exceptionally talented candidate to join our rapidly growing team as a Data Engineer. The Data Engineer will partner with our close-knit team of quantitative researchers, technologists and data sourcing colleagues to analyze and enrich a broad range of structured and unstructured large-scale data.
Job Responsibilities Include, But Not Limited To The Followings
Enriching a wide range of structured and unstructured data into datasets for quantitative analysis and financial engineering.
Enhancing data quality & integrity by developing validation tools to measure the effectiveness of data enrichment.
Becoming a domain expert on different deep learning and machine earning applications, analyzing & understanding the underlying dynamics and behaviors within the data.
Develop insights based on the data and collaborate with the research team to generate signals.
Developing the utility tools that can further automate the software development, testing and deployment workflow.
What You’ll Bring
Strong academic background – minimum of a bachelor’s degree in a technical or quantitative field.
Practical experience with and understanding of deep neural networks and other machine learning techniques.
Demonstrated ability to implement data science pipelines and real-time applications in Python (C++ is a plus).
3+ years of relevant experience as a Data Engineer or similar roles.
Proficiency with python based tools like Jupyter notebook, coding standards like pep8.
Experience with LLM/AI for data processing is a plus
Exceptional analytical & problem solving abilities, with a strong attention to detail.
Good command of English
Excellent software development skills: ability to convert rough overall use-cases to a working codebase.
Motivated by a deep curiosity and passion to learn is a plus.
Past experience as a data scientist or data engineer in finance or investment profile is a plus.
Experience with Linux/Unix shell and Git is a plus.
What We Offer
Competitive and attractive compensation package with clear career road-map – where you feel challenged everyday
We offer a strong culture of learning and development: training courses, library, speakers, share and learn events
Learn from who sits next to you! Working in WQ you are surrounded by smart and talented people
Premium Health Insurance and Employee Assistance Program
Generous time-off policy, re-creation sabbatical leave (based on tenure), Trade Union benefits for staff and family
Team building activities every month: Local engagement events, monthly team lunch – Employee clubs: football, ping-pong, badminton, yoga, running, PS5, movies, etc.
Annual company trip and occasional global conferences – opportunity to travel and connect with our global teams
Happy-hour with tea break, snacks and meals every day in the office!
By submitting this application, you acknowledge and consent to terms of the WorldQuant Privacy Policy. The privacy policy offers an explanation of how and why your data will be collected, how it will be used and disclosed, how it will be retained and secured, and what legal rights are associated with that data (including the rights of access, correction, and deletion). The policy also describes legal and contractual limitations on these rights. The specific rights and obligations of individuals living and working in different areas may vary by jurisdiction.
WorldQuant is an equal opportunity employer and does not discriminate in hiring on the basis of race, color, creed, religion, sex, sexual orientation or preference, age, marital status, citizenship, national origin, disability, military status, genetic predisposition or carrier status, or any other protected characteristic as established by applicable law."
System Engineer,The Grand Ho Tram,Ho Chi Minh City Metropolitan Area,https://vn.linkedin.com/jobs/view/system-engineer-at-the-grand-ho-tram-4300106098?position=39&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=VSMz55J4ICS5kEpZsQ5vUQ%3D%3D,"Information Technology
Work Location Ho Chi Minh (Ho Tram Commune)
Job Level Experienced (Non - Manager)
Job Type Permanent
Qualification College
Experiences 1 - 3 Years
Salary Competitive
Industry IT - Hardware / Network, Restaurant / Hotel
Contact Person HR
Job Description
Perform the day to day operations, management and administration to protect the integrity, confidentiality, and availability of information assets and technology infrastructure of the organization.
Identify security issues and risks, monitor and test application performance for potential bottlenecks, identify possible solutions and develop mitigation plans
Performing analysis of system security needs and contributes to design, integration, and installation of hardware and software.
Analyzing, troubleshooting and correcting system problems remotely and on-site.
Architect, design, implement, support, and evaluate security-focused tools and services including project leadership roles
Develop and interpret security policies and procedures
Mentor junior members of the team
Participate in security compliance efforts
Develop and deliver training materials and perform general security awareness and specific security technology training
Acquisition and vendor risk assessment due diligence
Evaluate and recommend new and emerging security products and technologies
Participate in tier 2 and tier 3 security operations support
Participate in incident handling
Participate in projects that develop new intellectual property
Manage and monitor all installed systems and infrastructure
Write and maintain custom scripts to increase system efficiency and lower the human intervention time on any tasks
Job Requirement
BS/MBS in Computer Science or equivalent desired
Emerging company-wide reputation in the field of information security
Consistent implementation of security solutions at the business unit level
At least 3 years of experience in infrastructure or application-level vulnerability testing and auditing
At least 3 years of system, network and/or application security experience
Strong experience and detailed technical knowledge in security engineering, system and network security, authentication and security protocols, cryptography, and application security
Knowledge of network and web related protocols (e.g., TCP/IP, UDP, IPSEC, HTTP, HTTPS, routing protocols)
Job tags: System Engineer IT Specialist Chuyên viên IT Nhân viên IT IT Engineer IT Assistant IT Executive Kỹ sư hệ thống"
Data Engineer,FPT Software Career,"Ho Chi Minh City, Vietnam",https://vn.linkedin.com/jobs/view/data-engineer-at-fpt-software-career-4309475547?position=40&pageNum=0&refId=pMTQpI%2BZF52r0jjSF3JfsQ%3D%3D&trackingId=TAxqL0eoVdFQzRpN7iacOg%3D%3D,"Job Description :
Design and implement scalable data pipelines to process large and complex datasets.
Build and maintain reports and dashboards to provide actionable insights across key business domains.  Analyze data to identify trends, patterns, and opportunities for business improvement.
Collaborate with cross-functional teams to gather business requirements and translate them into technical solutions.
Recommend and integrate data sources to improve data completeness and decision making.
Develop and maintain data models, dictionaries, and documentation to support BI initiatives.
Implement data visualization techniques and tools to communicate findings effectively.
Contribute to the definition and execution of BI strategy and roadmap.
Stay current with the latest advancements in data engineering, cloud platforms, and analytics technologies.
Job Qualification :
Bachelor’s degree in Computer Science, Engineering, or related field.
Excellent verbal and written communication skills in English.
3+ years of hands-on experience building and operationalizing data pipelines.
Strong experience with GCP, particularly BigQuery and related services (e.g., Cloud Storage, Dataflow, Pub/Sub).
Proficiency in SQL, ETL development, and data warehousing concepts.
Experience with BI tools such as Power BI, Tableau (optional), or other reporting solutions.
Solid understanding of data modeling, data integration, and data governance.
Familiarity with source control and DevOps practices using Git and CI/CD tools.
Hands-on experience working with various data sources: SQL databases, flat files (CSV/XML), Web APIs, etc. Strong understanding of storage technologies: Data Lake, Relational DBs, NoSQL, and Graph databases.  Comfortable working in Agile and fast-paced environments."
